{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c952b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, log_loss, confusion_matrix, ConfusionMatrixDisplay, f1_score, average_precision_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Utils\n",
    "import pickle\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "# Graph libs\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.max_rows',500)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Global vars\n",
    "random_state = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f1098de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df0 = pd.read_csv('dataset.csv', sep =';')\n",
    "\n",
    "# Select rows where default != NaN\n",
    "df = df0[df0['default'].isna() == False ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35c6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['uuid','default'],axis=1)\n",
    "y = df['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78a3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Train, Test, and Validation datasets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state,stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=random_state,stratify=y_train) # 0.25*0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "420581a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', MinMaxScaler())\n",
    "])\n",
    "\n",
    "\n",
    "cat_cat_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal',OneHotEncoder(handle_unknown='ignore',sparse_output=False))\n",
    "])\n",
    "\n",
    "\n",
    "cat_num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='constant',fill_value=999))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33ea130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = ColumnTransformer(transformers=[\n",
    "    ('num_pipeline',num_pipeline,\n",
    "     \n",
    "     [\n",
    "       'account_amount_added_12_24m',\n",
    "       'account_days_in_dc_12_24m', 'account_days_in_rem_12_24m',\n",
    "       'account_days_in_term_12_24m', 'account_incoming_debt_vs_paid_0_24m',\n",
    "       'age', 'avg_payment_span_0_12m', 'avg_payment_span_0_3m',\n",
    "       'max_paid_inv_0_12m', 'max_paid_inv_0_24m',\n",
    "       'num_active_div_by_paid_inv_0_12m', 'num_active_inv',\n",
    "       'num_arch_dc_0_12m', 'num_arch_dc_12_24m', 'num_arch_ok_0_12m',\n",
    "       'num_arch_ok_12_24m', 'num_arch_rem_0_12m',\n",
    "       'num_arch_written_off_0_12m', 'num_arch_written_off_12_24m',\n",
    "       'num_unpaid_bills','recovery_debt',\n",
    "       'sum_capital_paid_account_0_12m', 'sum_capital_paid_account_12_24m',\n",
    "       'sum_paid_inv_0_12m', 'time_hours'\n",
    "        ]),\n",
    "    \n",
    "    \n",
    "    ('cat_cat_pipeline',cat_cat_pipeline,\n",
    "     \n",
    "     [\n",
    "        'merchant_category', 'merchant_group',\n",
    "        'name_in_email', 'has_paid',\n",
    "        ]),\n",
    "    \n",
    "    \n",
    "    ('cat_num_pipeline',cat_num_pipeline,\n",
    "     \n",
    "     [\n",
    "       'account_status', 'account_worst_status_0_3m',\n",
    "       'account_worst_status_12_24m', 'account_worst_status_3_6m',\n",
    "       'account_worst_status_6_12m',  \n",
    "         'status_last_archived_0_24m',\n",
    "       'status_2nd_last_archived_0_24m', 'status_3rd_last_archived_0_24m',\n",
    "       'status_max_archived_0_6_months', 'status_max_archived_0_12_months',\n",
    "       'status_max_archived_0_24_months', 'worst_status_active_inv'\n",
    "        ])\n",
    "    \n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "149e933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=random_state,class_weight='balanced')\n",
    "\n",
    "parameters = [\n",
    "    {'solver' : ['saga'],\n",
    "      'penalty' : ['elasticnet', 'l1', 'l2'],\n",
    "      'C' : Real(1e-6, 1e+2, prior='log-uniform'),\n",
    "      'l1_ratio':Real(0, 1, prior='uniform')\n",
    "    },\n",
    "    {'solver' : ['newton-cg', 'lbfgs'],\n",
    "      'penalty' : ['l2'],\n",
    "      'C' : Real(1e-6, 1e+2, prior='log-uniform'),\n",
    "    }\n",
    "]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "lg_cv = BayesSearchCV(lg, parameters, cv=cv, scoring='neg_log_loss', n_jobs=-1, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f261f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_pipeline = Pipeline(steps=[\n",
    "    ('feature_pipeline', feature_pipeline),\n",
    "    ('model', lg_cv)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58a1c4aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:425: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/parvizalizada/Documents/GitHub.nosync/Klarna case/klr/lib/python3.9/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;feature_pipeline&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;num_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                  (&#x27;scale&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;account_amount_added_12_24m&#x27;,\n",
       "                                                   &#x27;account_days_in_dc_12_24m&#x27;,\n",
       "                                                   &#x27;account_days_in_rem_12_24m&#x27;,\n",
       "                                                   &#x27;account_days_in_term_12_24m&#x27;,\n",
       "                                                   &#x27;account_incoming_debt_vs_paid_0_24m&#x27;,\n",
       "                                                   &#x27;age&#x27;,\n",
       "                                                   &#x27;avg_pa...\n",
       "                               n_jobs=-1, scoring=&#x27;neg_log_loss&#x27;,\n",
       "                               search_spaces=[{&#x27;C&#x27;: Real(low=1e-06, high=100.0, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                                               &#x27;l1_ratio&#x27;: Real(low=0, high=1, prior=&#x27;uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                                               &#x27;penalty&#x27;: [&#x27;elasticnet&#x27;, &#x27;l1&#x27;,\n",
       "                                                           &#x27;l2&#x27;],\n",
       "                                               &#x27;solver&#x27;: [&#x27;saga&#x27;]},\n",
       "                                              {&#x27;C&#x27;: Real(low=1e-06, high=100.0, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                                               &#x27;penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                                               &#x27;solver&#x27;: [&#x27;newton-cg&#x27;,\n",
       "                                                          &#x27;lbfgs&#x27;]}],\n",
       "                               verbose=3))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;feature_pipeline&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[(&#x27;num_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                  (&#x27;scale&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;account_amount_added_12_24m&#x27;,\n",
       "                                                   &#x27;account_days_in_dc_12_24m&#x27;,\n",
       "                                                   &#x27;account_days_in_rem_12_24m&#x27;,\n",
       "                                                   &#x27;account_days_in_term_12_24m&#x27;,\n",
       "                                                   &#x27;account_incoming_debt_vs_paid_0_24m&#x27;,\n",
       "                                                   &#x27;age&#x27;,\n",
       "                                                   &#x27;avg_pa...\n",
       "                               n_jobs=-1, scoring=&#x27;neg_log_loss&#x27;,\n",
       "                               search_spaces=[{&#x27;C&#x27;: Real(low=1e-06, high=100.0, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                                               &#x27;l1_ratio&#x27;: Real(low=0, high=1, prior=&#x27;uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                                               &#x27;penalty&#x27;: [&#x27;elasticnet&#x27;, &#x27;l1&#x27;,\n",
       "                                                           &#x27;l2&#x27;],\n",
       "                                               &#x27;solver&#x27;: [&#x27;saga&#x27;]},\n",
       "                                              {&#x27;C&#x27;: Real(low=1e-06, high=100.0, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                                               &#x27;penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                                               &#x27;solver&#x27;: [&#x27;newton-cg&#x27;,\n",
       "                                                          &#x27;lbfgs&#x27;]}],\n",
       "                               verbose=3))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">feature_pipeline: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1,\n",
       "                  transformers=[(&#x27;num_pipeline&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                 (&#x27;scale&#x27;, MinMaxScaler())]),\n",
       "                                 [&#x27;account_amount_added_12_24m&#x27;,\n",
       "                                  &#x27;account_days_in_dc_12_24m&#x27;,\n",
       "                                  &#x27;account_days_in_rem_12_24m&#x27;,\n",
       "                                  &#x27;account_days_in_term_12_24m&#x27;,\n",
       "                                  &#x27;account_incoming_debt_vs_paid_0_24m&#x27;, &#x27;age&#x27;,\n",
       "                                  &#x27;avg_payment_span_0_12m&#x27;,\n",
       "                                  &#x27;avg_payment_span_...\n",
       "                                 [&#x27;account_status&#x27;, &#x27;account_worst_status_0_3m&#x27;,\n",
       "                                  &#x27;account_worst_status_12_24m&#x27;,\n",
       "                                  &#x27;account_worst_status_3_6m&#x27;,\n",
       "                                  &#x27;account_worst_status_6_12m&#x27;,\n",
       "                                  &#x27;status_last_archived_0_24m&#x27;,\n",
       "                                  &#x27;status_2nd_last_archived_0_24m&#x27;,\n",
       "                                  &#x27;status_3rd_last_archived_0_24m&#x27;,\n",
       "                                  &#x27;status_max_archived_0_6_months&#x27;,\n",
       "                                  &#x27;status_max_archived_0_12_months&#x27;,\n",
       "                                  &#x27;status_max_archived_0_24_months&#x27;,\n",
       "                                  &#x27;worst_status_active_inv&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num_pipeline</label><div class=\"sk-toggleable__content\"><pre>[&#x27;account_amount_added_12_24m&#x27;, &#x27;account_days_in_dc_12_24m&#x27;, &#x27;account_days_in_rem_12_24m&#x27;, &#x27;account_days_in_term_12_24m&#x27;, &#x27;account_incoming_debt_vs_paid_0_24m&#x27;, &#x27;age&#x27;, &#x27;avg_payment_span_0_12m&#x27;, &#x27;avg_payment_span_0_3m&#x27;, &#x27;max_paid_inv_0_12m&#x27;, &#x27;max_paid_inv_0_24m&#x27;, &#x27;num_active_div_by_paid_inv_0_12m&#x27;, &#x27;num_active_inv&#x27;, &#x27;num_arch_dc_0_12m&#x27;, &#x27;num_arch_dc_12_24m&#x27;, &#x27;num_arch_ok_0_12m&#x27;, &#x27;num_arch_ok_12_24m&#x27;, &#x27;num_arch_rem_0_12m&#x27;, &#x27;num_arch_written_off_0_12m&#x27;, &#x27;num_arch_written_off_12_24m&#x27;, &#x27;num_unpaid_bills&#x27;, &#x27;recovery_debt&#x27;, &#x27;sum_capital_paid_account_0_12m&#x27;, &#x27;sum_capital_paid_account_12_24m&#x27;, &#x27;sum_paid_inv_0_12m&#x27;, &#x27;time_hours&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat_cat_pipeline</label><div class=\"sk-toggleable__content\"><pre>[&#x27;merchant_category&#x27;, &#x27;merchant_group&#x27;, &#x27;name_in_email&#x27;, &#x27;has_paid&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat_num_pipeline</label><div class=\"sk-toggleable__content\"><pre>[&#x27;account_status&#x27;, &#x27;account_worst_status_0_3m&#x27;, &#x27;account_worst_status_12_24m&#x27;, &#x27;account_worst_status_3_6m&#x27;, &#x27;account_worst_status_6_12m&#x27;, &#x27;status_last_archived_0_24m&#x27;, &#x27;status_2nd_last_archived_0_24m&#x27;, &#x27;status_3rd_last_archived_0_24m&#x27;, &#x27;status_max_archived_0_6_months&#x27;, &#x27;status_max_archived_0_12_months&#x27;, &#x27;status_max_archived_0_24_months&#x27;, &#x27;worst_status_active_inv&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=999, strategy=&#x27;constant&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">model: BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=StratifiedKFold(n_splits=5, random_state=33, shuffle=True),\n",
       "              estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                           random_state=33),\n",
       "              n_jobs=-1, scoring=&#x27;neg_log_loss&#x27;,\n",
       "              search_spaces=[{&#x27;C&#x27;: Real(low=1e-06, high=100.0, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                              &#x27;l1_ratio&#x27;: Real(low=0, high=1, prior=&#x27;uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                              &#x27;penalty&#x27;: [&#x27;elasticnet&#x27;, &#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                              &#x27;solver&#x27;: [&#x27;saga&#x27;]},\n",
       "                             {&#x27;C&#x27;: Real(low=1e-06, high=100.0, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                              &#x27;penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                              &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;]}],\n",
       "              verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=33)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=33)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('feature_pipeline',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('num_pipeline',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scale',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['account_amount_added_12_24m',\n",
       "                                                   'account_days_in_dc_12_24m',\n",
       "                                                   'account_days_in_rem_12_24m',\n",
       "                                                   'account_days_in_term_12_24m',\n",
       "                                                   'account_incoming_debt_vs_paid_0_24m',\n",
       "                                                   'age',\n",
       "                                                   'avg_pa...\n",
       "                               n_jobs=-1, scoring='neg_log_loss',\n",
       "                               search_spaces=[{'C': Real(low=1e-06, high=100.0, prior='log-uniform', transform='normalize'),\n",
       "                                               'l1_ratio': Real(low=0, high=1, prior='uniform', transform='normalize'),\n",
       "                                               'penalty': ['elasticnet', 'l1',\n",
       "                                                           'l2'],\n",
       "                                               'solver': ['saga']},\n",
       "                                              {'C': Real(low=1e-06, high=100.0, prior='log-uniform', transform='normalize'),\n",
       "                                               'penalty': ['l2'],\n",
       "                                               'solver': ['newton-cg',\n",
       "                                                          'lbfgs']}],\n",
       "                               verbose=3))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=0.000157216648268923, l1_ratio=0.031112134583001334, penalty=l2, solver=saga;, score=-1.167 total time=   4.1s\n",
      "[CV 4/5] END C=0.010884022395173353, l1_ratio=0.5628602866504493, penalty=elasticnet, solver=saga;, score=-0.674 total time=   5.0s\n",
      "[CV 5/5] END C=0.00534829433244113, l1_ratio=0.4770662488005405, penalty=l2, solver=saga;, score=-3.934 total time=   3.9s\n",
      "[CV 1/5] END C=0.00030334248083875175, l1_ratio=0.967595009327712, penalty=l2, solver=saga;, score=-1.248 total time=   4.0s\n",
      "[CV 4/5] END C=0.0001397032301613737, l1_ratio=0.0936664126761883, penalty=l2, solver=saga;, score=-2.614 total time=   4.0s\n",
      "[CV 2/5] END C=29.995394909157383, l1_ratio=0.03350956435806217, penalty=l2, solver=saga;, score=-0.836 total time=   4.0s\n",
      "[CV 5/5] END C=8.104807987917363, l1_ratio=0.1852164423679063, penalty=l2, solver=saga;, score=-0.359 total time=   4.0s\n",
      "[CV 4/5] END C=0.0001013622747322455, l1_ratio=0.027744140128616427, penalty=l2, solver=saga;, score=-2.223 total time=   4.0s\n",
      "[CV 4/5] END C=0.0026472565453628787, l1_ratio=0.11087469247892914, penalty=l1, solver=saga;, score=-0.765 total time=   5.0s\n",
      "[CV 3/5] END C=0.9918769360216578, l1_ratio=0.09156490293619421, penalty=l1, solver=saga;, score=-0.752 total time=   5.2s\n",
      "[CV 5/5] END C=0.00016561251427846333, l1_ratio=0.031365254082526633, penalty=l2, solver=saga;, score=-1.679 total time=   4.0s\n",
      "[CV 1/5] END C=74.92030968067643, l1_ratio=0.9929613368809689, penalty=elasticnet, solver=saga;, score=-0.828 total time=   4.9s\n",
      "[CV 5/5] END C=1.113538326816699e-06, l1_ratio=0.019351609812173768, penalty=elasticnet, solver=saga;, score=-0.550 total time=   4.5s\n",
      "[CV 1/5] END C=2.874525954558476e-06, l1_ratio=0.5365263332818956, penalty=l1, solver=saga;, score=-1.660 total time=   4.5s\n",
      "[CV 1/5] END C=1.0206800181729177e-06, l1_ratio=0.2805266152311468, penalty=elasticnet, solver=saga;, score=-0.197 total time=   4.6s\n",
      "[CV 5/5] END C=1.7257053176695378e-06, l1_ratio=0.9773701274420823, penalty=elasticnet, solver=saga;, score=-1.178 total time=   4.5s\n",
      "[CV 3/5] END C=1.046741899032118e-06, l1_ratio=0.029691710002166867, penalty=elasticnet, solver=saga;, score=-1.166 total time=   4.5s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-1.907 total time=   4.0s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.9056272035259453, penalty=elasticnet, solver=saga;, score=-0.675 total time=   4.5s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.044765656817050366, penalty=elasticnet, solver=saga;, score=-0.961 total time=   4.5s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.22239444246524223, penalty=elasticnet, solver=saga;, score=-0.800 total time=   4.5s\n",
      "[CV 4/5] END C=83.31994480765619, l1_ratio=0.4369219655275994, penalty=l1, solver=saga;, score=-2.146 total time=   4.9s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.2222249364311293, penalty=elasticnet, solver=saga;, score=-1.104 total time=   4.5s\n",
      "[CV 4/5] END C=2.1088332972597617e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-1.458 total time=   4.0s\n",
      "[CV 3/5] END C=50.573851988719106, l1_ratio=0.0, penalty=l2, solver=saga;, score=-0.683 total time=   4.0s\n",
      "[CV 2/5] END C=48.33564844672137, l1_ratio=0.60566725135925, penalty=l2, solver=saga;, score=-0.644 total time=   4.0s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.36025904059441, penalty=elasticnet, solver=saga;, score=-0.747 total time=   4.5s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.1302390409272838, penalty=elasticnet, solver=saga;, score=-0.901 total time=   4.5s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.41007243225241313, penalty=l1, solver=saga;, score=-0.634 total time=   4.5s\n",
      "[CV 4/5] END C=4.564363662643437e-06, l1_ratio=0.3650926257419068, penalty=elasticnet, solver=saga;, score=-0.976 total time=   4.5s\n",
      "[CV 4/5] END C=0.2775039396950022, penalty=l2, solver=newton-cg;, score=-0.427 total time=  13.5s\n",
      "[CV 1/5] END C=1.2161722885563447e-06, penalty=l2, solver=lbfgs;, score=-0.667 total time=   0.8s\n",
      "[CV 5/5] END C=0.26518016158591345, penalty=l2, solver=newton-cg;, score=-0.428 total time=  13.3s\n",
      "[CV 2/5] END C=6.550622032760515e-05, penalty=l2, solver=newton-cg;, score=-0.639 total time=   2.3s\n",
      "[CV 4/5] END C=0.0015545115853648592, penalty=l2, solver=newton-cg;, score=-0.534 total time=   3.4s\n",
      "[CV 1/5] END C=15.350223919735981, penalty=l2, solver=newton-cg;, score=-0.424 total time=  26.5s\n",
      "[CV 3/5] END C=3.531636013544588, penalty=l2, solver=newton-cg;, score=-0.426 total time=  26.5s\n",
      "[CV 3/5] END C=99.97654290222484, penalty=l2, solver=lbfgs;, score=-0.535 total time=   0.8s\n",
      "[CV 1/5] END C=0.0018437887099508744, penalty=l2, solver=lbfgs;, score=-0.532 total time=   1.0s\n",
      "[CV 1/5] END C=1.3775518449687307, penalty=l2, solver=newton-cg;, score=-0.426 total time=  21.6s\n",
      "[CV 2/5] END C=60.96870166912985, penalty=l2, solver=newton-cg;, score=-0.417 total time=  26.4s\n",
      "[CV 2/5] END C=0.06425474817108065, penalty=l2, solver=newton-cg;, score=-0.446 total time=   8.6s\n",
      "[CV 3/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.426 total time=  34.5s\n",
      "[CV 4/5] END C=0.00021416219013963825, penalty=l2, solver=lbfgs;, score=-0.582 total time=   0.8s\n",
      "[CV 1/5] END C=18.93524522703305, penalty=l2, solver=newton-cg;, score=-0.424 total time=  38.8s\n",
      "[CV 2/5] END C=32.47166281091174, penalty=l2, solver=newton-cg;, score=-0.417 total time=  26.8s\n",
      "[CV 4/5] END C=16.46375775183702, penalty=l2, solver=newton-cg;, score=-0.413 total time=  28.1s\n",
      "[CV 2/5] END C=0.00033716060099170316, penalty=l2, solver=lbfgs;, score=-0.571 total time=   0.9s\n",
      "[CV 5/5] END C=61.12377065065248, penalty=l2, solver=newton-cg;, score=-0.415 total time=  25.8s\n",
      "[CV 4/5] END C=1.8304217860414715e-06, penalty=l2, solver=lbfgs;, score=-0.664 total time=   0.9s\n",
      "[CV 2/5] END C=32.737842063919054, penalty=l2, solver=newton-cg;, score=-0.417 total time=  26.0s\n",
      "[CV 5/5] END C=65.69833852219975, penalty=l2, solver=newton-cg;, score=-0.415 total time=  25.6s\n",
      "[CV 4/5] END C=23.572484624181357, penalty=l2, solver=newton-cg;, score=-0.413 total time=  28.0s\n",
      "[CV 2/5] END C=35.15322066119099, penalty=l2, solver=newton-cg;, score=-0.417 total time=  24.6s\n",
      "[CV 1/5] END C=23.08643993035239, penalty=l2, solver=newton-cg;, score=-0.424 total time=  26.6s\n",
      "[CV 4/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.413 total time=  26.4s\n",
      "[CV 4/5] END C=71.78305243868218, penalty=l2, solver=newton-cg;, score=-0.413 total time=  27.1s\n",
      "[CV 1/5] END C=0.012419773574361123, penalty=l2, solver=lbfgs;, score=-0.570 total time=   0.9s\n",
      "[CV 1/5] END C=5.851058017523036, penalty=l2, solver=newton-cg;, score=-0.424 total time=  27.0s\n",
      "[CV 1/5] END C=25.19332308408921, penalty=l2, solver=newton-cg;, score=-0.424 total time=  28.4s\n",
      "[CV 3/5] END C=0.000157216648268923, l1_ratio=0.031112134583001334, penalty=l2, solver=saga;, score=-0.444 total time=   4.1s\n",
      "[CV 1/5] END C=2.152452064249206e-05, l1_ratio=0.9522868587604243, penalty=l1, solver=saga;, score=-1.464 total time=   4.5s\n",
      "[CV 5/5] END C=0.13517539668499592, l1_ratio=0.7945627717582758, penalty=l1, solver=saga;, score=-2.245 total time=   5.4s\n",
      "[CV 1/5] END C=0.0010944462842573863, l1_ratio=0.07850312334621491, penalty=l1, solver=saga;, score=-0.965 total time=   4.8s\n",
      "[CV 2/5] END C=0.010884022395173353, l1_ratio=0.5628602866504493, penalty=elasticnet, solver=saga;, score=-0.890 total time=   5.0s\n",
      "[CV 2/5] END C=0.00534829433244113, l1_ratio=0.4770662488005405, penalty=l2, solver=saga;, score=-1.211 total time=   3.9s\n",
      "[CV 4/5] END C=0.00030334248083875175, l1_ratio=0.967595009327712, penalty=l2, solver=saga;, score=-1.522 total time=   4.0s\n",
      "[CV 5/5] END C=0.0001397032301613737, l1_ratio=0.0936664126761883, penalty=l2, solver=saga;, score=-9.889 total time=   4.0s\n",
      "[CV 1/5] END C=29.995394909157383, l1_ratio=0.03350956435806217, penalty=l2, solver=saga;, score=-0.271 total time=   4.0s\n",
      "[CV 1/5] END C=1.187745251728505e-06, l1_ratio=0.00036008331429404633, penalty=elasticnet, solver=saga;, score=-1.050 total time=   5.1s\n",
      "[CV 4/5] END C=7.51866387393607e-05, l1_ratio=0.055257308203277206, penalty=l1, solver=saga;, score=-0.706 total time=   4.5s\n",
      "[CV 5/5] END C=0.0001013622747322455, l1_ratio=0.027744140128616427, penalty=l2, solver=saga;, score=-0.693 total time=   4.0s\n",
      "[CV 5/5] END C=0.9918769360216578, l1_ratio=0.09156490293619421, penalty=l1, solver=saga;, score=-0.765 total time=   5.1s\n",
      "[CV 5/5] END C=74.92030968067643, l1_ratio=0.9929613368809689, penalty=elasticnet, solver=saga;, score=-0.252 total time=   4.9s\n",
      "[CV 1/5] END C=0.008657125066312213, l1_ratio=0.010764208935954646, penalty=elasticnet, solver=saga;, score=-0.800 total time=   5.1s\n",
      "[CV 5/5] END C=1.3632557671340782e-06, l1_ratio=0.007828895688134784, penalty=l2, solver=saga;, score=-0.759 total time=   4.0s\n",
      "[CV 4/5] END C=2.874525954558476e-06, l1_ratio=0.5365263332818956, penalty=l1, solver=saga;, score=-0.570 total time=   4.5s\n",
      "[CV 1/5] END C=1.7257053176695378e-06, l1_ratio=0.9773701274420823, penalty=elasticnet, solver=saga;, score=-5.128 total time=   4.5s\n",
      "[CV 4/5] END C=1.046741899032118e-06, l1_ratio=0.029691710002166867, penalty=elasticnet, solver=saga;, score=-0.384 total time=   4.5s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.42654426332093837, penalty=elasticnet, solver=saga;, score=-0.434 total time=   4.5s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.9056272035259453, penalty=elasticnet, solver=saga;, score=-0.652 total time=   4.5s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.044765656817050366, penalty=elasticnet, solver=saga;, score=-0.239 total time=   4.5s\n",
      "[CV 2/5] END C=1.5211501269043081e-06, l1_ratio=0.09444866847397723, penalty=elasticnet, solver=saga;, score=-0.820 total time=   4.5s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.22239444246524223, penalty=elasticnet, solver=saga;, score=-0.591 total time=   4.5s\n",
      "[CV 1/5] END C=83.31994480765619, l1_ratio=0.4369219655275994, penalty=l1, solver=saga;, score=-8.182 total time=   4.9s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.2222249364311293, penalty=elasticnet, solver=saga;, score=-0.559 total time=   4.5s\n",
      "[CV 1/5] END C=48.33564844672137, l1_ratio=0.60566725135925, penalty=l2, solver=saga;, score=-1.202 total time=   4.0s\n",
      "[CV 4/5] END C=0.0013353908501185876, l1_ratio=0.6322549928496771, penalty=l1, solver=saga;, score=-0.682 total time=   4.9s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.3898445302892915, penalty=elasticnet, solver=saga;, score=-0.804 total time=   4.5s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.1302390409272838, penalty=elasticnet, solver=saga;, score=-0.872 total time=   4.5s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.41007243225241313, penalty=l1, solver=saga;, score=-1.309 total time=   4.5s\n",
      "[CV 2/5] END C=4.564363662643437e-06, l1_ratio=0.3650926257419068, penalty=elasticnet, solver=saga;, score=-0.650 total time=   4.5s\n",
      "[CV 4/5] END C=39.154313760916295, l1_ratio=0.21909019135539431, penalty=l2, solver=saga;, score=-1.732 total time=   4.0s\n",
      "[CV 2/5] END C=20.41944230665062, penalty=l2, solver=lbfgs;, score=-0.533 total time=   0.9s\n",
      "[CV 5/5] END C=0.2775039396950022, penalty=l2, solver=newton-cg;, score=-0.427 total time=  13.9s\n",
      "[CV 2/5] END C=9.36086184228762e-06, penalty=l2, solver=lbfgs;, score=-0.666 total time=   0.8s\n",
      "[CV 1/5] END C=0.26518016158591345, penalty=l2, solver=newton-cg;, score=-0.431 total time=  13.0s\n",
      "[CV 4/5] END C=6.550622032760515e-05, penalty=l2, solver=newton-cg;, score=-0.631 total time=   2.1s\n",
      "[CV 4/5] END C=0.007049212372886417, penalty=l2, solver=newton-cg;, score=-0.495 total time=   6.2s\n",
      "[CV 5/5] END C=0.44051095635231635, penalty=l2, solver=newton-cg;, score=-0.424 total time=  16.8s\n",
      "[CV 2/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.417 total time=  27.1s\n",
      "[CV 2/5] END C=15.350223919735981, penalty=l2, solver=newton-cg;, score=-0.416 total time=  27.5s\n",
      "[CV 3/5] END C=0.07438262337019581, penalty=l2, solver=lbfgs;, score=-0.532 total time=   0.9s\n",
      "[CV 5/5] END C=99.97654290222484, penalty=l2, solver=lbfgs;, score=-0.529 total time=   0.9s\n",
      "[CV 4/5] END C=43.842781999862595, penalty=l2, solver=newton-cg;, score=-0.413 total time=  27.2s\n",
      "[CV 4/5] END C=0.0018437887099508744, penalty=l2, solver=lbfgs;, score=-0.534 total time=   1.0s\n",
      "[CV 3/5] END C=1.0073007075897124e-06, penalty=l2, solver=newton-cg;, score=-0.667 total time=   1.5s\n",
      "[CV 3/5] END C=9.07990255597525, penalty=l2, solver=newton-cg;, score=-0.425 total time=  27.2s\n",
      "[CV 1/5] END C=25.624233672003008, penalty=l2, solver=newton-cg;, score=-0.424 total time=  27.7s\n",
      "[CV 1/5] END C=60.96870166912985, penalty=l2, solver=newton-cg;, score=-0.424 total time=  26.0s\n",
      "[CV 3/5] END C=25.37331128014798, penalty=l2, solver=newton-cg;, score=-0.425 total time=  26.0s\n",
      "[CV 1/5] END C=0.00021416219013963825, penalty=l2, solver=lbfgs;, score=-0.577 total time=   0.8s\n",
      "[CV 1/5] END C=32.47166281091174, penalty=l2, solver=newton-cg;, score=-0.424 total time=  26.4s\n",
      "[CV 3/5] END C=61.12377065065248, penalty=l2, solver=newton-cg;, score=-0.425 total time=  27.5s\n",
      "[CV 5/5] END C=32.737842063919054, penalty=l2, solver=newton-cg;, score=-0.415 total time=  24.6s\n",
      "[CV 2/5] END C=65.69833852219975, penalty=l2, solver=newton-cg;, score=-0.417 total time=  23.6s\n",
      "[CV 2/5] END C=23.572484624181357, penalty=l2, solver=newton-cg;, score=-0.416 total time=  26.5s\n",
      "[CV 5/5] END C=14.857871819134768, penalty=l2, solver=newton-cg;, score=-0.415 total time=  24.6s\n",
      "[CV 3/5] END C=23.08643993035239, penalty=l2, solver=newton-cg;, score=-0.425 total time=  26.1s\n",
      "[CV 3/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.426 total time=  23.9s\n",
      "[CV 5/5] END C=37.49015476724531, penalty=l2, solver=newton-cg;, score=-0.415 total time=  22.8s\n",
      "[CV 4/5] END C=1.6403783477783998e-05, penalty=l2, solver=lbfgs;, score=-0.644 total time=   0.8s\n",
      "[CV 3/5] END C=71.78305243868218, penalty=l2, solver=newton-cg;, score=-0.425 total time=  28.1s\n",
      "[CV 4/5] END C=0.012419773574361123, penalty=l2, solver=lbfgs;, score=-0.533 total time=   0.9s\n",
      "[CV 4/5] END C=12.405204992665801, penalty=l2, solver=newton-cg;, score=-0.413 total time=  23.6s\n",
      "[CV 5/5] END C=5.851058017523036, penalty=l2, solver=newton-cg;, score=-0.416 total time=  23.7s\n",
      "[CV 4/5] END C=22.43475073033113, penalty=l2, solver=newton-cg;, score=-0.413 total time=  27.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END C=0.000157216648268923, l1_ratio=0.031112134583001334, penalty=l2, solver=saga;, score=-0.303 total time=   3.9s\n",
      "[CV 2/5] END C=0.033798293944658546, l1_ratio=0.6660092185205861, penalty=l1, solver=saga;, score=-1.254 total time=   5.1s\n",
      "[CV 4/5] END C=1.1942306149170254, l1_ratio=0.5262314503703501, penalty=l2, solver=saga;, score=-1.671 total time=   3.9s\n",
      "[CV 5/5] END C=2.152452064249206e-05, l1_ratio=0.9522868587604243, penalty=l1, solver=saga;, score=-5.289 total time=   4.5s\n",
      "[CV 2/5] END C=0.13517539668499592, l1_ratio=0.7945627717582758, penalty=l1, solver=saga;, score=-1.553 total time=   5.4s\n",
      "[CV 2/5] END C=3.168415741553067e-05, l1_ratio=0.6014088370818449, penalty=l1, solver=saga;, score=-0.981 total time=   4.4s\n",
      "[CV 4/5] END C=0.0010944462842573863, l1_ratio=0.07850312334621491, penalty=l1, solver=saga;, score=-0.377 total time=   4.8s\n",
      "[CV 5/5] END C=0.010884022395173353, l1_ratio=0.5628602866504493, penalty=elasticnet, solver=saga;, score=-1.170 total time=   5.0s\n",
      "[CV 3/5] END C=0.00534829433244113, l1_ratio=0.4770662488005405, penalty=l2, solver=saga;, score=-1.313 total time=   3.9s\n",
      "[CV 4/5] END C=9.874513896646211e-05, l1_ratio=0.5486382640667588, penalty=l2, solver=saga;, score=-0.337 total time=   4.0s\n",
      "[CV 1/5] END C=0.0001820188584119676, l1_ratio=0.01407277409207082, penalty=l2, solver=saga;, score=-1.372 total time=   4.0s\n",
      "[CV 3/5] END C=8.104807987917363, l1_ratio=0.1852164423679063, penalty=l2, solver=saga;, score=-9.637 total time=   4.0s\n",
      "[CV 2/5] END C=1.187745251728505e-06, l1_ratio=0.00036008331429404633, penalty=elasticnet, solver=saga;, score=-0.445 total time=   5.0s\n",
      "[CV 2/5] END C=7.51866387393607e-05, l1_ratio=0.055257308203277206, penalty=l1, solver=saga;, score=-0.619 total time=   4.5s\n",
      "[CV 5/5] END C=0.0026472565453628787, l1_ratio=0.11087469247892914, penalty=l1, solver=saga;, score=-1.539 total time=   5.0s\n",
      "[CV 1/5] END C=0.00016561251427846333, l1_ratio=0.031365254082526633, penalty=l2, solver=saga;, score=-10.916 total time=   4.0s\n",
      "[CV 5/5] END C=26.971176066692006, l1_ratio=0.026897956907127048, penalty=l2, solver=saga;, score=-1.188 total time=   3.9s\n",
      "[CV 2/5] END C=74.92030968067643, l1_ratio=0.9929613368809689, penalty=elasticnet, solver=saga;, score=-1.644 total time=   4.9s\n",
      "[CV 4/5] END C=1.113538326816699e-06, l1_ratio=0.019351609812173768, penalty=elasticnet, solver=saga;, score=-1.405 total time=   4.5s\n",
      "[CV 4/5] END C=0.008657125066312213, l1_ratio=0.010764208935954646, penalty=elasticnet, solver=saga;, score=-0.552 total time=   5.2s\n",
      "[CV 1/5] END C=49.91518968649303, l1_ratio=0.5367366541959938, penalty=l1, solver=saga;, score=-0.336 total time=   4.9s\n",
      "[CV 2/5] END C=1.0206800181729177e-06, l1_ratio=0.2805266152311468, penalty=elasticnet, solver=saga;, score=-0.595 total time=   4.5s\n",
      "[CV 1/5] END C=1.046741899032118e-06, l1_ratio=0.029691710002166867, penalty=elasticnet, solver=saga;, score=-0.241 total time=   4.6s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-0.389 total time=   4.0s\n",
      "[CV 4/5] END C=1.5211501269043081e-06, l1_ratio=0.09444866847397723, penalty=elasticnet, solver=saga;, score=-1.160 total time=   4.4s\n",
      "[CV 3/5] END C=1.6551780295725852e-06, l1_ratio=0.13089958306254687, penalty=elasticnet, solver=saga;, score=-0.534 total time=   4.5s\n",
      "[CV 2/5] END C=83.31994480765619, l1_ratio=0.4369219655275994, penalty=l1, solver=saga;, score=-0.704 total time=   4.9s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.2222249364311293, penalty=elasticnet, solver=saga;, score=-0.775 total time=   4.4s\n",
      "[CV 2/5] END C=2.1088332972597617e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-0.908 total time=   3.9s\n",
      "[CV 2/5] END C=50.573851988719106, l1_ratio=0.0, penalty=l2, solver=saga;, score=-0.529 total time=   3.9s\n",
      "[CV 4/5] END C=48.33564844672137, l1_ratio=0.60566725135925, penalty=l2, solver=saga;, score=-0.986 total time=   4.0s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.36025904059441, penalty=elasticnet, solver=saga;, score=-0.876 total time=   4.5s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.1302390409272838, penalty=elasticnet, solver=saga;, score=-0.191 total time=   4.4s\n",
      "[CV 1/5] END C=20.41944230665062, penalty=l2, solver=lbfgs;, score=-0.527 total time=   0.9s\n",
      "[CV 3/5] END C=0.2775039396950022, penalty=l2, solver=newton-cg;, score=-0.433 total time=  13.7s\n",
      "[CV 3/5] END C=9.36086184228762e-06, penalty=l2, solver=lbfgs;, score=-0.659 total time=   0.9s\n",
      "[CV 5/5] END C=1.2161722885563447e-06, penalty=l2, solver=lbfgs;, score=-0.666 total time=   0.8s\n",
      "[CV 3/5] END C=0.26518016158591345, penalty=l2, solver=newton-cg;, score=-0.434 total time=  12.7s\n",
      "[CV 2/5] END C=0.0015545115853648592, penalty=l2, solver=newton-cg;, score=-0.535 total time=   4.0s\n",
      "[CV 2/5] END C=0.00021951636072127157, penalty=l2, solver=newton-cg;, score=-0.601 total time=   3.0s\n",
      "[CV 3/5] END C=0.007049212372886417, penalty=l2, solver=newton-cg;, score=-0.498 total time=   6.3s\n",
      "[CV 4/5] END C=0.44051095635231635, penalty=l2, solver=newton-cg;, score=-0.423 total time=  16.2s\n",
      "[CV 3/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.426 total time=  23.9s\n",
      "[CV 5/5] END C=15.350223919735981, penalty=l2, solver=newton-cg;, score=-0.415 total time=  25.1s\n",
      "[CV 4/5] END C=0.07438262337019581, penalty=l2, solver=lbfgs;, score=-0.526 total time=   0.9s\n",
      "[CV 1/5] END C=43.842781999862595, penalty=l2, solver=newton-cg;, score=-0.424 total time=  27.4s\n",
      "[CV 5/5] END C=0.0018437887099508744, penalty=l2, solver=lbfgs;, score=-0.533 total time=   1.0s\n",
      "[CV 5/5] END C=1.0073007075897124e-06, penalty=l2, solver=newton-cg;, score=-0.666 total time=   1.5s\n",
      "[CV 2/5] END C=9.07990255597525, penalty=l2, solver=newton-cg;, score=-0.417 total time=  27.1s\n",
      "[CV 5/5] END C=1.1919232573948424, penalty=l2, solver=lbfgs;, score=-0.528 total time=   0.9s\n",
      "[CV 5/5] END C=25.624233672003008, penalty=l2, solver=newton-cg;, score=-0.415 total time=  24.4s\n",
      "[CV 5/5] END C=60.96870166912985, penalty=l2, solver=newton-cg;, score=-0.415 total time=  26.2s\n",
      "[CV 3/5] END C=0.00021416219013963825, penalty=l2, solver=lbfgs;, score=-0.581 total time=   0.8s\n",
      "[CV 3/5] END C=52.74307687576777, penalty=l2, solver=newton-cg;, score=-0.425 total time=  25.5s\n",
      "[CV 1/5] END C=0.00033716060099170316, penalty=l2, solver=lbfgs;, score=-0.561 total time=   0.9s\n",
      "[CV 2/5] END C=61.12377065065248, penalty=l2, solver=newton-cg;, score=-0.417 total time=  27.3s\n",
      "[CV 2/5] END C=1.8304217860414715e-06, penalty=l2, solver=lbfgs;, score=-0.667 total time=   0.9s\n",
      "[CV 3/5] END C=32.737842063919054, penalty=l2, solver=newton-cg;, score=-0.425 total time=  25.8s\n",
      "[CV 1/5] END C=23.572484624181357, penalty=l2, solver=newton-cg;, score=-0.424 total time=  28.0s\n",
      "[CV 5/5] END C=35.15322066119099, penalty=l2, solver=newton-cg;, score=-0.415 total time=  25.2s\n",
      "[CV 2/5] END C=23.08643993035239, penalty=l2, solver=newton-cg;, score=-0.416 total time=  25.6s\n",
      "[CV 2/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.417 total time=  27.1s\n",
      "[CV 2/5] END C=1.6403783477783998e-05, penalty=l2, solver=lbfgs;, score=-0.654 total time=   0.8s\n",
      "[CV 4/5] END C=21.10031343896973, penalty=l2, solver=newton-cg;, score=-0.413 total time=  25.6s\n",
      "[CV 3/5] END C=0.012419773574361123, penalty=l2, solver=lbfgs;, score=-0.536 total time=   0.9s\n",
      "[CV 3/5] END C=12.405204992665801, penalty=l2, solver=newton-cg;, score=-0.425 total time=  20.3s\n",
      "[CV 3/5] END C=5.851058017523036, penalty=l2, solver=newton-cg;, score=-0.425 total time=  25.9s\n",
      "[CV 3/5] END C=22.43475073033113, penalty=l2, solver=newton-cg;, score=-0.425 total time=  27.7s\n",
      "[CV 1/5] END C=0.000157216648268923, l1_ratio=0.031112134583001334, penalty=l2, solver=saga;, score=-0.274 total time=   3.9s\n",
      "[CV 1/5] END C=1.1942306149170254, l1_ratio=0.5262314503703501, penalty=l2, solver=saga;, score=-0.290 total time=   3.9s\n",
      "[CV 1/5] END C=0.13517539668499592, l1_ratio=0.7945627717582758, penalty=l1, solver=saga;, score=-1.023 total time=   5.4s\n",
      "[CV 4/5] END C=3.168415741553067e-05, l1_ratio=0.6014088370818449, penalty=l1, solver=saga;, score=-2.310 total time=   4.5s\n",
      "[CV 5/5] END C=0.0010944462842573863, l1_ratio=0.07850312334621491, penalty=l1, solver=saga;, score=-1.316 total time=   4.8s\n",
      "[CV 1/5] END C=0.00534829433244113, l1_ratio=0.4770662488005405, penalty=l2, solver=saga;, score=-1.054 total time=   3.9s\n",
      "[CV 3/5] END C=0.00030334248083875175, l1_ratio=0.967595009327712, penalty=l2, solver=saga;, score=-3.656 total time=   4.0s\n",
      "[CV 2/5] END C=0.0001397032301613737, l1_ratio=0.0936664126761883, penalty=l2, solver=saga;, score=-0.304 total time=   4.0s\n",
      "[CV 3/5] END C=0.0001820188584119676, l1_ratio=0.01407277409207082, penalty=l2, solver=saga;, score=-1.861 total time=   4.0s\n",
      "[CV 1/5] END C=8.104807987917363, l1_ratio=0.1852164423679063, penalty=l2, solver=saga;, score=-1.035 total time=   4.0s\n",
      "[CV 5/5] END C=1.187745251728505e-06, l1_ratio=0.00036008331429404633, penalty=elasticnet, solver=saga;, score=-1.221 total time=   5.0s\n",
      "[CV 5/5] END C=7.51866387393607e-05, l1_ratio=0.055257308203277206, penalty=l1, solver=saga;, score=-2.154 total time=   4.5s\n",
      "[CV 3/5] END C=0.0001013622747322455, l1_ratio=0.027744140128616427, penalty=l2, solver=saga;, score=-2.333 total time=   4.0s\n",
      "[CV 2/5] END C=0.0026472565453628787, l1_ratio=0.11087469247892914, penalty=l1, solver=saga;, score=-1.048 total time=   4.9s\n",
      "[CV 4/5] END C=0.9918769360216578, l1_ratio=0.09156490293619421, penalty=l1, solver=saga;, score=-0.559 total time=   5.1s\n",
      "[CV 1/5] END C=26.971176066692006, l1_ratio=0.026897956907127048, penalty=l2, solver=saga;, score=-1.230 total time=   4.0s\n",
      "[CV 2/5] END C=1.113538326816699e-06, l1_ratio=0.019351609812173768, penalty=elasticnet, solver=saga;, score=-0.506 total time=   4.6s\n",
      "[CV 4/5] END C=1.3632557671340782e-06, l1_ratio=0.007828895688134784, penalty=l2, solver=saga;, score=-0.750 total time=   4.0s\n",
      "[CV 5/5] END C=2.874525954558476e-06, l1_ratio=0.5365263332818956, penalty=l1, solver=saga;, score=-0.764 total time=   4.5s\n",
      "[CV 4/5] END C=1.7257053176695378e-06, l1_ratio=0.9773701274420823, penalty=elasticnet, solver=saga;, score=-0.635 total time=   4.5s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.42654426332093837, penalty=elasticnet, solver=saga;, score=-0.631 total time=   4.5s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-0.474 total time=   4.0s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.044765656817050366, penalty=elasticnet, solver=saga;, score=-0.861 total time=   4.5s\n",
      "[CV 3/5] END C=1.5211501269043081e-06, l1_ratio=0.09444866847397723, penalty=elasticnet, solver=saga;, score=-0.776 total time=   4.5s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.2222249364311293, penalty=elasticnet, solver=saga;, score=-5.859 total time=   4.5s\n",
      "[CV 3/5] END C=2.1088332972597617e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-1.211 total time=   4.0s\n",
      "[CV 5/5] END C=0.0013353908501185876, l1_ratio=0.6322549928496771, penalty=l1, solver=saga;, score=-0.802 total time=   4.8s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.36025904059441, penalty=elasticnet, solver=saga;, score=-0.550 total time=   4.5s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.3898445302892915, penalty=elasticnet, solver=saga;, score=-0.792 total time=   4.5s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.1302390409272838, penalty=elasticnet, solver=saga;, score=-0.238 total time=   4.5s\n",
      "[CV 1/5] END C=39.154313760916295, l1_ratio=0.21909019135539431, penalty=l2, solver=saga;, score=-0.323 total time=   4.0s\n",
      "[CV 1/5] END C=9.36086184228762e-06, penalty=l2, solver=lbfgs;, score=-0.659 total time=   0.9s\n",
      "[CV 3/5] END C=1.2161722885563447e-06, penalty=l2, solver=lbfgs;, score=-0.667 total time=   0.7s\n",
      "[CV 3/5] END C=6.550622032760515e-05, penalty=l2, solver=newton-cg;, score=-0.634 total time=   2.3s\n",
      "[CV 3/5] END C=0.0015545115853648592, penalty=l2, solver=newton-cg;, score=-0.537 total time=   3.9s\n",
      "[CV 4/5] END C=0.00021951636072127157, penalty=l2, solver=newton-cg;, score=-0.596 total time=   2.8s\n",
      "[CV 5/5] END C=0.007049212372886417, penalty=l2, solver=newton-cg;, score=-0.493 total time=   6.2s\n",
      "[CV 1/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.424 total time=  25.3s\n",
      "[CV 1/5] END C=3.531636013544588, penalty=l2, solver=newton-cg;, score=-0.425 total time=  25.9s\n",
      "[CV 1/5] END C=9.07990255597525, penalty=l2, solver=newton-cg;, score=-0.424 total time=  28.0s\n",
      "[CV 3/5] END C=1.1919232573948424, penalty=l2, solver=lbfgs;, score=-0.533 total time=   0.9s\n",
      "[CV 4/5] END C=1.3775518449687307, penalty=l2, solver=newton-cg;, score=-0.418 total time=  24.0s\n",
      "[CV 1/5] END C=0.06425474817108065, penalty=l2, solver=newton-cg;, score=-0.447 total time=   8.7s\n",
      "[CV 4/5] END C=25.37331128014798, penalty=l2, solver=newton-cg;, score=-0.413 total time=  27.3s\n",
      "[CV 5/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.415 total time=  37.1s\n",
      "[CV 5/5] END C=0.00021416219013963825, penalty=l2, solver=lbfgs;, score=-0.576 total time=   0.8s\n",
      "[CV 2/5] END C=52.74307687576777, penalty=l2, solver=newton-cg;, score=-0.417 total time=  27.2s\n",
      "[CV 3/5] END C=32.47166281091174, penalty=l2, solver=newton-cg;, score=-0.425 total time=  23.9s\n",
      "[CV 5/5] END C=16.46375775183702, penalty=l2, solver=newton-cg;, score=-0.415 total time=  26.9s\n",
      "[CV 3/5] END C=0.00033716060099170316, penalty=l2, solver=lbfgs;, score=-0.569 total time=   0.9s\n",
      "[CV 3/5] END C=1.8304217860414715e-06, penalty=l2, solver=lbfgs;, score=-0.666 total time=   0.9s\n",
      "[CV 3/5] END C=23.572484624181357, penalty=l2, solver=newton-cg;, score=-0.425 total time=  27.6s\n",
      "[CV 3/5] END C=14.857871819134768, penalty=l2, solver=newton-cg;, score=-0.425 total time=  24.2s\n",
      "[CV 4/5] END C=23.08643993035239, penalty=l2, solver=newton-cg;, score=-0.413 total time=  26.4s\n",
      "[CV 1/5] END C=37.49015476724531, penalty=l2, solver=newton-cg;, score=-0.424 total time=  24.9s\n",
      "[CV 3/5] END C=1.6403783477783998e-05, penalty=l2, solver=lbfgs;, score=-0.648 total time=   0.8s\n",
      "[CV 2/5] END C=71.78305243868218, penalty=l2, solver=newton-cg;, score=-0.417 total time=  26.5s\n",
      "[CV 5/5] END C=12.405204992665801, penalty=l2, solver=newton-cg;, score=-0.415 total time=  23.3s\n",
      "[CV 5/5] END C=22.43475073033113, penalty=l2, solver=newton-cg;, score=-0.415 total time=  26.0s\n",
      "[CV 4/5] END C=25.19332308408921, penalty=l2, solver=newton-cg;, score=-0.413 total time=  28.2s\n",
      "[CV 4/5] END C=0.033798293944658546, l1_ratio=0.6660092185205861, penalty=l1, solver=saga;, score=-0.967 total time=   5.1s\n",
      "[CV 5/5] END C=1.1942306149170254, l1_ratio=0.5262314503703501, penalty=l2, solver=saga;, score=-1.325 total time=   4.0s\n",
      "[CV 2/5] END C=2.152452064249206e-05, l1_ratio=0.9522868587604243, penalty=l1, solver=saga;, score=-0.641 total time=   4.5s\n",
      "[CV 1/5] END C=3.168415741553067e-05, l1_ratio=0.6014088370818449, penalty=l1, solver=saga;, score=-9.251 total time=   4.5s\n",
      "[CV 3/5] END C=0.0010944462842573863, l1_ratio=0.07850312334621491, penalty=l1, solver=saga;, score=-0.412 total time=   4.8s\n",
      "[CV 2/5] END C=9.874513896646211e-05, l1_ratio=0.5486382640667588, penalty=l2, solver=saga;, score=-0.643 total time=   4.0s\n",
      "[CV 5/5] END C=0.00030334248083875175, l1_ratio=0.967595009327712, penalty=l2, solver=saga;, score=-2.329 total time=   4.0s\n",
      "[CV 5/5] END C=0.0001820188584119676, l1_ratio=0.01407277409207082, penalty=l2, solver=saga;, score=-0.365 total time=   3.9s\n",
      "[CV 4/5] END C=8.104807987917363, l1_ratio=0.1852164423679063, penalty=l2, solver=saga;, score=-1.004 total time=   4.0s\n",
      "[CV 3/5] END C=1.187745251728505e-06, l1_ratio=0.00036008331429404633, penalty=elasticnet, solver=saga;, score=-0.600 total time=   5.0s\n",
      "[CV 2/5] END C=0.0001013622747322455, l1_ratio=0.027744140128616427, penalty=l2, solver=saga;, score=-0.860 total time=   4.0s\n",
      "[CV 3/5] END C=0.0026472565453628787, l1_ratio=0.11087469247892914, penalty=l1, solver=saga;, score=-0.996 total time=   5.0s\n",
      "[CV 4/5] END C=26.971176066692006, l1_ratio=0.026897956907127048, penalty=l2, solver=saga;, score=-1.730 total time=   3.9s\n",
      "[CV 1/5] END C=1.113538326816699e-06, l1_ratio=0.019351609812173768, penalty=elasticnet, solver=saga;, score=-0.837 total time=   4.5s\n",
      "[CV 1/5] END C=1.3632557671340782e-06, l1_ratio=0.007828895688134784, penalty=l2, solver=saga;, score=-9.982 total time=   4.1s\n",
      "[CV 4/5] END C=49.91518968649303, l1_ratio=0.5367366541959938, penalty=l1, solver=saga;, score=-0.342 total time=   4.9s\n",
      "[CV 4/5] END C=1.0206800181729177e-06, l1_ratio=0.2805266152311468, penalty=elasticnet, solver=saga;, score=-0.462 total time=   4.5s\n",
      "[CV 2/5] END C=1.046741899032118e-06, l1_ratio=0.029691710002166867, penalty=elasticnet, solver=saga;, score=-0.663 total time=   4.5s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-0.913 total time=   4.0s\n",
      "[CV 1/5] END C=1.5211501269043081e-06, l1_ratio=0.09444866847397723, penalty=elasticnet, solver=saga;, score=-0.236 total time=   4.5s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.22239444246524223, penalty=elasticnet, solver=saga;, score=-1.338 total time=   4.5s\n",
      "[CV 2/5] END C=1.6551780295725852e-06, l1_ratio=0.13089958306254687, penalty=elasticnet, solver=saga;, score=-0.874 total time=   4.5s\n",
      "[CV 1/5] END C=50.573851988719106, l1_ratio=0.0, penalty=l2, solver=saga;, score=-0.770 total time=   4.0s\n",
      "[CV 2/5] END C=0.0013353908501185876, l1_ratio=0.6322549928496771, penalty=l1, solver=saga;, score=-0.891 total time=   4.9s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.36025904059441, penalty=elasticnet, solver=saga;, score=-1.077 total time=   4.5s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.3898445302892915, penalty=elasticnet, solver=saga;, score=-0.619 total time=   4.5s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.41007243225241313, penalty=l1, solver=saga;, score=-0.609 total time=   4.5s\n",
      "[CV 5/5] END C=4.564363662643437e-06, l1_ratio=0.3650926257419068, penalty=elasticnet, solver=saga;, score=-1.099 total time=   4.5s\n",
      "[CV 2/5] END C=39.154313760916295, l1_ratio=0.21909019135539431, penalty=l2, solver=saga;, score=-0.747 total time=   4.0s\n",
      "[CV 4/5] END C=20.41944230665062, penalty=l2, solver=lbfgs;, score=-0.526 total time=   0.9s\n",
      "[CV 1/5] END C=0.0015545115853648592, penalty=l2, solver=newton-cg;, score=-0.534 total time=   3.5s\n",
      "[CV 1/5] END C=0.007049212372886417, penalty=l2, solver=newton-cg;, score=-0.495 total time=   6.2s\n",
      "[CV 3/5] END C=15.350223919735981, penalty=l2, solver=newton-cg;, score=-0.425 total time=  27.3s\n",
      "[CV 2/5] END C=0.07438262337019581, penalty=l2, solver=lbfgs;, score=-0.601 total time=   0.8s\n",
      "[CV 5/5] END C=3.531636013544588, penalty=l2, solver=newton-cg;, score=-0.417 total time=  23.2s\n",
      "[CV 2/5] END C=99.97654290222484, penalty=l2, solver=lbfgs;, score=-0.533 total time=   0.9s\n",
      "[CV 2/5] END C=43.842781999862595, penalty=l2, solver=newton-cg;, score=-0.417 total time=  26.8s\n",
      "[CV 2/5] END C=1.0073007075897124e-06, penalty=l2, solver=newton-cg;, score=-0.674 total time=   1.4s\n",
      "[CV 4/5] END C=9.07990255597525, penalty=l2, solver=newton-cg;, score=-0.414 total time=  28.8s\n",
      "[CV 2/5] END C=1.1919232573948424, penalty=l2, solver=lbfgs;, score=-0.585 total time=   0.9s\n",
      "[CV 3/5] END C=25.624233672003008, penalty=l2, solver=newton-cg;, score=-0.425 total time=  23.8s\n",
      "[CV 4/5] END C=60.96870166912985, penalty=l2, solver=newton-cg;, score=-0.412 total time=  26.4s\n",
      "[CV 1/5] END C=25.37331128014798, penalty=l2, solver=newton-cg;, score=-0.424 total time=  27.1s\n",
      "[CV 4/5] END C=52.74307687576777, penalty=l2, solver=newton-cg;, score=-0.412 total time=  26.2s\n",
      "[CV 3/5] END C=18.93524522703305, penalty=l2, solver=newton-cg;, score=-0.425 total time=  38.4s\n",
      "[CV 2/5] END C=16.46375775183702, penalty=l2, solver=newton-cg;, score=-0.416 total time=  28.5s\n",
      "[CV 4/5] END C=0.00033716060099170316, penalty=l2, solver=lbfgs;, score=-0.569 total time=   0.9s\n",
      "[CV 5/5] END C=1.8304217860414715e-06, penalty=l2, solver=lbfgs;, score=-0.666 total time=   0.9s\n",
      "[CV 3/5] END C=65.69833852219975, penalty=l2, solver=newton-cg;, score=-0.425 total time=  23.2s\n",
      "[CV 4/5] END C=35.15322066119099, penalty=l2, solver=newton-cg;, score=-0.413 total time=  25.9s\n",
      "[CV 4/5] END C=14.857871819134768, penalty=l2, solver=newton-cg;, score=-0.413 total time=  25.4s\n",
      "[CV 5/5] END C=23.08643993035239, penalty=l2, solver=newton-cg;, score=-0.415 total time=  22.0s\n",
      "[CV 3/5] END C=37.49015476724531, penalty=l2, solver=newton-cg;, score=-0.425 total time=  25.6s\n",
      "[CV 5/5] END C=71.78305243868218, penalty=l2, solver=newton-cg;, score=-0.415 total time=  26.5s\n",
      "[CV 3/5] END C=21.10031343896973, penalty=l2, solver=newton-cg;, score=-0.425 total time=  24.2s\n",
      "[CV 2/5] END C=0.012419773574361123, penalty=l2, solver=lbfgs;, score=-0.532 total time=   0.9s\n",
      "[CV 2/5] END C=12.405204992665801, penalty=l2, solver=newton-cg;, score=-0.416 total time=  25.1s\n",
      "[CV 2/5] END C=5.851058017523036, penalty=l2, solver=newton-cg;, score=-0.417 total time=  26.1s\n",
      "[CV 2/5] END C=22.43475073033113, penalty=l2, solver=newton-cg;, score=-0.416 total time=  28.5s\n",
      "[CV 2/5] END C=25.19332308408921, penalty=l2, solver=newton-cg;, score=-0.417 total time=  27.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=0.000157216648268923, l1_ratio=0.031112134583001334, penalty=l2, solver=saga;, score=-0.565 total time=   4.1s\n",
      "[CV 5/5] END C=0.033798293944658546, l1_ratio=0.6660092185205861, penalty=l1, solver=saga;, score=-1.985 total time=   5.2s\n",
      "[CV 4/5] END C=0.13517539668499592, l1_ratio=0.7945627717582758, penalty=l1, solver=saga;, score=-0.189 total time=   5.4s\n",
      "[CV 3/5] END C=0.010884022395173353, l1_ratio=0.5628602866504493, penalty=elasticnet, solver=saga;, score=-0.943 total time=   5.0s\n",
      "[CV 1/5] END C=9.874513896646211e-05, l1_ratio=0.5486382640667588, penalty=l2, solver=saga;, score=-0.304 total time=   4.0s\n",
      "[CV 3/5] END C=0.0001397032301613737, l1_ratio=0.0936664126761883, penalty=l2, solver=saga;, score=-0.702 total time=   4.0s\n",
      "[CV 4/5] END C=0.0001820188584119676, l1_ratio=0.01407277409207082, penalty=l2, solver=saga;, score=-0.759 total time=   4.0s\n",
      "[CV 5/5] END C=29.995394909157383, l1_ratio=0.03350956435806217, penalty=l2, solver=saga;, score=-0.300 total time=   4.0s\n",
      "[CV 2/5] END C=8.104807987917363, l1_ratio=0.1852164423679063, penalty=l2, solver=saga;, score=-1.095 total time=   4.0s\n",
      "[CV 3/5] END C=7.51866387393607e-05, l1_ratio=0.055257308203277206, penalty=l1, solver=saga;, score=-7.960 total time=   4.6s\n",
      "[CV 1/5] END C=0.9918769360216578, l1_ratio=0.09156490293619421, penalty=l1, solver=saga;, score=-13.029 total time=   5.1s\n",
      "[CV 4/5] END C=0.00016561251427846333, l1_ratio=0.031365254082526633, penalty=l2, solver=saga;, score=-1.904 total time=   4.0s\n",
      "[CV 2/5] END C=26.971176066692006, l1_ratio=0.026897956907127048, penalty=l2, solver=saga;, score=-0.758 total time=   4.1s\n",
      "[CV 3/5] END C=1.113538326816699e-06, l1_ratio=0.019351609812173768, penalty=elasticnet, solver=saga;, score=-0.669 total time=   4.6s\n",
      "[CV 5/5] END C=0.008657125066312213, l1_ratio=0.010764208935954646, penalty=elasticnet, solver=saga;, score=-1.486 total time=   5.2s\n",
      "[CV 2/5] END C=1.3632557671340782e-06, l1_ratio=0.007828895688134784, penalty=l2, solver=saga;, score=-0.580 total time=   4.0s\n",
      "[CV 3/5] END C=49.91518968649303, l1_ratio=0.5367366541959938, penalty=l1, solver=saga;, score=-0.501 total time=   4.9s\n",
      "[CV 3/5] END C=1.7257053176695378e-06, l1_ratio=0.9773701274420823, penalty=elasticnet, solver=saga;, score=-0.975 total time=   4.5s\n",
      "[CV 5/5] END C=1.046741899032118e-06, l1_ratio=0.029691710002166867, penalty=elasticnet, solver=saga;, score=-1.147 total time=   4.5s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.42654426332093837, penalty=elasticnet, solver=saga;, score=-0.349 total time=   4.5s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-0.930 total time=   3.9s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.9056272035259453, penalty=elasticnet, solver=saga;, score=-0.562 total time=   4.5s\n",
      "[CV 5/5] END C=1.5211501269043081e-06, l1_ratio=0.09444866847397723, penalty=elasticnet, solver=saga;, score=-1.170 total time=   4.5s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.22239444246524223, penalty=elasticnet, solver=saga;, score=-0.474 total time=   4.5s\n",
      "[CV 4/5] END C=1.6551780295725852e-06, l1_ratio=0.13089958306254687, penalty=elasticnet, solver=saga;, score=-0.767 total time=   4.5s\n",
      "[CV 5/5] END C=83.31994480765619, l1_ratio=0.4369219655275994, penalty=l1, solver=saga;, score=-1.023 total time=   4.8s\n",
      "[CV 5/5] END C=50.573851988719106, l1_ratio=0.0, penalty=l2, solver=saga;, score=-1.756 total time=   4.0s\n",
      "[CV 5/5] END C=48.33564844672137, l1_ratio=0.60566725135925, penalty=l2, solver=saga;, score=-1.643 total time=   4.0s\n",
      "[CV 3/5] END C=0.0013353908501185876, l1_ratio=0.6322549928496771, penalty=l1, solver=saga;, score=-1.960 total time=   4.9s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.36025904059441, penalty=elasticnet, solver=saga;, score=-1.006 total time=   4.5s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.3898445302892915, penalty=elasticnet, solver=saga;, score=-1.255 total time=   4.5s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.41007243225241313, penalty=l1, solver=saga;, score=-6.257 total time=   4.5s\n",
      "[CV 3/5] END C=4.564363662643437e-06, l1_ratio=0.3650926257419068, penalty=elasticnet, solver=saga;, score=-1.172 total time=   4.5s\n",
      "[CV 5/5] END C=39.154313760916295, l1_ratio=0.21909019135539431, penalty=l2, solver=saga;, score=-0.253 total time=   4.0s\n",
      "[CV 1/5] END C=0.2775039396950022, penalty=l2, solver=newton-cg;, score=-0.431 total time=  12.6s\n",
      "[CV 1/5] END C=6.550622032760515e-05, penalty=l2, solver=newton-cg;, score=-0.633 total time=   2.1s\n",
      "[CV 1/5] END C=0.00021951636072127157, penalty=l2, solver=newton-cg;, score=-0.598 total time=   2.7s\n",
      "[CV 2/5] END C=0.007049212372886417, penalty=l2, solver=newton-cg;, score=-0.496 total time=   6.1s\n",
      "[CV 3/5] END C=0.44051095635231635, penalty=l2, solver=newton-cg;, score=-0.431 total time=  17.0s\n",
      "[CV 4/5] END C=15.350223919735981, penalty=l2, solver=newton-cg;, score=-0.413 total time=  27.0s\n",
      "[CV 5/5] END C=0.07438262337019581, penalty=l2, solver=lbfgs;, score=-0.530 total time=   0.9s\n",
      "[CV 2/5] END C=3.531636013544588, penalty=l2, solver=newton-cg;, score=-0.418 total time=  24.5s\n",
      "[CV 4/5] END C=99.97654290222484, penalty=l2, solver=lbfgs;, score=-0.527 total time=   0.9s\n",
      "[CV 5/5] END C=43.842781999862595, penalty=l2, solver=newton-cg;, score=-0.415 total time=  23.0s\n",
      "[CV 3/5] END C=0.0018437887099508744, penalty=l2, solver=lbfgs;, score=-0.533 total time=   1.0s\n",
      "[CV 2/5] END C=1.3775518449687307, penalty=l2, solver=newton-cg;, score=-0.420 total time=  26.0s\n",
      "[CV 5/5] END C=0.06425474817108065, penalty=l2, solver=newton-cg;, score=-0.445 total time=   8.7s\n",
      "[CV 5/5] END C=25.37331128014798, penalty=l2, solver=newton-cg;, score=-0.415 total time=  23.9s\n",
      "[CV 2/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.417 total time=  37.6s\n",
      "[CV 4/5] END C=18.93524522703305, penalty=l2, solver=newton-cg;, score=-0.413 total time=  39.4s\n",
      "[CV 1/5] END C=16.46375775183702, penalty=l2, solver=newton-cg;, score=-0.424 total time=  27.8s\n",
      "[CV 5/5] END C=0.00033716060099170316, penalty=l2, solver=lbfgs;, score=-0.565 total time=   0.9s\n",
      "[CV 1/5] END C=1.8304217860414715e-06, penalty=l2, solver=lbfgs;, score=-0.666 total time=   0.9s\n",
      "[CV 1/5] END C=65.69833852219975, penalty=l2, solver=newton-cg;, score=-0.424 total time=  25.4s\n",
      "[CV 5/5] END C=23.572484624181357, penalty=l2, solver=newton-cg;, score=-0.415 total time=  24.0s\n",
      "[CV 3/5] END C=35.15322066119099, penalty=l2, solver=newton-cg;, score=-0.425 total time=  24.6s\n",
      "[CV 2/5] END C=14.857871819134768, penalty=l2, solver=newton-cg;, score=-0.417 total time=  23.7s\n",
      "[CV 1/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.424 total time=  25.3s\n",
      "[CV 1/5] END C=1.6403783477783998e-05, penalty=l2, solver=lbfgs;, score=-0.646 total time=   0.8s\n",
      "[CV 1/5] END C=21.10031343896973, penalty=l2, solver=newton-cg;, score=-0.424 total time=  24.0s\n",
      "[CV 4/5] END C=5.851058017523036, penalty=l2, solver=newton-cg;, score=-0.414 total time=  25.3s\n",
      "[CV 5/5] END C=25.19332308408921, penalty=l2, solver=newton-cg;, score=-0.415 total time=  24.8s\n",
      "[CV 3/5] END C=0.033798293944658546, l1_ratio=0.6660092185205861, penalty=l1, solver=saga;, score=-2.294 total time=   5.1s\n",
      "[CV 3/5] END C=1.1942306149170254, l1_ratio=0.5262314503703501, penalty=l2, solver=saga;, score=-0.725 total time=   4.0s\n",
      "[CV 4/5] END C=2.152452064249206e-05, l1_ratio=0.9522868587604243, penalty=l1, solver=saga;, score=-1.617 total time=   4.5s\n",
      "[CV 3/5] END C=0.13517539668499592, l1_ratio=0.7945627717582758, penalty=l1, solver=saga;, score=-1.130 total time=   5.5s\n",
      "[CV 3/5] END C=3.168415741553067e-05, l1_ratio=0.6014088370818449, penalty=l1, solver=saga;, score=-1.288 total time=   4.5s\n",
      "[CV 1/5] END C=0.010884022395173353, l1_ratio=0.5628602866504493, penalty=elasticnet, solver=saga;, score=-12.218 total time=   5.0s\n",
      "[CV 4/5] END C=0.00534829433244113, l1_ratio=0.4770662488005405, penalty=l2, solver=saga;, score=-0.190 total time=   3.9s\n",
      "[CV 5/5] END C=9.874513896646211e-05, l1_ratio=0.5486382640667588, penalty=l2, solver=saga;, score=-1.710 total time=   4.0s\n",
      "[CV 1/5] END C=0.0001397032301613737, l1_ratio=0.0936664126761883, penalty=l2, solver=saga;, score=-1.407 total time=   4.0s\n",
      "[CV 2/5] END C=0.0001820188584119676, l1_ratio=0.01407277409207082, penalty=l2, solver=saga;, score=-0.498 total time=   4.0s\n",
      "[CV 4/5] END C=29.995394909157383, l1_ratio=0.03350956435806217, penalty=l2, solver=saga;, score=-1.297 total time=   4.0s\n",
      "[CV 1/5] END C=7.51866387393607e-05, l1_ratio=0.055257308203277206, penalty=l1, solver=saga;, score=-1.432 total time=   4.6s\n",
      "[CV 1/5] END C=0.0026472565453628787, l1_ratio=0.11087469247892914, penalty=l1, solver=saga;, score=-1.085 total time=   5.0s\n",
      "[CV 2/5] END C=0.00016561251427846333, l1_ratio=0.031365254082526633, penalty=l2, solver=saga;, score=-0.767 total time=   4.0s\n",
      "[CV 4/5] END C=74.92030968067643, l1_ratio=0.9929613368809689, penalty=elasticnet, solver=saga;, score=-3.566 total time=   5.0s\n",
      "[CV 2/5] END C=0.008657125066312213, l1_ratio=0.010764208935954646, penalty=elasticnet, solver=saga;, score=-0.911 total time=   5.2s\n",
      "[CV 3/5] END C=1.3632557671340782e-06, l1_ratio=0.007828895688134784, penalty=l2, solver=saga;, score=-0.552 total time=   4.0s\n",
      "[CV 2/5] END C=2.874525954558476e-06, l1_ratio=0.5365263332818956, penalty=l1, solver=saga;, score=-0.564 total time=   4.5s\n",
      "[CV 5/5] END C=49.91518968649303, l1_ratio=0.5367366541959938, penalty=l1, solver=saga;, score=-1.578 total time=   4.9s\n",
      "[CV 5/5] END C=1.0206800181729177e-06, l1_ratio=0.2805266152311468, penalty=elasticnet, solver=saga;, score=-0.735 total time=   4.5s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.42654426332093837, penalty=elasticnet, solver=saga;, score=-0.982 total time=   4.5s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.9056272035259453, penalty=elasticnet, solver=saga;, score=-5.841 total time=   4.5s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.044765656817050366, penalty=elasticnet, solver=saga;, score=-0.291 total time=   4.5s\n",
      "[CV 1/5] END C=1.6551780295725852e-06, l1_ratio=0.13089958306254687, penalty=elasticnet, solver=saga;, score=-3.524 total time=   4.5s\n",
      "[CV 3/5] END C=83.31994480765619, l1_ratio=0.4369219655275994, penalty=l1, solver=saga;, score=-0.796 total time=   4.9s\n",
      "[CV 4/5] END C=1e-06, l1_ratio=0.2222249364311293, penalty=elasticnet, solver=saga;, score=-0.705 total time=   4.5s\n",
      "[CV 5/5] END C=2.1088332972597617e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-1.525 total time=   3.9s\n",
      "[CV 1/5] END C=0.0013353908501185876, l1_ratio=0.6322549928496771, penalty=l1, solver=saga;, score=-17.527 total time=   4.9s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.1302390409272838, penalty=elasticnet, solver=saga;, score=-0.702 total time=   4.5s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.41007243225241313, penalty=l1, solver=saga;, score=-0.564 total time=   4.5s\n",
      "[CV 5/5] END C=20.41944230665062, penalty=l2, solver=lbfgs;, score=-0.528 total time=   0.9s\n",
      "[CV 5/5] END C=9.36086184228762e-06, penalty=l2, solver=lbfgs;, score=-0.660 total time=   0.9s\n",
      "[CV 2/5] END C=1.2161722885563447e-06, penalty=l2, solver=lbfgs;, score=-0.674 total time=   0.9s\n",
      "[CV 2/5] END C=0.26518016158591345, penalty=l2, solver=newton-cg;, score=-0.428 total time=  12.7s\n",
      "[CV 5/5] END C=6.550622032760515e-05, penalty=l2, solver=newton-cg;, score=-0.631 total time=   1.8s\n",
      "[CV 3/5] END C=0.00021951636072127157, penalty=l2, solver=newton-cg;, score=-0.599 total time=   2.8s\n",
      "[CV 1/5] END C=0.44051095635231635, penalty=l2, solver=newton-cg;, score=-0.429 total time=  17.4s\n",
      "[CV 5/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.415 total time=  26.6s\n",
      "[CV 1/5] END C=0.07438262337019581, penalty=l2, solver=lbfgs;, score=-0.528 total time=   0.9s\n",
      "[CV 1/5] END C=99.97654290222484, penalty=l2, solver=lbfgs;, score=-0.528 total time=   0.9s\n",
      "[CV 3/5] END C=43.842781999862595, penalty=l2, solver=newton-cg;, score=-0.425 total time=  26.2s\n",
      "[CV 1/5] END C=1.0073007075897124e-06, penalty=l2, solver=newton-cg;, score=-0.667 total time=   1.3s\n",
      "[CV 5/5] END C=9.07990255597525, penalty=l2, solver=newton-cg;, score=-0.415 total time=  27.1s\n",
      "[CV 4/5] END C=1.1919232573948424, penalty=l2, solver=lbfgs;, score=-0.526 total time=   0.9s\n",
      "[CV 4/5] END C=25.624233672003008, penalty=l2, solver=newton-cg;, score=-0.413 total time=  27.7s\n",
      "[CV 5/5] END C=1.3775518449687307, penalty=l2, solver=newton-cg;, score=-0.420 total time=  23.0s\n",
      "[CV 4/5] END C=0.06425474817108065, penalty=l2, solver=newton-cg;, score=-0.446 total time=   8.6s\n",
      "[CV 1/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.424 total time=  35.9s\n",
      "[CV 1/5] END C=52.74307687576777, penalty=l2, solver=newton-cg;, score=-0.424 total time=  28.5s\n",
      "[CV 2/5] END C=18.93524522703305, penalty=l2, solver=newton-cg;, score=-0.417 total time=  38.7s\n",
      "[CV 5/5] END C=32.47166281091174, penalty=l2, solver=newton-cg;, score=-0.415 total time=  26.0s\n",
      "[CV 3/5] END C=16.46375775183702, penalty=l2, solver=newton-cg;, score=-0.425 total time=  27.2s\n",
      "[CV 1/5] END C=61.12377065065248, penalty=l2, solver=newton-cg;, score=-0.424 total time=  26.0s\n",
      "[CV 4/5] END C=32.737842063919054, penalty=l2, solver=newton-cg;, score=-0.413 total time=  27.1s\n",
      "[CV 1/5] END C=14.857871819134768, penalty=l2, solver=newton-cg;, score=-0.424 total time=  26.2s\n",
      "[CV 5/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.415 total time=  26.6s\n",
      "[CV 2/5] END C=37.49015476724531, penalty=l2, solver=newton-cg;, score=-0.417 total time=  26.1s\n",
      "[CV 1/5] END C=71.78305243868218, penalty=l2, solver=newton-cg;, score=-0.424 total time=  27.0s\n",
      "[CV 5/5] END C=21.10031343896973, penalty=l2, solver=newton-cg;, score=-0.415 total time=  23.0s\n",
      "[CV 5/5] END C=0.012419773574361123, penalty=l2, solver=lbfgs;, score=-0.527 total time=   0.9s\n",
      "[CV 1/5] END C=22.43475073033113, penalty=l2, solver=newton-cg;, score=-0.424 total time=  26.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=0.033798293944658546, l1_ratio=0.6660092185205861, penalty=l1, solver=saga;, score=-1.336 total time=   5.1s\n",
      "[CV 2/5] END C=1.1942306149170254, l1_ratio=0.5262314503703501, penalty=l2, solver=saga;, score=-0.549 total time=   3.9s\n",
      "[CV 3/5] END C=2.152452064249206e-05, l1_ratio=0.9522868587604243, penalty=l1, solver=saga;, score=-1.895 total time=   4.5s\n",
      "[CV 5/5] END C=3.168415741553067e-05, l1_ratio=0.6014088370818449, penalty=l1, solver=saga;, score=-2.975 total time=   4.5s\n",
      "[CV 2/5] END C=0.0010944462842573863, l1_ratio=0.07850312334621491, penalty=l1, solver=saga;, score=-1.120 total time=   4.8s\n",
      "[CV 3/5] END C=9.874513896646211e-05, l1_ratio=0.5486382640667588, penalty=l2, solver=saga;, score=-0.739 total time=   4.0s\n",
      "[CV 2/5] END C=0.00030334248083875175, l1_ratio=0.967595009327712, penalty=l2, solver=saga;, score=-0.619 total time=   4.0s\n",
      "[CV 3/5] END C=29.995394909157383, l1_ratio=0.03350956435806217, penalty=l2, solver=saga;, score=-0.798 total time=   4.0s\n",
      "[CV 4/5] END C=1.187745251728505e-06, l1_ratio=0.00036008331429404633, penalty=elasticnet, solver=saga;, score=-1.512 total time=   5.0s\n",
      "[CV 1/5] END C=0.0001013622747322455, l1_ratio=0.027744140128616427, penalty=l2, solver=saga;, score=-0.319 total time=   4.0s\n",
      "[CV 2/5] END C=0.9918769360216578, l1_ratio=0.09156490293619421, penalty=l1, solver=saga;, score=-1.262 total time=   5.2s\n",
      "[CV 3/5] END C=0.00016561251427846333, l1_ratio=0.031365254082526633, penalty=l2, solver=saga;, score=-0.980 total time=   4.0s\n",
      "[CV 3/5] END C=26.971176066692006, l1_ratio=0.026897956907127048, penalty=l2, solver=saga;, score=-0.807 total time=   4.0s\n",
      "[CV 3/5] END C=74.92030968067643, l1_ratio=0.9929613368809689, penalty=elasticnet, solver=saga;, score=-1.085 total time=   4.9s\n",
      "[CV 3/5] END C=0.008657125066312213, l1_ratio=0.010764208935954646, penalty=elasticnet, solver=saga;, score=-1.784 total time=   5.2s\n",
      "[CV 3/5] END C=2.874525954558476e-06, l1_ratio=0.5365263332818956, penalty=l1, solver=saga;, score=-0.499 total time=   4.5s\n",
      "[CV 2/5] END C=49.91518968649303, l1_ratio=0.5367366541959938, penalty=l1, solver=saga;, score=-0.590 total time=   4.9s\n",
      "[CV 3/5] END C=1.0206800181729177e-06, l1_ratio=0.2805266152311468, penalty=elasticnet, solver=saga;, score=-0.693 total time=   4.5s\n",
      "[CV 2/5] END C=1.7257053176695378e-06, l1_ratio=0.9773701274420823, penalty=elasticnet, solver=saga;, score=-0.817 total time=   4.5s\n",
      "[CV 5/5] END C=1e-06, l1_ratio=0.42654426332093837, penalty=elasticnet, solver=saga;, score=-1.370 total time=   4.5s\n",
      "[CV 3/5] END C=1e-06, l1_ratio=0.9056272035259453, penalty=elasticnet, solver=saga;, score=-0.637 total time=   4.5s\n",
      "[CV 2/5] END C=1e-06, l1_ratio=0.044765656817050366, penalty=elasticnet, solver=saga;, score=-0.531 total time=   4.6s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.22239444246524223, penalty=elasticnet, solver=saga;, score=-0.831 total time=   4.5s\n",
      "[CV 5/5] END C=1.6551780295725852e-06, l1_ratio=0.13089958306254687, penalty=elasticnet, solver=saga;, score=-0.709 total time=   4.5s\n",
      "[CV 1/5] END C=2.1088332972597617e-06, l1_ratio=0.0, penalty=elasticnet, solver=saga;, score=-2.169 total time=   4.0s\n",
      "[CV 4/5] END C=50.573851988719106, l1_ratio=0.0, penalty=l2, solver=saga;, score=-1.192 total time=   3.9s\n",
      "[CV 3/5] END C=48.33564844672137, l1_ratio=0.60566725135925, penalty=l2, solver=saga;, score=-1.233 total time=   4.0s\n",
      "[CV 1/5] END C=1e-06, l1_ratio=0.3898445302892915, penalty=elasticnet, solver=saga;, score=-0.964 total time=   4.5s\n",
      "[CV 1/5] END C=4.564363662643437e-06, l1_ratio=0.3650926257419068, penalty=elasticnet, solver=saga;, score=-11.112 total time=   4.5s\n",
      "[CV 3/5] END C=39.154313760916295, l1_ratio=0.21909019135539431, penalty=l2, solver=saga;, score=-1.188 total time=   4.0s\n",
      "[CV 3/5] END C=20.41944230665062, penalty=l2, solver=lbfgs;, score=-0.533 total time=   0.9s\n",
      "[CV 2/5] END C=0.2775039396950022, penalty=l2, solver=newton-cg;, score=-0.428 total time=  13.7s\n",
      "[CV 4/5] END C=9.36086184228762e-06, penalty=l2, solver=lbfgs;, score=-0.656 total time=   0.8s\n",
      "[CV 4/5] END C=1.2161722885563447e-06, penalty=l2, solver=lbfgs;, score=-0.664 total time=   0.8s\n",
      "[CV 4/5] END C=0.26518016158591345, penalty=l2, solver=newton-cg;, score=-0.427 total time=  13.1s\n",
      "[CV 5/5] END C=0.0015545115853648592, penalty=l2, solver=newton-cg;, score=-0.532 total time=   3.7s\n",
      "[CV 5/5] END C=0.00021951636072127157, penalty=l2, solver=newton-cg;, score=-0.595 total time=   2.6s\n",
      "[CV 2/5] END C=0.44051095635231635, penalty=l2, solver=newton-cg;, score=-0.425 total time=  17.0s\n",
      "[CV 4/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.413 total time=  26.4s\n",
      "[CV 4/5] END C=3.531636013544588, penalty=l2, solver=newton-cg;, score=-0.415 total time=  22.4s\n",
      "[CV 2/5] END C=0.0018437887099508744, penalty=l2, solver=lbfgs;, score=-0.538 total time=   1.0s\n",
      "[CV 4/5] END C=1.0073007075897124e-06, penalty=l2, solver=newton-cg;, score=-0.664 total time=   1.4s\n",
      "[CV 1/5] END C=1.1919232573948424, penalty=l2, solver=lbfgs;, score=-0.533 total time=   0.9s\n",
      "[CV 2/5] END C=25.624233672003008, penalty=l2, solver=newton-cg;, score=-0.417 total time=  25.0s\n",
      "[CV 3/5] END C=1.3775518449687307, penalty=l2, solver=newton-cg;, score=-0.428 total time=  23.9s\n",
      "[CV 3/5] END C=60.96870166912985, penalty=l2, solver=newton-cg;, score=-0.425 total time=  25.5s\n",
      "[CV 3/5] END C=0.06425474817108065, penalty=l2, solver=newton-cg;, score=-0.450 total time=   8.9s\n",
      "[CV 2/5] END C=25.37331128014798, penalty=l2, solver=newton-cg;, score=-0.416 total time=  27.7s\n",
      "[CV 4/5] END C=100.0, penalty=l2, solver=newton-cg;, score=-0.413 total time=  36.9s\n",
      "[CV 2/5] END C=0.00021416219013963825, penalty=l2, solver=lbfgs;, score=-0.585 total time=   0.8s\n",
      "[CV 5/5] END C=52.74307687576777, penalty=l2, solver=newton-cg;, score=-0.415 total time=  25.6s\n",
      "[CV 5/5] END C=18.93524522703305, penalty=l2, solver=newton-cg;, score=-0.415 total time=  36.6s\n",
      "[CV 4/5] END C=32.47166281091174, penalty=l2, solver=newton-cg;, score=-0.413 total time=  26.3s\n",
      "[CV 4/5] END C=61.12377065065248, penalty=l2, solver=newton-cg;, score=-0.412 total time=  26.6s\n",
      "[CV 1/5] END C=32.737842063919054, penalty=l2, solver=newton-cg;, score=-0.424 total time=  26.6s\n",
      "[CV 4/5] END C=65.69833852219975, penalty=l2, solver=newton-cg;, score=-0.412 total time=  26.3s\n",
      "[CV 1/5] END C=35.15322066119099, penalty=l2, solver=newton-cg;, score=-0.424 total time=  25.4s\n",
      "[CV 4/5] END C=37.49015476724531, penalty=l2, solver=newton-cg;, score=-0.413 total time=  23.4s\n",
      "[CV 5/5] END C=1.6403783477783998e-05, penalty=l2, solver=lbfgs;, score=-0.648 total time=   0.8s\n",
      "[CV 2/5] END C=21.10031343896973, penalty=l2, solver=newton-cg;, score=-0.417 total time=  23.9s\n",
      "[CV 1/5] END C=12.405204992665801, penalty=l2, solver=newton-cg;, score=-0.424 total time=  25.3s\n",
      "[CV 3/5] END C=25.19332308408921, penalty=l2, solver=newton-cg;, score=-0.425 total time=  27.4s\n"
     ]
    }
   ],
   "source": [
    "lg_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19a7f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = f\"lg_pipeline.pkl\"\n",
    "\n",
    "with open(pickle_path, \"wb\") as f:\n",
    "    pickle.dump(lg_pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_xgb_clf = CalibratedClassifierCV(lg_pipeline.steps[1][1].best_estimator_, \n",
    "                                            method = 'sigmoid', cv=\"prefit\")\n",
    "\n",
    "calibrated_xgb_clf.fit(feature_pipeline.transform(X_val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfd84c",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb90a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lg_pipe_pred = lg_pipeline.predict_proba(X_test)\n",
    "y_train_lg_pipe_pred = lg_pipeline.predict_proba(X_train)\n",
    "y_val_lg_pipe_pred = lg_pipeline.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bef36c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9032241995429642\n",
      "0.9005316467580647\n",
      "0.8829989660003793\n"
     ]
    }
   ],
   "source": [
    "for i in xgb_metrics:\n",
    "    fpr, tpr, thresholds = roc_curve(i[0], i[1])\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bcb7b9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "fill": "tozeroy",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          1.8792753514244906e-05,
          1.8792753514244906e-05,
          9.396376757122454e-05,
          9.396376757122454e-05,
          0.0002443057956851838,
          0.0002443057956851838,
          0.0002630985491994287,
          0.0002630985491994287,
          0.0003194768097421634,
          0.0003194768097421634,
          0.0003570623167706532,
          0.0003570623167706532,
          0.00037585507028489815,
          0.00037585507028489815,
          0.0004698188378561227,
          0.0004698188378561227,
          0.0004886115913703676,
          0.0004886115913703676,
          0.000601368112455837,
          0.000601368112455837,
          0.0008832594151695106,
          0.0008832594151695106,
          0.0009396376757122454,
          0.0009396376757122454,
          0.0009772231827407352,
          0.0009772231827407352,
          0.00099601593625498,
          0.00099601593625498,
          0.0011087724573404494,
          0.0011087724573404494,
          0.0011839434713974291,
          0.0011839434713974291,
          0.0012779072389686537,
          0.0012779072389686537,
          0.0012966999924828985,
          0.0012966999924828985,
          0.0013154927459971435,
          0.0013154927459971435,
          0.0013530782530256334,
          0.0013530782530256334,
          0.001409456513568368,
          0.001409456513568368,
          0.0015222130346538374,
          0.0015222130346538374,
          0.0015410057881680825,
          0.0015410057881680825,
          0.001804104337367511,
          0.001804104337367511,
          0.001973239118995715,
          0.001973239118995715,
          0.00199203187250996,
          0.00199203187250996,
          0.0020296173795384498,
          0.0020296173795384498,
          0.00206720288656694,
          0.00206720288656694,
          0.0021047883935954295,
          0.0021047883935954295,
          0.0021235811471096745,
          0.0021235811471096745,
          0.002161166654138164,
          0.002161166654138164,
          0.002217544914680899,
          0.002217544914680899,
          0.002255130421709389,
          0.002255130421709389,
          0.0023490941892806132,
          0.0023490941892806132,
          0.0023678869427948583,
          0.0023678869427948583,
          0.002424265203337593,
          0.002424265203337593,
          0.002443057956851838,
          0.002443057956851838,
          0.0024806434638803276,
          0.0024806434638803276,
          0.002593399984965797,
          0.002593399984965797,
          0.002612192738480042,
          0.002612192738480042,
          0.002630985491994287,
          0.002630985491994287,
          0.002649778245508532,
          0.002649778245508532,
          0.002969255055250695,
          0.002969255055250695,
          0.0031383898368788996,
          0.0031383898368788996,
          0.0031571825903931442,
          0.0031571825903931442,
          0.003232353604450124,
          0.003232353604450124,
          0.003251146357964369,
          0.003251146357964369,
          0.0032887318649928586,
          0.0032887318649928586,
          0.0033451101255355933,
          0.0033451101255355933,
          0.0034766594001353077,
          0.0034766594001353077,
          0.003702172442306247,
          0.003702172442306247,
          0.003777343456363226,
          0.003777343456363226,
          0.003814928963391716,
          0.003814928963391716,
          0.0040592347590768995,
          0.0040592347590768995,
          0.004115613019619635,
          0.004115613019619635,
          0.0044162970758475535,
          0.0044162970758475535,
          0.004472675336390288,
          0.004472675336390288,
          0.004491468089904533,
          0.004491468089904533,
          0.004529053596933023,
          0.004529053596933023,
          0.004623017364504247,
          0.004623017364504247,
          0.0046981883785612265,
          0.0046981883785612265,
          0.0047169811320754715,
          0.0047169811320754715,
          0.004848530406675186,
          0.004848530406675186,
          0.004867323160189431,
          0.004867323160189431,
          0.00494249417424641,
          0.00494249417424641,
          0.00505525069533188,
          0.00505525069533188,
          0.005074043448846125,
          0.005074043448846125,
          0.005205592723445839,
          0.005205592723445839,
          0.005224385476960084,
          0.005224385476960084,
          0.005261970983988574,
          0.005261970983988574,
          0.005280763737502819,
          0.005280763737502819,
          0.0053747275050740435,
          0.0053747275050740435,
          0.0053935202585882885,
          0.0053935202585882885,
          0.005431105765616778,
          0.005431105765616778,
          0.005468691272645268,
          0.005468691272645268,
          0.005506276779673758,
          0.005506276779673758,
          0.005525069533188003,
          0.005525069533188003,
          0.005637826054273472,
          0.005637826054273472,
          0.005656618807787717,
          0.005656618807787717,
          0.005675411561301962,
          0.005675411561301962,
          0.00608885213861535,
          0.00608885213861535,
          0.006201608659700819,
          0.006201608659700819,
          0.006389536194843269,
          0.006389536194843269,
          0.006408328948357514,
          0.006408328948357514,
          0.006445914455386003,
          0.006445914455386003,
          0.006896940539727881,
          0.006896940539727881,
          0.0069909043072991055,
          0.0069909043072991055,
          0.007047282567841841,
          0.007047282567841841,
          0.007235210102984289,
          0.007235210102984289,
          0.007254002856498534,
          0.007254002856498534,
          0.007291588363527024,
          0.007291588363527024,
          0.007592272419754942,
          0.007592272419754942,
          0.0076486506802976775,
          0.0076486506802976775,
          0.0076674434338119225,
          0.0076674434338119225,
          0.007686236187326167,
          0.007686236187326167,
          0.007723821694354657,
          0.007723821694354657,
          0.007780199954897392,
          0.007780199954897392,
          0.007949334736525596,
          0.007949334736525596,
          0.00800571299706833,
          0.00800571299706833,
          0.008062091257611065,
          0.008062091257611065,
          0.008212433285725025,
          0.008212433285725025,
          0.008250018792753513,
          0.008250018792753513,
          0.008400360820867474,
          0.008400360820867474,
          0.008701044877095393,
          0.008701044877095393,
          0.008795008644666617,
          0.008795008644666617,
          0.008813801398180861,
          0.008813801398180861,
          0.008907765165752085,
          0.008907765165752085,
          0.008926557919266331,
          0.008926557919266331,
          0.008945350672780576,
          0.008945350672780576,
          0.008964143426294821,
          0.008964143426294821,
          0.00907689994738029,
          0.00907689994738029,
          0.009095692700894536,
          0.009095692700894536,
          0.00911448545440878,
          0.00911448545440878,
          0.009133278207923024,
          0.009133278207923024,
          0.00918965646846576,
          0.00918965646846576,
          0.009264827482522739,
          0.009264827482522739,
          0.009283620236036984,
          0.009283620236036984,
          0.009302412989551229,
          0.009302412989551229,
          0.009527926031722167,
          0.009527926031722167,
          0.009659475306321882,
          0.009659475306321882,
          0.009697060813350372,
          0.009697060813350372,
          0.00988498834849282,
          0.00988498834849282,
          0.009903781102007066,
          0.009903781102007066,
          0.010054123130121025,
          0.010054123130121025,
          0.010091708637149515,
          0.010091708637149515,
          0.01011050139066376,
          0.01011050139066376,
          0.010204465158234985,
          0.010204465158234985,
          0.01022325791174923,
          0.01022325791174923,
          0.0103360144328347,
          0.0103360144328347,
          0.010354807186348944,
          0.010354807186348944,
          0.010411185446891678,
          0.010411185446891678,
          0.010467563707434414,
          0.010467563707434414,
          0.010561527475005638,
          0.010561527475005638,
          0.010711869503119597,
          0.010711869503119597,
          0.010768247763662331,
          0.010768247763662331,
          0.010843418777719311,
          0.010843418777719311,
          0.010918589791776291,
          0.010918589791776291,
          0.010937382545290536,
          0.010937382545290536,
          0.01103134631286176,
          0.01103134631286176,
          0.011050139066376006,
          0.011050139066376006,
          0.011087724573404496,
          0.011087724573404496,
          0.011369615876118169,
          0.011369615876118169,
          0.011745470946403066,
          0.011745470946403066,
          0.011801849206945802,
          0.011801849206945802,
          0.011895812974517027,
          0.011895812974517027,
          0.012046155002630985,
          0.012046155002630985,
          0.012064947756145231,
          0.012064947756145231,
          0.012102533263173721,
          0.012102533263173721,
          0.012121326016687965,
          0.012121326016687965,
          0.01214011877020221,
          0.01214011877020221,
          0.012234082537773434,
          0.012234082537773434,
          0.012271668044801924,
          0.012271668044801924,
          0.01229046079831617,
          0.01229046079831617,
          0.012365631812373148,
          0.012365631812373148,
          0.012722694129143803,
          0.012722694129143803,
          0.012760279636172291,
          0.012760279636172291,
          0.012929414417800496,
          0.012929414417800496,
          0.013004585431857476,
          0.013004585431857476,
          0.01306096369240021,
          0.01306096369240021,
          0.013079756445914456,
          0.013079756445914456,
          0.013154927459971434,
          0.013154927459971434,
          0.013549575283770578,
          0.013549575283770578,
          0.013568368037284824,
          0.013568368037284824,
          0.013699917311884538,
          0.013699917311884538,
          0.013793881079455762,
          0.013793881079455762,
          0.013831466586484252,
          0.013831466586484252,
          0.013981808614598211,
          0.013981808614598211,
          0.01420732165676915,
          0.01420732165676915,
          0.014658347741111027,
          0.014658347741111027,
          0.014827482522739232,
          0.014827482522739232,
          0.01505299556491017,
          0.01505299556491017,
          0.01539126512816658,
          0.01539126512816658,
          0.015410057881680823,
          0.015410057881680823,
          0.01542885063519507,
          0.01542885063519507,
          0.01546643614222356,
          0.01546643614222356,
          0.015541607156280538,
          0.015541607156280538,
          0.015635570923851764,
          0.015635570923851764,
          0.015767120198451477,
          0.015767120198451477,
          0.0158610839660227,
          0.0158610839660227,
          0.01593625498007968,
          0.01593625498007968,
          0.016142975268736374,
          0.016142975268736374,
          0.016368488310907314,
          0.016368488310907314,
          0.016500037585507027,
          0.016500037585507027,
          0.016518830339021275,
          0.016518830339021275,
          0.016763136134706456,
          0.016763136134706456,
          0.016969856423363152,
          0.016969856423363152,
          0.016988649176877396,
          0.016988649176877396,
          0.01719536946553409,
          0.01719536946553409,
          0.01723295497256258,
          0.01723295497256258,
          0.017439675261219274,
          0.017439675261219274,
          0.017777944824475683,
          0.017777944824475683,
          0.017947079606103887,
          0.017947079606103887,
          0.01800345786664662,
          0.01800345786664662,
          0.01822897090881756,
          0.01822897090881756,
          0.018435691197474253,
          0.018435691197474253,
          0.01864241148613095,
          0.01864241148613095,
          0.018661204239645193,
          0.018661204239645193,
          0.01871758250018793,
          0.01871758250018793,
          0.018830339021273398,
          0.018830339021273398,
          0.019281365105615275,
          0.019281365105615275,
          0.01930015785912952,
          0.01930015785912952,
          0.019563256408328948,
          0.019563256408328948,
          0.019751183943471397,
          0.019751183943471397,
          0.01976997669698564,
          0.01976997669698564,
          0.020277381041870254,
          0.020277381041870254,
          0.0202961737953845,
          0.0202961737953845,
          0.020765992633240624,
          0.020765992633240624,
          0.020878749154326092,
          0.020878749154326092,
          0.02132977523866797,
          0.02132977523866797,
          0.021630459294895887,
          0.021630459294895887,
          0.02232579117492295,
          0.02232579117492295,
          0.022494925956551153,
          0.022494925956551153,
          0.022664060738179358,
          0.022664060738179358,
          0.022720438998722094,
          0.022720438998722094,
          0.023152672329549727,
          0.023152672329549727,
          0.023227843343606704,
          0.023227843343606704,
          0.023453356385777644,
          0.023453356385777644,
          0.023566112906863113,
          0.023566112906863113,
          0.023716454934977073,
          0.023716454934977073,
          0.02382921145606254,
          0.02382921145606254,
          0.023848004209576786,
          0.023848004209576786,
          0.02390438247011952,
          0.02390438247011952,
          0.023998346237690746,
          0.023998346237690746,
          0.024148688265804706,
          0.024148688265804706,
          0.024336615800947155,
          0.024336615800947155,
          0.02439299406148989,
          0.02439299406148989,
          0.02443057956851838,
          0.02443057956851838,
          0.024468165075546867,
          0.024468165075546867,
          0.0249003984063745,
          0.0249003984063745,
          0.025257460723145154,
          0.025257460723145154,
          0.0252762534766594,
          0.0252762534766594,
          0.02572727956100128,
          0.02572727956100128,
          0.025764865068029767,
          0.025764865068029767,
          0.02585882883560099,
          0.02585882883560099,
          0.026103134631286176,
          0.026103134631286176,
          0.0261970983988574,
          0.0261970983988574,
          0.02627226941291438,
          0.02627226941291438,
          0.02634744042697136,
          0.02634744042697136,
          0.02726828534916936,
          0.02726828534916936,
          0.02745621288431181,
          0.02745621288431181,
          0.02753138389836879,
          0.02753138389836879,
          0.027550176651883033,
          0.027550176651883033,
          0.02773810418702548,
          0.02773810418702548,
          0.02807637375028189,
          0.02807637375028189,
          0.028095166503796135,
          0.028095166503796135,
          0.028771705630308952,
          0.028771705630308952,
          0.028846876644365933,
          0.028846876644365933,
          0.028940840411937157,
          0.028940840411937157,
          0.028978425918965645,
          0.028978425918965645,
          0.029166353454108097,
          0.029166353454108097,
          0.02918514620762234,
          0.02918514620762234,
          0.02926031722167932,
          0.02926031722167932,
          0.02952341577087875,
          0.02952341577087875,
          0.029579794031421483,
          0.029579794031421483,
          0.029730136059535443,
          0.029730136059535443,
          0.029918063594677892,
          0.029918063594677892,
          0.0302563331579343,
          0.0302563331579343,
          0.030632188228219198,
          0.030632188228219198,
          0.03070735924227618,
          0.03070735924227618,
          0.030970457791475607,
          0.030970457791475607,
          0.031045628805532587,
          0.031045628805532587,
          0.031158385326618056,
          0.031158385326618056,
          0.031214763587160792,
          0.031214763587160792,
          0.031327520108246264,
          0.031327520108246264,
          0.031459069382845976,
          0.031459069382845976,
          0.031646996917988425,
          0.031646996917988425,
          0.03221077952341577,
          0.03221077952341577,
          0.03241749981207247,
          0.03241749981207247,
          0.032605427347214916,
          0.032605427347214916,
          0.03292490415695708,
          0.03292490415695708,
          0.03384574907915508,
          0.03384574907915508,
          0.03424039690295422,
          0.03424039690295422,
          0.034447117191610914,
          0.034447117191610914,
          0.034898143275952795,
          0.034898143275952795,
          0.03499210704352402,
          0.03499210704352402,
          0.03502969255055251,
          0.03502969255055251,
          0.035236412839209204,
          0.035236412839209204,
          0.03529279109975193,
          0.03529279109975193,
          0.03544313312786589,
          0.03544313312786589,
          0.03549951138840863,
          0.03549951138840863,
          0.036044501240321734,
          0.036044501240321734,
          0.036063293993835975,
          0.036063293993835975,
          0.0361572577614072,
          0.0361572577614072,
          0.03673983312034879,
          0.03673983312034879,
          0.03709689543711945,
          0.03709689543711945,
          0.03741637224686161,
          0.03741637224686161,
          0.03758550702848981,
          0.03758550702848981,
          0.03764188528903255,
          0.03764188528903255,
          0.03856273021123055,
          0.03856273021123055,
          0.03869427948583026,
          0.03869427948583026,
          0.03876945049988725,
          0.03876945049988725,
          0.038807036006915735,
          0.038807036006915735,
          0.03886341426745847,
          0.03886341426745847,
          0.03905134180260092,
          0.03905134180260092,
          0.03925806209125761,
          0.03925806209125761,
          0.039314440351800345,
          0.039314440351800345,
          0.03933323310531459,
          0.03933323310531459,
          0.039464782379914305,
          0.039464782379914305,
          0.039803051943170714,
          0.039803051943170714,
          0.03985943020371345,
          0.03985943020371345,
          0.04017890701345561,
          0.04017890701345561,
          0.04044200556265504,
          0.04044200556265504,
          0.04087423889348267,
          0.04087423889348267,
          0.04102458092159663,
          0.04102458092159663,
          0.041099751935653614,
          0.041099751935653614,
          0.04115613019619635,
          0.04115613019619635,
          0.041250093963767574,
          0.041250093963767574,
          0.041513192512967,
          0.041513192512967,
          0.041719912801623696,
          0.041719912801623696,
          0.041907840336766145,
          0.041907840336766145,
          0.042377659174622266,
          0.042377659174622266,
          0.04254679395625047,
          0.04254679395625047,
          0.0426031722167932,
          0.0426031722167932,
          0.04267834323085019,
          0.04267834323085019,
          0.04269713598436443,
          0.04269713598436443,
          0.04314816206870631,
          0.04314816206870631,
          0.04361798090656243,
          0.04361798090656243,
          0.04391866496279035,
          0.04391866496279035,
          0.04397504322333308,
          0.04397504322333308,
          0.04414417800496129,
          0.04414417800496129,
          0.044219349019018264,
          0.044219349019018264,
          0.04423814177253251,
          0.04423814177253251,
          0.044332105540103736,
          0.044332105540103736,
          0.04440727655416071,
          0.04440727655416071,
          0.04446365481470345,
          0.04446365481470345,
          0.044482447568217696,
          0.044482447568217696,
          0.04457641133578892,
          0.04457641133578892,
          0.044820717131474105,
          0.044820717131474105,
          0.045158986694730514,
          0.045158986694730514,
          0.04606103886341427,
          0.04606103886341427,
          0.046324137412613695,
          0.046324137412613695,
          0.04651206494775614,
          0.04651206494775614,
          0.04669999248289859,
          0.04669999248289859,
          0.04701946929264076,
          0.04701946929264076,
          0.04722618958129745,
          0.04722618958129745,
          0.04752687363752537,
          0.04752687363752537,
          0.04782755769375329,
          0.04782755769375329,
          0.04801548522889574,
          0.04801548522889574,
          0.04814703450349545,
          0.04814703450349545,
          0.048240998271066676,
          0.048240998271066676,
          0.0485416823272946,
          0.0485416823272946,
          0.048616853341351574,
          0.048616853341351574,
          0.048936330151093735,
          0.048936330151093735,
          0.04897391565812223,
          0.04897391565812223,
          0.049030293918664966,
          0.049030293918664966,
          0.04912425768623619,
          0.04912425768623619,
          0.04951890551003533,
          0.04951890551003533,
          0.04961286927760655,
          0.04961286927760655,
          0.049800796812749,
          0.049800796812749,
          0.0498383823197775,
          0.0498383823197775,
          0.05075922724197549,
          0.05075922724197549,
          0.05186799969931594,
          0.05186799969931594,
          0.05246936781177178,
          0.05246936781177178,
          0.0526009170863715,
          0.0526009170863715,
          0.052826430128542434,
          0.052826430128542434,
          0.05284522288205668,
          0.05284522288205668,
          0.05288280838908517,
          0.05288280838908517,
          0.053465383748026764,
          0.053465383748026764,
          0.05401037359993986,
          0.05401037359993986,
          0.05442381417725325,
          0.05442381417725325,
          0.05466811997293843,
          0.05466811997293843,
          0.05472449823348117,
          0.05472449823348117,
          0.055419830113508234,
          0.055419830113508234,
          0.055889648951364355,
          0.055889648951364355,
          0.0568292866270766,
          0.0568292866270766,
          0.05712997068330452,
          0.05712997068330452,
          0.057223934450875744,
          0.057223934450875744,
          0.05728031271141848,
          0.05728031271141848,
          0.05746824024656093,
          0.05746824024656093,
          0.05782530256333158,
          0.05782530256333158,
          0.057994437344959786,
          0.057994437344959786,
          0.059497857626099375,
          0.059497857626099375,
          0.059836127189355784,
          0.059836127189355784,
          0.06066300834398256,
          0.06066300834398256,
          0.06092610689318199,
          0.06092610689318199,
          0.06122679094940991,
          0.06122679094940991,
          0.061940915582951214,
          0.061940915582951214,
          0.06231677065323611,
          0.06231677065323611,
          0.0625234909418928,
          0.0625234909418928,
          0.06286176050514922,
          0.06286176050514922,
          0.0628993460121777,
          0.0628993460121777,
          0.06383898368788996,
          0.06383898368788996,
          0.06417725325114636,
          0.06417725325114636,
          0.0641960460046606,
          0.0641960460046606,
          0.06509809817334436,
          0.06509809817334436,
          0.06633841990528452,
          0.06633841990528452,
          0.06637600541231302,
          0.06637600541231302,
          0.06648876193339848,
          0.06648876193339848,
          0.06654514019394121,
          0.06654514019394121,
          0.06725926482748253,
          0.06725926482748253,
          0.06752236337668195,
          0.06752236337668195,
          0.06759753439073893,
          0.06759753439073893,
          0.0677102909118244,
          0.0677102909118244,
          0.06812373148913779,
          0.06812373148913779,
          0.06831165902428024,
          0.06831165902428024,
          0.06883785612267909,
          0.06883785612267909,
          0.0699278358265053,
          0.0699278358265053,
          0.07066075321356086,
          0.07066075321356086,
          0.07135608509358791,
          0.07135608509358791,
          0.07145004886115913,
          0.07145004886115913,
          0.07195745320604376,
          0.07195745320604376,
          0.07203262422010073,
          0.07203262422010073,
          0.07242727204389987,
          0.07242727204389987,
          0.07259640682552808,
          0.07259640682552808,
          0.07353604450124032,
          0.07353604450124032,
          0.0739870705855822,
          0.0739870705855822,
          0.07404344884612493,
          0.07404344884612493,
          0.07409982710666767,
          0.07409982710666767,
          0.0743629256558671,
          0.0743629256558671,
          0.07441930391640983,
          0.07441930391640983,
          0.0745320604374953,
          0.0745320604374953,
          0.07473878072615199,
          0.07473878072615199,
          0.07515222130346538,
          0.07515222130346538,
          0.07547169811320754,
          0.07547169811320754,
          0.0757911749229497,
          0.0757911749229497,
          0.07594151695106367,
          0.07594151695106367,
          0.07633616477486281,
          0.07633616477486281,
          0.07650529955649102,
          0.07650529955649102,
          0.07693753288731865,
          0.07693753288731865,
          0.07720063143651808,
          0.07720063143651808,
          0.07742614447868902,
          0.07742614447868902,
          0.07744493723220326,
          0.07744493723220326,
          0.07748252273923176,
          0.07748252273923176,
          0.0782154401262873,
          0.0782154401262873,
          0.07891077200631437,
          0.07891077200631437,
          0.07932421258362775,
          0.07932421258362775,
          0.07962489663985567,
          0.07962489663985567,
          0.07986920243554085,
          0.07986920243554085,
          0.0800947154777118,
          0.0800947154777118,
          0.08015109373825453,
          0.08015109373825453,
          0.08041419228745396,
          0.08041419228745396,
          0.08048936330151094,
          0.08048936330151094,
          0.08095918213936706,
          0.08095918213936706,
          0.08120348793505225,
          0.08120348793505225,
          0.08169209952642262,
          0.08169209952642262,
          0.08184244155453657,
          0.08184244155453657,
          0.08191761256859355,
          0.08191761256859355,
          0.08343982560324739,
          0.08343982560324739,
          0.08362775313838984,
          0.08362775313838984,
          0.08419153574381719,
          0.08419153574381719,
          0.08460497632113058,
          0.08460497632113058,
          0.08464256182815906,
          0.08464256182815906,
          0.08516875892655792,
          0.08516875892655792,
          0.08526272269412914,
          0.08526272269412914,
          0.08531910095467188,
          0.08531910095467188,
          0.08545065022927159,
          0.08545065022927159,
          0.08776215891152371,
          0.08776215891152371,
          0.08796887920018041,
          0.08796887920018041,
          0.08832594151695107,
          0.08832594151695107,
          0.08836352702397955,
          0.08836352702397955,
          0.08873938209426445,
          0.08873938209426445,
          0.08885213861534992,
          0.08885213861534992,
          0.0895098849883485,
          0.0895098849883485,
          0.09001728933323311,
          0.09001728933323311,
          0.09050590092460348,
          0.09050590092460348,
          0.09052469367811772,
          0.09052469367811772,
          0.0905998646921747,
          0.0905998646921747,
          0.09178380816357212,
          0.09178380816357212,
          0.09217845598737127,
          0.09217845598737127,
          0.09234759076899947,
          0.09234759076899947,
          0.09255431105765617,
          0.09255431105765617,
          0.09371946177553936,
          0.09371946177553936,
          0.09460272119070887,
          0.09460272119070887,
          0.09464030669773735,
          0.09464030669773735,
          0.09471547771179434,
          0.09471547771179434,
          0.09505374727505074,
          0.09505374727505074,
          0.09589942118319177,
          0.09589942118319177,
          0.09631286176050514,
          0.09631286176050514,
          0.09668871683079004,
          0.09672630233781854,
          0.0970833646545892,
          0.0970833646545892,
          0.09730887769676012,
          0.09730887769676012,
          0.09753439073893107,
          0.09753439073893107,
          0.09779748928813049,
          0.09779748928813049,
          0.09824851537247238,
          0.09824851537247238,
          0.10168758926557919,
          0.10168758926557919,
          0.10217620085694956,
          0.10217620085694956,
          0.1021949936104638,
          0.1021949936104638,
          0.10230775013154927,
          0.10230775013154927,
          0.1040742689618883,
          0.1040742689618883,
          0.10463805156731565,
          0.10463805156731565,
          0.10465684432082989,
          0.10465684432082989,
          0.10491994287002931,
          0.10491994287002931,
          0.10531459069382847,
          0.10531459069382847,
          0.10540855446139968,
          0.10540855446139968,
          0.10572803127114185,
          0.10572803127114185,
          0.1059159588062843,
          0.1059159588062843,
          0.1077952341577088,
          0.1077952341577088,
          0.10788919792528001,
          0.10788919792528001,
          0.10903555588964896,
          0.10903555588964896,
          0.1092234834247914,
          0.1092234834247914,
          0.10956175298804781,
          0.10956175298804781,
          0.11018191385401789,
          0.11018191385401789,
          0.11025708486807487,
          0.11025708486807487,
          0.11375253702172443,
          0.11375253702172443,
          0.11388408629632414,
          0.11388408629632414,
          0.11472976020446515,
          0.11472976020446515,
          0.11501165150717883,
          0.11501165150717883,
          0.1155002630985492,
          0.1155002630985492,
          0.11649627903480418,
          0.11649627903480418,
          0.1175110877245734,
          0.1175110877245734,
          0.11822521235811471,
          0.11822521235811471,
          0.11952191235059761,
          0.11952191235059761,
          0.119935352927911,
          0.119935352927911,
          0.12002931669548222,
          0.12002931669548222,
          0.1205179282868526,
          0.1205179282868526,
          0.12055551379388108,
          0.12055551379388108,
          0.12125084567390815,
          0.12125084567390815,
          0.12213410508907765,
          0.12213410508907765,
          0.12286702247613321,
          0.12286702247613321,
          0.1234120123280463,
          0.1234120123280463,
          0.12534766594001354,
          0.12534766594001354,
          0.126456438397354,
          0.126456438397354,
          0.12653160941141095,
          0.12653160941141095,
          0.1270953920168383,
          0.1270953920168383,
          0.1276779673757799,
          0.1276779673757799,
          0.1278471021574081,
          0.1278471021574081,
          0.12833571374877847,
          0.12833571374877847,
          0.12882432534014884,
          0.12882432534014884,
          0.13008343982560325,
          0.13008343982560325,
          0.13277080357814028,
          0.13277080357814028,
          0.1335413064722243,
          0.1335413064722243,
          0.13442456588739382,
          0.13442456588739382,
          0.13481921371119296,
          0.13481921371119296,
          0.13718710065398781,
          0.13718710065398781,
          0.1375629557242727,
          0.1375629557242727,
          0.14028790498383822,
          0.14028790498383822,
          0.14137788468766443,
          0.14137788468766443,
          0.1424490716379764,
          0.1424490716379764,
          0.143952491919116,
          0.143952491919116,
          0.14444110351048636,
          0.14444110351048636,
          0.1452867774186274,
          0.1452867774186274,
          0.14590693828459747,
          0.14590693828459747,
          0.14607607306622566,
          0.14607607306622566,
          0.1470532962489664,
          0.1470532962489664,
          0.1472224310305946,
          0.1472224310305946,
          0.14724122378410884,
          0.14724122378410884,
          0.14750432233330826,
          0.14750432233330826,
          0.14757949334736525,
          0.14757949334736525,
          0.14902653536796212,
          0.14902653536796212,
          0.14919567014959031,
          0.14919567014959031,
          0.15017289333233105,
          0.15017289333233105,
          0.15075546869127265,
          0.15075546869127265,
          0.15124408028264302,
          0.1512816657896715,
          0.15193941216267007,
          0.15193941216267007,
          0.15201458317672706,
          0.15201458317672706,
          0.15214613245132677,
          0.15214613245132677,
          0.1524844020145832,
          0.1524844020145832,
          0.153160941141096,
          0.153160941141096,
          0.15428850635195068,
          0.15428850635195068,
          0.1544200556265504,
          0.1544200556265504,
          0.15579192663309027,
          0.15579192663309027,
          0.15656242952717433,
          0.15656242952717433,
          0.1568067353228595,
          0.1568067353228595,
          0.1579343005337142,
          0.1579343005337142,
          0.15799067879425693,
          0.15799067879425693,
          0.16003908892730964,
          0.16003908892730964,
          0.16017063820190935,
          0.16017063820190935,
          0.16037735849056603,
          0.16037735849056603,
          0.16141095993384952,
          0.16141095993384952,
          0.16468089904532812,
          0.16468089904532812,
          0.16590242802375404,
          0.16590242802375404,
          0.16851462076223409,
          0.16851462076223409,
          0.16900323235360445,
          0.16900323235360445,
          0.17020596857851614,
          0.17020596857851614,
          0.17076975118394347,
          0.17076975118394347,
          0.1714462903104563,
          0.1714462903104563,
          0.17184093813425544,
          0.17184093813425544,
          0.17223558595805458,
          0.17223558595805458,
          0.17289333233105314,
          0.17289333233105314,
          0.1760129294144178,
          0.1760129294144178,
          0.1763887844847027,
          0.1763887844847027,
          0.17685860332255882,
          0.17685860332255882,
          0.1769713598436443,
          0.1769713598436443,
          0.17712170187175824,
          0.17712170187175824,
          0.17740359317447194,
          0.17740359317447194,
          0.17809892505449898,
          0.17809892505449898,
          0.18010974968052318,
          0.18010974968052318,
          0.1806547395324363,
          0.1806547395324363,
          0.18396226415094338,
          0.18396226415094338,
          0.1843757047282568,
          0.1843757047282568,
          0.18582274674885363,
          0.18582274674885363,
          0.1872697887694505,
          0.1872697887694505,
          0.1874013380440502,
          0.1874013380440502,
          0.18792753514244906,
          0.18792753514244906,
          0.19151695106366984,
          0.19151695106366984,
          0.1917048785988123,
          0.1917048785988123,
          0.19226866120423963,
          0.19226866120423963,
          0.19369691047132226,
          0.19369691047132226,
          0.19510636698489062,
          0.19510636698489062,
          0.19659099451251597,
          0.19659099451251597,
          0.19700443508982937,
          0.19700443508982937,
          0.19867699015259715,
          0.19867699015259715,
          0.2016274524543336,
          0.2016274524543336,
          0.2050101480868977,
          0.2050101480868977,
          0.20692700894535068,
          0.20692700894535068,
          0.20775389009997744,
          0.20775389009997744,
          0.20910696835300308,
          0.20910696835300308,
          0.20952040893031648,
          0.20952040893031648,
          0.21074193790874238,
          0.21074193790874238,
          0.21156881906336916,
          0.21156881906336916,
          0.21235811471096744,
          0.21235811471096744,
          0.21260242050665262,
          0.21260242050665262,
          0.21545891904081785,
          0.21545891904081785,
          0.2188604074268962,
          0.2188604074268962,
          0.21897316394798166,
          0.21897316394798166,
          0.2199128016236939,
          0.2199128016236939,
          0.22000676539126512,
          0.22000676539126512,
          0.22026986394046455,
          0.22026986394046455,
          0.22104036683454859,
          0.22104036683454859,
          0.22316394798165828,
          0.22316394798165828,
          0.22489288130496882,
          0.22489288130496882,
          0.22579493347365256,
          0.22579493347365256,
          0.2271480117266782,
          0.2271480117266782,
          0.22867022476133203,
          0.22867022476133203,
          0.23009847402841466,
          0.23009847402841466,
          0.23043674359167104,
          0.23043674359167104,
          0.23289859430203713,
          0.23289859430203713,
          0.23466511313237615,
          0.23466511313237615,
          0.23848004209576787,
          0.23848004209576787,
          0.23936330151093738,
          0.23936330151093738,
          0.2403781102007066,
          0.2403781102007066,
          0.240716379763963,
          0.240716379763963,
          0.24299030293918664,
          0.24299030293918664,
          0.2430466811997294,
          0.2430466811997294,
          0.24383597684732766,
          0.24383597684732766,
          0.245320604374953,
          0.245320604374953,
          0.24575283770578066,
          0.24575283770578066,
          0.24725625798692025,
          0.24725625798692025,
          0.2489476058032023,
          0.2489476058032023,
          0.2493234608734872,
          0.2493234608734872,
          0.24971810869728633,
          0.24971810869728633,
          0.2523490941892806,
          0.2523490941892806,
          0.2527249492595655,
          0.2527249492595655,
          0.25420957678719086,
          0.25420957678719086,
          0.2565398782229572,
          0.2565398782229572,
          0.2571976245959558,
          0.2571976245959558,
          0.25905810719386607,
          0.25905810719386607,
          0.2590768999473803,
          0.2590768999473803,
          0.26048635646094864,
          0.26048635646094864,
          0.26230925355183043,
          0.26230925355183043,
          0.2633052694880854,
          0.2633052694880854,
          0.26749605352176203,
          0.26749605352176203,
          0.26854844771855974,
          0.26854844771855974,
          0.26911223032398707,
          0.26911223032398707,
          0.2762910621664286,
          0.2762910621664286,
          0.27670450274374203,
          0.27670450274374203,
          0.27672329549725627,
          0.27672329549725627,
          0.2799932346087349,
          0.2799932346087349,
          0.2821731940163873,
          0.2821731940163873,
          0.28781102007066073,
          0.28781102007066073,
          0.2913440577313388,
          0.2913440577313388,
          0.2922649026535368,
          0.2922649026535368,
          0.29245283018867924,
          0.29245283018867924,
          0.2925467939562505,
          0.2925467939562505,
          0.29322333308276327,
          0.29322333308276327,
          0.2945200330752462,
          0.2945200330752462,
          0.29487709539201684,
          0.29487709539201684,
          0.2972073968277832,
          0.2972073968277832,
          0.2982409982710667,
          0.2982409982710667,
          0.2993873562354356,
          0.2993873562354356,
          0.2995376982635496,
          0.2995376982635496,
          0.30495001127565213,
          0.30495001127565213,
          0.30620912576110654,
          0.30620912576110654,
          0.3071863489438473,
          0.3071863489438473,
          0.30752461850710366,
          0.30752461850710366,
          0.3114335112380666,
          0.3114335112380666,
          0.3114710967450951,
          0.3114710967450951,
          0.3123543561602646,
          0.3123543561602646,
          0.31637600541231303,
          0.31637600541231303,
          0.31645117642637,
          0.31645117642637,
          0.3207359242276178,
          0.3207359242276178,
          0.3223521010298429,
          0.3223521010298429,
          0.33793129369315195,
          0.33793129369315195,
          0.34170863714951516,
          0.34170863714951516,
          0.3442832443809667,
          0.3442832443809667,
          0.3447530632188228,
          0.3447530632188228,
          0.3475531834924453,
          0.3475531834924453,
          0.3728858152296474,
          0.3728858152296474,
          0.3738442456588739,
          0.3738442456588739,
          0.3748402615951289,
          0.3748402615951289,
          0.37688867172818163,
          0.37688867172818163,
          0.37728331955198074,
          0.37728331955198074,
          0.37904983838231976,
          0.37904983838231976,
          0.38205667894459894,
          0.38205667894459894,
          0.38478162820416445,
          0.38478162820416445,
          0.3899120499135533,
          0.3899120499135533,
          0.3940840411937157,
          0.3940840411937157,
          0.3959445237916259,
          0.3959445237916259,
          0.3979365556641359,
          0.3979365556641359,
          0.39979703826204616,
          0.39979703826204616,
          0.4065060512666316,
          0.4065060512666316,
          0.4073705179282869,
          0.4073705179282869,
          0.41297075847553183,
          0.41297075847553183,
          0.4212019845147711,
          0.4212019845147711,
          0.42285574682402466,
          0.42285574682402466,
          0.42742238592798615,
          0.42742238592798615,
          0.43065473953243627,
          0.43065473953243627,
          0.43181989025031947,
          0.43181989025031947,
          0.43719461775539353,
          0.43719461775539353,
          0.4456325640832895,
          0.4456325640832895,
          0.46985642336315114,
          0.46985642336315114,
          0.4875591971735699,
          0.4875591971735699,
          0.4891753739757949,
          0.4891753739757949,
          0.510204465158235,
          0.510204465158235,
          0.5134180260091709,
          0.5134180260091709,
          0.5322483650304443,
          0.5322483650304443,
          0.5534277982409983,
          0.5534277982409983,
          0.5617905735548373,
          0.5617905735548373,
          0.5837217169059611,
          0.5837217169059611,
          0.6113658573254154,
          0.6113658573254154,
          0.686236187326167,
          0.686236187326167,
          0.8081447793730737,
          0.8081447793730737,
          1
         ],
         "xaxis": "x",
         "y": [
          0,
          0.00129366106080207,
          0.00258732212160414,
          0.00258732212160414,
          0.0038809831824062097,
          0.0038809831824062097,
          0.00646830530401035,
          0.00646830530401035,
          0.007761966364812419,
          0.007761966364812419,
          0.009055627425614488,
          0.009055627425614488,
          0.01034928848641656,
          0.01034928848641656,
          0.0129366106080207,
          0.0129366106080207,
          0.015523932729624839,
          0.015523932729624839,
          0.016817593790426907,
          0.016817593790426907,
          0.019404915912031046,
          0.019404915912031046,
          0.02199223803363519,
          0.02199223803363519,
          0.02328589909443726,
          0.02328589909443726,
          0.02457956015523933,
          0.02457956015523933,
          0.027166882276843468,
          0.027166882276843468,
          0.028460543337645538,
          0.028460543337645538,
          0.029754204398447608,
          0.029754204398447608,
          0.031047865459249677,
          0.031047865459249677,
          0.03363518758085381,
          0.03363518758085381,
          0.03492884864165589,
          0.03492884864165589,
          0.03622250970245795,
          0.03622250970245795,
          0.037516170763260026,
          0.037516170763260026,
          0.03880983182406209,
          0.03880983182406209,
          0.040103492884864166,
          0.040103492884864166,
          0.042690815006468305,
          0.042690815006468305,
          0.04398447606727038,
          0.04398447606727038,
          0.045278137128072445,
          0.045278137128072445,
          0.04657179818887452,
          0.04657179818887452,
          0.047865459249676584,
          0.047865459249676584,
          0.04915912031047866,
          0.04915912031047866,
          0.050452781371280724,
          0.050452781371280724,
          0.05304010349288486,
          0.05304010349288486,
          0.054333764553686936,
          0.054333764553686936,
          0.055627425614489,
          0.055627425614489,
          0.056921086675291076,
          0.056921086675291076,
          0.05821474773609314,
          0.05821474773609314,
          0.059508408796895215,
          0.059508408796895215,
          0.062095730918499355,
          0.062095730918499355,
          0.06338939197930142,
          0.06338939197930142,
          0.0646830530401035,
          0.0646830530401035,
          0.06597671410090557,
          0.06597671410090557,
          0.0685640362225097,
          0.0685640362225097,
          0.06985769728331177,
          0.06985769728331177,
          0.07115135834411385,
          0.07115135834411385,
          0.0724450194049159,
          0.0724450194049159,
          0.07632600258732213,
          0.07632600258732213,
          0.07761966364812418,
          0.07761966364812418,
          0.07891332470892626,
          0.07891332470892626,
          0.08020698576972833,
          0.08020698576972833,
          0.0815006468305304,
          0.0815006468305304,
          0.08279430789133248,
          0.08279430789133248,
          0.08408796895213454,
          0.08408796895213454,
          0.08667529107373868,
          0.08667529107373868,
          0.08796895213454076,
          0.08796895213454076,
          0.08926261319534282,
          0.08926261319534282,
          0.09055627425614489,
          0.09055627425614489,
          0.09184993531694696,
          0.09184993531694696,
          0.09314359637774904,
          0.09314359637774904,
          0.0944372574385511,
          0.0944372574385511,
          0.09702457956015524,
          0.09702457956015524,
          0.09961190168175937,
          0.09961190168175937,
          0.10090556274256145,
          0.10090556274256145,
          0.10219922380336352,
          0.10219922380336352,
          0.1034928848641656,
          0.1034928848641656,
          0.10478654592496765,
          0.10478654592496765,
          0.10608020698576973,
          0.10608020698576973,
          0.1073738680465718,
          0.1073738680465718,
          0.10866752910737387,
          0.10866752910737387,
          0.10996119016817593,
          0.10996119016817593,
          0.111254851228978,
          0.111254851228978,
          0.11254851228978008,
          0.11254851228978008,
          0.11384217335058215,
          0.11384217335058215,
          0.11513583441138421,
          0.11513583441138421,
          0.11642949547218628,
          0.11642949547218628,
          0.11901681759379043,
          0.11901681759379043,
          0.1203104786545925,
          0.1203104786545925,
          0.12160413971539456,
          0.12160413971539456,
          0.12289780077619664,
          0.12289780077619664,
          0.12419146183699871,
          0.12419146183699871,
          0.12548512289780078,
          0.12548512289780078,
          0.12677878395860284,
          0.12677878395860284,
          0.12807244501940493,
          0.12807244501940493,
          0.129366106080207,
          0.129366106080207,
          0.13065976714100905,
          0.13065976714100905,
          0.13195342820181113,
          0.13195342820181113,
          0.1332470892626132,
          0.1332470892626132,
          0.13454075032341525,
          0.13454075032341525,
          0.13583441138421734,
          0.13583441138421734,
          0.13971539456662355,
          0.13971539456662355,
          0.1410090556274256,
          0.1410090556274256,
          0.1423027166882277,
          0.1423027166882277,
          0.14359637774902975,
          0.14359637774902975,
          0.1448900388098318,
          0.1448900388098318,
          0.1461836998706339,
          0.1461836998706339,
          0.14747736093143596,
          0.14747736093143596,
          0.14877102199223805,
          0.14877102199223805,
          0.15135834411384216,
          0.15135834411384216,
          0.15653298835705046,
          0.15653298835705046,
          0.15782664941785252,
          0.15782664941785252,
          0.1591203104786546,
          0.1591203104786546,
          0.16041397153945666,
          0.16041397153945666,
          0.1630012936610608,
          0.1630012936610608,
          0.16429495472186287,
          0.16429495472186287,
          0.16688227684346701,
          0.16688227684346701,
          0.16817593790426907,
          0.16817593790426907,
          0.16946959896507116,
          0.16946959896507116,
          0.17076326002587322,
          0.17076326002587322,
          0.17205692108667528,
          0.17205692108667528,
          0.17464424320827943,
          0.17464424320827943,
          0.1759379042690815,
          0.1759379042690815,
          0.17723156532988357,
          0.17723156532988357,
          0.17852522639068563,
          0.17852522639068563,
          0.17981888745148772,
          0.17981888745148772,
          0.18111254851228978,
          0.18111254851228978,
          0.18240620957309184,
          0.18240620957309184,
          0.18369987063389392,
          0.18369987063389392,
          0.18499353169469598,
          0.18499353169469598,
          0.18628719275549807,
          0.18628719275549807,
          0.18758085381630013,
          0.18758085381630013,
          0.19016817593790428,
          0.19016817593790428,
          0.19146183699870634,
          0.19146183699870634,
          0.1927554980595084,
          0.1927554980595084,
          0.19534282018111254,
          0.19534282018111254,
          0.19663648124191463,
          0.19663648124191463,
          0.1979301423027167,
          0.1979301423027167,
          0.19922380336351875,
          0.19922380336351875,
          0.20051746442432083,
          0.20051746442432083,
          0.2018111254851229,
          0.2018111254851229,
          0.20439844760672704,
          0.20439844760672704,
          0.2056921086675291,
          0.2056921086675291,
          0.2069857697283312,
          0.2069857697283312,
          0.20827943078913325,
          0.20827943078913325,
          0.2095730918499353,
          0.2095730918499353,
          0.2108667529107374,
          0.2108667529107374,
          0.21216041397153945,
          0.21216041397153945,
          0.21345407503234154,
          0.21345407503234154,
          0.2147477360931436,
          0.2147477360931436,
          0.21604139715394566,
          0.21604139715394566,
          0.21733505821474774,
          0.21733505821474774,
          0.222509702457956,
          0.222509702457956,
          0.2238033635187581,
          0.2238033635187581,
          0.22509702457956016,
          0.22509702457956016,
          0.22639068564036222,
          0.22639068564036222,
          0.22897800776196636,
          0.22897800776196636,
          0.23027166882276842,
          0.23027166882276842,
          0.2315653298835705,
          0.2315653298835705,
          0.23415265200517466,
          0.23415265200517466,
          0.23544631306597671,
          0.23544631306597671,
          0.23673997412677877,
          0.23673997412677877,
          0.23803363518758086,
          0.23803363518758086,
          0.240620957309185,
          0.240620957309185,
          0.2445019404915912,
          0.2445019404915912,
          0.24579560155239327,
          0.24579560155239327,
          0.24708926261319533,
          0.24708926261319533,
          0.24838292367399742,
          0.24838292367399742,
          0.25097024579560157,
          0.25097024579560157,
          0.2522639068564036,
          0.2522639068564036,
          0.2535575679172057,
          0.2535575679172057,
          0.25485122897800777,
          0.25485122897800777,
          0.25614489003880986,
          0.25614489003880986,
          0.2574385510996119,
          0.2574385510996119,
          0.258732212160414,
          0.258732212160414,
          0.26002587322121606,
          0.26002587322121606,
          0.2613195342820181,
          0.2613195342820181,
          0.2626131953428202,
          0.2626131953428202,
          0.26390685640362227,
          0.26390685640362227,
          0.2652005174644243,
          0.2652005174644243,
          0.2664941785252264,
          0.2664941785252264,
          0.2677878395860285,
          0.2677878395860285,
          0.2690815006468305,
          0.2690815006468305,
          0.2703751617076326,
          0.2703751617076326,
          0.2716688227684347,
          0.2716688227684347,
          0.2729624838292367,
          0.2729624838292367,
          0.2742561448900388,
          0.2742561448900388,
          0.2755498059508409,
          0.2755498059508409,
          0.278137128072445,
          0.278137128072445,
          0.2794307891332471,
          0.2794307891332471,
          0.2807244501940492,
          0.2807244501940492,
          0.2820181112548512,
          0.2820181112548512,
          0.2833117723156533,
          0.2833117723156533,
          0.2846054333764554,
          0.2846054333764554,
          0.2858990944372574,
          0.2858990944372574,
          0.2871927554980595,
          0.2871927554980595,
          0.2884864165588616,
          0.2884864165588616,
          0.2923673997412678,
          0.2923673997412678,
          0.2936610608020699,
          0.2936610608020699,
          0.2949547218628719,
          0.2949547218628719,
          0.296248382923674,
          0.296248382923674,
          0.2975420439844761,
          0.2975420439844761,
          0.2988357050452781,
          0.2988357050452781,
          0.3001293661060802,
          0.3001293661060802,
          0.3014230271668823,
          0.3014230271668823,
          0.3027166882276843,
          0.3027166882276843,
          0.3040103492884864,
          0.3040103492884864,
          0.3053040103492885,
          0.3053040103492885,
          0.30659767141009053,
          0.30659767141009053,
          0.3078913324708926,
          0.3078913324708926,
          0.3091849935316947,
          0.3091849935316947,
          0.31047865459249674,
          0.31047865459249674,
          0.3117723156532988,
          0.3117723156532988,
          0.3130659767141009,
          0.3130659767141009,
          0.314359637774903,
          0.314359637774903,
          0.31565329883570503,
          0.31565329883570503,
          0.3169469598965071,
          0.3169469598965071,
          0.3182406209573092,
          0.3182406209573092,
          0.31953428201811124,
          0.31953428201811124,
          0.3208279430789133,
          0.3208279430789133,
          0.3221216041397154,
          0.3221216041397154,
          0.32341526520051744,
          0.32341526520051744,
          0.32470892626131953,
          0.32470892626131953,
          0.3260025873221216,
          0.3260025873221216,
          0.32729624838292365,
          0.32729624838292365,
          0.32858990944372574,
          0.32858990944372574,
          0.3298835705045278,
          0.3298835705045278,
          0.3311772315653299,
          0.3311772315653299,
          0.33247089262613194,
          0.33247089262613194,
          0.33376455368693403,
          0.33376455368693403,
          0.33635187580853815,
          0.33635187580853815,
          0.33764553686934023,
          0.33764553686934023,
          0.3389391979301423,
          0.3389391979301423,
          0.34023285899094435,
          0.34023285899094435,
          0.34152652005174644,
          0.34152652005174644,
          0.3428201811125485,
          0.3428201811125485,
          0.34411384217335056,
          0.34411384217335056,
          0.34540750323415265,
          0.34540750323415265,
          0.34670116429495473,
          0.34670116429495473,
          0.34799482535575677,
          0.34799482535575677,
          0.34928848641655885,
          0.34928848641655885,
          0.351875808538163,
          0.351875808538163,
          0.35316946959896506,
          0.35316946959896506,
          0.35446313065976714,
          0.35446313065976714,
          0.35575679172056923,
          0.35575679172056923,
          0.35705045278137126,
          0.35705045278137126,
          0.35834411384217335,
          0.35834411384217335,
          0.35963777490297544,
          0.35963777490297544,
          0.36093143596377747,
          0.36093143596377747,
          0.3648124191461837,
          0.3648124191461837,
          0.36610608020698576,
          0.36610608020698576,
          0.36739974126778785,
          0.36739974126778785,
          0.36869340232858994,
          0.36869340232858994,
          0.37128072445019406,
          0.37128072445019406,
          0.3738680465717982,
          0.3738680465717982,
          0.37516170763260026,
          0.37516170763260026,
          0.37645536869340235,
          0.37645536869340235,
          0.3777490297542044,
          0.3777490297542044,
          0.37904269081500647,
          0.37904269081500647,
          0.3816300129366106,
          0.3816300129366106,
          0.3829236739974127,
          0.3829236739974127,
          0.38421733505821476,
          0.38421733505821476,
          0.3855109961190168,
          0.3855109961190168,
          0.3868046571798189,
          0.3868046571798189,
          0.38809831824062097,
          0.38809831824062097,
          0.38939197930142305,
          0.38939197930142305,
          0.3906856403622251,
          0.3906856403622251,
          0.39197930142302717,
          0.39197930142302717,
          0.39327296248382926,
          0.39327296248382926,
          0.3945666235446313,
          0.3945666235446313,
          0.3958602846054334,
          0.3958602846054334,
          0.3984476067270375,
          0.3984476067270375,
          0.3997412677878396,
          0.3997412677878396,
          0.40103492884864167,
          0.40103492884864167,
          0.4023285899094437,
          0.4023285899094437,
          0.4036222509702458,
          0.4036222509702458,
          0.4049159120310479,
          0.4049159120310479,
          0.40620957309184996,
          0.40620957309184996,
          0.407503234152652,
          0.407503234152652,
          0.4087968952134541,
          0.4087968952134541,
          0.4113842173350582,
          0.4113842173350582,
          0.4126778783958603,
          0.4126778783958603,
          0.4139715394566624,
          0.4139715394566624,
          0.4152652005174644,
          0.4152652005174644,
          0.4165588615782665,
          0.4165588615782665,
          0.4191461836998706,
          0.4191461836998706,
          0.4217335058214748,
          0.4217335058214748,
          0.4230271668822768,
          0.4230271668822768,
          0.4243208279430789,
          0.4243208279430789,
          0.425614489003881,
          0.425614489003881,
          0.4269081500646831,
          0.4269081500646831,
          0.4282018111254851,
          0.4282018111254851,
          0.4294954721862872,
          0.4294954721862872,
          0.4307891332470893,
          0.4307891332470893,
          0.4320827943078913,
          0.4320827943078913,
          0.4333764553686934,
          0.4333764553686934,
          0.4346701164294955,
          0.4346701164294955,
          0.4359637774902975,
          0.4359637774902975,
          0.4372574385510996,
          0.4372574385510996,
          0.4385510996119017,
          0.4385510996119017,
          0.4398447606727037,
          0.4398447606727037,
          0.4424320827943079,
          0.4424320827943079,
          0.44372574385511,
          0.44372574385511,
          0.445019404915912,
          0.445019404915912,
          0.4489003880983182,
          0.4489003880983182,
          0.4501940491591203,
          0.4501940491591203,
          0.4514877102199224,
          0.4514877102199224,
          0.45278137128072443,
          0.45278137128072443,
          0.4540750323415265,
          0.4540750323415265,
          0.4553686934023286,
          0.4553686934023286,
          0.45666235446313064,
          0.45666235446313064,
          0.4579560155239327,
          0.4579560155239327,
          0.4592496765847348,
          0.4592496765847348,
          0.46054333764553684,
          0.46054333764553684,
          0.46183699870633893,
          0.46183699870633893,
          0.463130659767141,
          0.463130659767141,
          0.4644243208279431,
          0.4644243208279431,
          0.46571798188874514,
          0.46571798188874514,
          0.4670116429495472,
          0.4670116429495472,
          0.4683053040103493,
          0.4683053040103493,
          0.46959896507115134,
          0.46959896507115134,
          0.47089262613195343,
          0.47089262613195343,
          0.4721862871927555,
          0.4721862871927555,
          0.47347994825355755,
          0.47347994825355755,
          0.47477360931435963,
          0.47477360931435963,
          0.4760672703751617,
          0.4760672703751617,
          0.47736093143596375,
          0.47736093143596375,
          0.47865459249676584,
          0.47865459249676584,
          0.4799482535575679,
          0.4799482535575679,
          0.48124191461837,
          0.48124191461837,
          0.48253557567917205,
          0.48253557567917205,
          0.48382923673997413,
          0.48382923673997413,
          0.4851228978007762,
          0.4851228978007762,
          0.48641655886157825,
          0.48641655886157825,
          0.48771021992238034,
          0.48771021992238034,
          0.4890038809831824,
          0.4890038809831824,
          0.49029754204398446,
          0.49029754204398446,
          0.49159120310478654,
          0.49159120310478654,
          0.49288486416558863,
          0.49288486416558863,
          0.49547218628719275,
          0.49547218628719275,
          0.49676584734799484,
          0.49676584734799484,
          0.49805950840879687,
          0.49805950840879687,
          0.49935316946959896,
          0.49935316946959896,
          0.500646830530401,
          0.500646830530401,
          0.5019404915912031,
          0.5019404915912031,
          0.5032341526520052,
          0.5032341526520052,
          0.5045278137128072,
          0.5045278137128072,
          0.5058214747736093,
          0.5058214747736093,
          0.5071151358344114,
          0.5071151358344114,
          0.5084087968952135,
          0.5084087968952135,
          0.5097024579560155,
          0.5097024579560155,
          0.5109961190168176,
          0.5109961190168176,
          0.5122897800776197,
          0.5122897800776197,
          0.5135834411384217,
          0.5135834411384217,
          0.5148771021992238,
          0.5148771021992238,
          0.5161707632600259,
          0.5161707632600259,
          0.517464424320828,
          0.517464424320828,
          0.51875808538163,
          0.51875808538163,
          0.5200517464424321,
          0.5200517464424321,
          0.5213454075032341,
          0.5213454075032341,
          0.5226390685640362,
          0.5226390685640362,
          0.5239327296248383,
          0.5239327296248383,
          0.5252263906856404,
          0.5252263906856404,
          0.5265200517464425,
          0.5265200517464425,
          0.5278137128072445,
          0.5278137128072445,
          0.5291073738680466,
          0.5291073738680466,
          0.5304010349288486,
          0.5304010349288486,
          0.5316946959896507,
          0.5316946959896507,
          0.5329883570504528,
          0.5329883570504528,
          0.5342820181112549,
          0.5342820181112549,
          0.535575679172057,
          0.535575679172057,
          0.536869340232859,
          0.536869340232859,
          0.538163001293661,
          0.538163001293661,
          0.5394566623544631,
          0.5394566623544631,
          0.5407503234152652,
          0.5407503234152652,
          0.5420439844760673,
          0.5420439844760673,
          0.5433376455368694,
          0.5433376455368694,
          0.5446313065976714,
          0.5446313065976714,
          0.5459249676584734,
          0.5459249676584734,
          0.5472186287192755,
          0.5472186287192755,
          0.5485122897800776,
          0.5485122897800776,
          0.5498059508408797,
          0.5498059508408797,
          0.5510996119016818,
          0.5510996119016818,
          0.5523932729624839,
          0.5523932729624839,
          0.553686934023286,
          0.553686934023286,
          0.5549805950840879,
          0.5549805950840879,
          0.55627425614489,
          0.55627425614489,
          0.5575679172056921,
          0.5575679172056921,
          0.5588615782664942,
          0.5588615782664942,
          0.5601552393272963,
          0.5601552393272963,
          0.5614489003880984,
          0.5614489003880984,
          0.5627425614489003,
          0.5627425614489003,
          0.5640362225097024,
          0.5640362225097024,
          0.5653298835705045,
          0.5653298835705045,
          0.5666235446313066,
          0.5666235446313066,
          0.5679172056921087,
          0.5679172056921087,
          0.5692108667529108,
          0.5692108667529108,
          0.5705045278137129,
          0.5705045278137129,
          0.5717981888745148,
          0.5717981888745148,
          0.5730918499353169,
          0.5730918499353169,
          0.574385510996119,
          0.574385510996119,
          0.5756791720569211,
          0.5756791720569211,
          0.5769728331177232,
          0.5769728331177232,
          0.5782664941785253,
          0.5782664941785253,
          0.5795601552393272,
          0.5795601552393272,
          0.5808538163001293,
          0.5808538163001293,
          0.5821474773609314,
          0.5821474773609314,
          0.5834411384217335,
          0.5834411384217335,
          0.5847347994825356,
          0.5847347994825356,
          0.5860284605433377,
          0.5860284605433377,
          0.5873221216041398,
          0.5873221216041398,
          0.5886157826649417,
          0.5886157826649417,
          0.5912031047865459,
          0.5912031047865459,
          0.5937904269081501,
          0.5937904269081501,
          0.5950840879689522,
          0.5950840879689522,
          0.5963777490297542,
          0.5963777490297542,
          0.5976714100905562,
          0.5976714100905562,
          0.5989650711513583,
          0.5989650711513583,
          0.6002587322121604,
          0.6002587322121604,
          0.6015523932729625,
          0.6015523932729625,
          0.6028460543337646,
          0.6028460543337646,
          0.6041397153945667,
          0.6041397153945667,
          0.6054333764553687,
          0.6054333764553687,
          0.6067270375161707,
          0.6067270375161707,
          0.6080206985769728,
          0.6080206985769728,
          0.6093143596377749,
          0.6093143596377749,
          0.610608020698577,
          0.610608020698577,
          0.6119016817593791,
          0.6119016817593791,
          0.6131953428201811,
          0.6131953428201811,
          0.6144890038809832,
          0.6144890038809832,
          0.6157826649417852,
          0.6157826649417852,
          0.6170763260025873,
          0.6170763260025873,
          0.6183699870633894,
          0.6183699870633894,
          0.6196636481241915,
          0.6196636481241915,
          0.6209573091849935,
          0.6209573091849935,
          0.6222509702457956,
          0.6222509702457956,
          0.6235446313065977,
          0.6235446313065977,
          0.6248382923673997,
          0.6248382923673997,
          0.6261319534282018,
          0.6261319534282018,
          0.6274256144890039,
          0.6274256144890039,
          0.628719275549806,
          0.628719275549806,
          0.630012936610608,
          0.630012936610608,
          0.6313065976714101,
          0.6313065976714101,
          0.6326002587322122,
          0.6326002587322122,
          0.6338939197930142,
          0.6338939197930142,
          0.6351875808538163,
          0.6351875808538163,
          0.6364812419146184,
          0.6364812419146184,
          0.6377749029754204,
          0.6377749029754204,
          0.6403622250970246,
          0.6403622250970246,
          0.6416558861578266,
          0.6416558861578266,
          0.6429495472186287,
          0.6429495472186287,
          0.6442432082794308,
          0.6442432082794308,
          0.6455368693402329,
          0.6455368693402329,
          0.6468305304010349,
          0.6468305304010349,
          0.648124191461837,
          0.648124191461837,
          0.6494178525226391,
          0.6494178525226391,
          0.6507115135834411,
          0.6507115135834411,
          0.6520051746442432,
          0.6520051746442432,
          0.6532988357050453,
          0.6532988357050453,
          0.6545924967658473,
          0.6545924967658473,
          0.6558861578266494,
          0.6558861578266494,
          0.6571798188874515,
          0.6571798188874515,
          0.6584734799482536,
          0.6584734799482536,
          0.6597671410090556,
          0.6597671410090556,
          0.6610608020698577,
          0.6610608020698577,
          0.6623544631306598,
          0.6623544631306598,
          0.6636481241914618,
          0.6636481241914618,
          0.6649417852522639,
          0.6649417852522639,
          0.666235446313066,
          0.666235446313066,
          0.6675291073738681,
          0.6675291073738681,
          0.6688227684346701,
          0.6688227684346701,
          0.6701164294954722,
          0.6701164294954722,
          0.6714100905562742,
          0.6714100905562742,
          0.6727037516170763,
          0.6727037516170763,
          0.6739974126778784,
          0.6739974126778784,
          0.6752910737386805,
          0.6752910737386805,
          0.6765847347994826,
          0.6765847347994826,
          0.6778783958602846,
          0.6778783958602846,
          0.6791720569210866,
          0.6791720569210866,
          0.6804657179818887,
          0.6804657179818887,
          0.6817593790426908,
          0.6817593790426908,
          0.6830530401034929,
          0.6830530401034929,
          0.684346701164295,
          0.684346701164295,
          0.684346701164295,
          0.684346701164295,
          0.685640362225097,
          0.685640362225097,
          0.6869340232858991,
          0.6869340232858991,
          0.6882276843467011,
          0.6882276843467011,
          0.6895213454075032,
          0.6895213454075032,
          0.6908150064683053,
          0.6908150064683053,
          0.6921086675291074,
          0.6921086675291074,
          0.6934023285899095,
          0.6934023285899095,
          0.6946959896507116,
          0.6946959896507116,
          0.6959896507115135,
          0.6959896507115135,
          0.6972833117723156,
          0.6972833117723156,
          0.6985769728331177,
          0.6985769728331177,
          0.6998706338939198,
          0.6998706338939198,
          0.7011642949547219,
          0.7011642949547219,
          0.702457956015524,
          0.702457956015524,
          0.703751617076326,
          0.703751617076326,
          0.705045278137128,
          0.705045278137128,
          0.7063389391979301,
          0.7063389391979301,
          0.7076326002587322,
          0.7076326002587322,
          0.7089262613195343,
          0.7089262613195343,
          0.7102199223803364,
          0.7102199223803364,
          0.7115135834411385,
          0.7115135834411385,
          0.7128072445019404,
          0.7128072445019404,
          0.7141009055627425,
          0.7141009055627425,
          0.7153945666235446,
          0.7153945666235446,
          0.7166882276843467,
          0.7166882276843467,
          0.7179818887451488,
          0.7179818887451488,
          0.7192755498059509,
          0.7192755498059509,
          0.720569210866753,
          0.720569210866753,
          0.7218628719275549,
          0.7218628719275549,
          0.723156532988357,
          0.723156532988357,
          0.7244501940491591,
          0.7244501940491591,
          0.7257438551099612,
          0.7257438551099612,
          0.7270375161707633,
          0.7270375161707633,
          0.7283311772315654,
          0.7283311772315654,
          0.7296248382923674,
          0.7296248382923674,
          0.7309184993531694,
          0.7309184993531694,
          0.7322121604139715,
          0.7322121604139715,
          0.7335058214747736,
          0.7335058214747736,
          0.7347994825355757,
          0.7347994825355757,
          0.7360931435963778,
          0.7360931435963778,
          0.7373868046571799,
          0.7373868046571799,
          0.7386804657179818,
          0.7386804657179818,
          0.7399741267787839,
          0.7399741267787839,
          0.741267787839586,
          0.741267787839586,
          0.7425614489003881,
          0.7425614489003881,
          0.7438551099611902,
          0.7438551099611902,
          0.7451487710219923,
          0.7451487710219923,
          0.7464424320827943,
          0.7464424320827943,
          0.7477360931435963,
          0.7477360931435963,
          0.7490297542043984,
          0.7490297542043984,
          0.7503234152652005,
          0.7503234152652005,
          0.7516170763260026,
          0.7516170763260026,
          0.7529107373868047,
          0.7529107373868047,
          0.7542043984476067,
          0.7542043984476067,
          0.7554980595084088,
          0.7554980595084088,
          0.7567917205692108,
          0.7567917205692108,
          0.7580853816300129,
          0.7580853816300129,
          0.759379042690815,
          0.759379042690815,
          0.7606727037516171,
          0.7606727037516171,
          0.7619663648124192,
          0.7619663648124192,
          0.7632600258732212,
          0.7632600258732212,
          0.7645536869340233,
          0.7645536869340233,
          0.7658473479948253,
          0.7658473479948253,
          0.7671410090556274,
          0.7671410090556274,
          0.7684346701164295,
          0.7684346701164295,
          0.7697283311772316,
          0.7697283311772316,
          0.7710219922380336,
          0.7710219922380336,
          0.7723156532988357,
          0.7723156532988357,
          0.7736093143596378,
          0.7736093143596378,
          0.7749029754204398,
          0.7749029754204398,
          0.7761966364812419,
          0.7761966364812419,
          0.777490297542044,
          0.777490297542044,
          0.7787839586028461,
          0.7787839586028461,
          0.7787839586028461,
          0.7787839586028461,
          0.7800776196636481,
          0.7800776196636481,
          0.7813712807244502,
          0.7813712807244502,
          0.7826649417852523,
          0.7826649417852523,
          0.7839586028460543,
          0.7839586028460543,
          0.7852522639068564,
          0.7852522639068564,
          0.7865459249676585,
          0.7865459249676585,
          0.7878395860284605,
          0.7878395860284605,
          0.7891332470892626,
          0.7891332470892626,
          0.7904269081500647,
          0.7904269081500647,
          0.7917205692108668,
          0.7917205692108668,
          0.7930142302716688,
          0.7930142302716688,
          0.7943078913324709,
          0.7943078913324709,
          0.795601552393273,
          0.795601552393273,
          0.796895213454075,
          0.796895213454075,
          0.7981888745148771,
          0.7981888745148771,
          0.7994825355756792,
          0.7994825355756792,
          0.8007761966364813,
          0.8007761966364813,
          0.8020698576972833,
          0.8020698576972833,
          0.8033635187580854,
          0.8033635187580854,
          0.8046571798188874,
          0.8046571798188874,
          0.8059508408796895,
          0.8059508408796895,
          0.8072445019404916,
          0.8072445019404916,
          0.8098318240620958,
          0.8098318240620958,
          0.8111254851228978,
          0.8111254851228978,
          0.8124191461836999,
          0.8124191461836999,
          0.8137128072445019,
          0.8137128072445019,
          0.815006468305304,
          0.815006468305304,
          0.8163001293661061,
          0.8163001293661061,
          0.8175937904269082,
          0.8175937904269082,
          0.8188874514877102,
          0.8188874514877102,
          0.8201811125485123,
          0.8201811125485123,
          0.8214747736093143,
          0.8214747736093143,
          0.8227684346701164,
          0.8227684346701164,
          0.8240620957309185,
          0.8240620957309185,
          0.8253557567917206,
          0.8253557567917206,
          0.8266494178525227,
          0.8266494178525227,
          0.8279430789133247,
          0.8279430789133247,
          0.8292367399741267,
          0.8292367399741267,
          0.8318240620957309,
          0.8318240620957309,
          0.833117723156533,
          0.833117723156533,
          0.8344113842173351,
          0.8344113842173351,
          0.8357050452781372,
          0.8357050452781372,
          0.8369987063389392,
          0.8369987063389392,
          0.8382923673997412,
          0.8382923673997412,
          0.8395860284605433,
          0.8395860284605433,
          0.8408796895213454,
          0.8408796895213454,
          0.8421733505821475,
          0.8421733505821475,
          0.8434670116429496,
          0.8434670116429496,
          0.8447606727037517,
          0.8447606727037517,
          0.8460543337645536,
          0.8460543337645536,
          0.8473479948253557,
          0.8473479948253557,
          0.8486416558861578,
          0.8486416558861578,
          0.8499353169469599,
          0.8499353169469599,
          0.851228978007762,
          0.851228978007762,
          0.8525226390685641,
          0.8525226390685641,
          0.8538163001293662,
          0.8538163001293662,
          0.8551099611901681,
          0.8551099611901681,
          0.8564036222509702,
          0.8564036222509702,
          0.8576972833117723,
          0.8576972833117723,
          0.8589909443725744,
          0.8589909443725744,
          0.8602846054333765,
          0.8602846054333765,
          0.8615782664941786,
          0.8615782664941786,
          0.8628719275549805,
          0.8628719275549805,
          0.8641655886157826,
          0.8641655886157826,
          0.8654592496765847,
          0.8654592496765847,
          0.8667529107373868,
          0.8667529107373868,
          0.8680465717981889,
          0.8680465717981889,
          0.869340232858991,
          0.869340232858991,
          0.8706338939197931,
          0.8706338939197931,
          0.871927554980595,
          0.871927554980595,
          0.8732212160413971,
          0.8732212160413971,
          0.8745148771021992,
          0.8745148771021992,
          0.8758085381630013,
          0.8758085381630013,
          0.8771021992238034,
          0.8771021992238034,
          0.8783958602846055,
          0.8783958602846055,
          0.8796895213454075,
          0.8796895213454075,
          0.8809831824062095,
          0.8809831824062095,
          0.8822768434670116,
          0.8822768434670116,
          0.8835705045278137,
          0.8835705045278137,
          0.8848641655886158,
          0.8848641655886158,
          0.8861578266494179,
          0.8861578266494179,
          0.88745148771022,
          0.88745148771022,
          0.888745148771022,
          0.888745148771022,
          0.890038809831824,
          0.890038809831824,
          0.8913324708926261,
          0.8913324708926261,
          0.8926261319534282,
          0.8926261319534282,
          0.8939197930142303,
          0.8939197930142303,
          0.8952134540750324,
          0.8952134540750324,
          0.8965071151358344,
          0.8965071151358344,
          0.8978007761966365,
          0.8978007761966365,
          0.8990944372574385,
          0.8990944372574385,
          0.9003880983182406,
          0.9003880983182406,
          0.9016817593790427,
          0.9016817593790427,
          0.9029754204398448,
          0.9029754204398448,
          0.9042690815006468,
          0.9042690815006468,
          0.9055627425614489,
          0.9055627425614489,
          0.906856403622251,
          0.906856403622251,
          0.908150064683053,
          0.908150064683053,
          0.9094437257438551,
          0.9094437257438551,
          0.9107373868046572,
          0.9107373868046572,
          0.9120310478654593,
          0.9120310478654593,
          0.9133247089262613,
          0.9133247089262613,
          0.9146183699870634,
          0.9146183699870634,
          0.9159120310478654,
          0.9159120310478654,
          0.9172056921086675,
          0.9172056921086675,
          0.9184993531694696,
          0.9184993531694696,
          0.9197930142302717,
          0.9197930142302717,
          0.9210866752910737,
          0.9210866752910737,
          0.9223803363518758,
          0.9223803363518758,
          0.9236739974126779,
          0.9236739974126779,
          0.92496765847348,
          0.92496765847348,
          0.926261319534282,
          0.926261319534282,
          0.9275549805950841,
          0.9275549805950841,
          0.9288486416558862,
          0.9288486416558862,
          0.9301423027166882,
          0.9301423027166882,
          0.9314359637774903,
          0.9314359637774903,
          0.9327296248382924,
          0.9327296248382924,
          0.9340232858990944,
          0.9340232858990944,
          0.9353169469598965,
          0.9353169469598965,
          0.9366106080206986,
          0.9366106080206986,
          0.9379042690815006,
          0.9379042690815006,
          0.9391979301423027,
          0.9391979301423027,
          0.9404915912031048,
          0.9404915912031048,
          0.9417852522639069,
          0.9417852522639069,
          0.943078913324709,
          0.943078913324709,
          0.944372574385511,
          0.944372574385511,
          0.9456662354463131,
          0.9456662354463131,
          0.9469598965071151,
          0.9469598965071151,
          0.9482535575679172,
          0.9482535575679172,
          0.9495472186287193,
          0.9495472186287193,
          0.9508408796895214,
          0.9508408796895214,
          0.9521345407503234,
          0.9521345407503234,
          0.9534282018111255,
          0.9534282018111255,
          0.9547218628719275,
          0.9547218628719275,
          0.9560155239327296,
          0.9560155239327296,
          0.9573091849935317,
          0.9573091849935317,
          0.9586028460543338,
          0.9586028460543338,
          0.9598965071151359,
          0.9598965071151359,
          0.9611901681759379,
          0.9611901681759379,
          0.96248382923674,
          0.96248382923674,
          0.963777490297542,
          0.963777490297542,
          0.9650711513583441,
          0.9650711513583441,
          0.9663648124191462,
          0.9663648124191462,
          0.9676584734799483,
          0.9676584734799483,
          0.9689521345407504,
          0.9689521345407504,
          0.9702457956015524,
          0.9702457956015524,
          0.9715394566623544,
          0.9715394566623544,
          0.9728331177231565,
          0.9728331177231565,
          0.9741267787839586,
          0.9741267787839586,
          0.9754204398447607,
          0.9754204398447607,
          0.9767141009055628,
          0.9767141009055628,
          0.9780077619663649,
          0.9780077619663649,
          0.9793014230271668,
          0.9793014230271668,
          0.9805950840879689,
          0.9805950840879689,
          0.981888745148771,
          0.981888745148771,
          0.9831824062095731,
          0.9831824062095731,
          0.9844760672703752,
          0.9844760672703752,
          0.9857697283311773,
          0.9857697283311773,
          0.9870633893919794,
          0.9870633893919794,
          0.9883570504527813,
          0.9883570504527813,
          0.9896507115135834,
          0.9896507115135834,
          0.9909443725743855,
          0.9909443725743855,
          0.9922380336351876,
          0.9922380336351876,
          0.9935316946959897,
          0.9935316946959897,
          0.9948253557567918,
          0.9948253557567918,
          0.9961190168175937,
          0.9961190168175937,
          0.9974126778783958,
          0.9974126778783958,
          0.9987063389391979,
          0.9987063389391979,
          1,
          1
         ],
         "yaxis": "y"
        },
        {
         "fill": "tozeroy",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          0,
          5.637614161686774e-05,
          5.637614161686774e-05,
          0.00016912842485060324,
          0.00016912842485060324,
          0.00022550456646747095,
          0.00022550456646747095,
          0.00033825684970120647,
          0.00033825684970120647,
          0.0005637614161686774,
          0.0005637614161686774,
          0.0007328898410192806,
          0.0007328898410192806,
          0.0010147705491036193,
          0.0010147705491036193,
          0.0011838989739542227,
          0.0011838989739542227,
          0.001296651257187958,
          0.001296651257187958,
          0.0013530273988048259,
          0.0013530273988048259,
          0.0014094035404216936,
          0.0014094035404216936,
          0.0016912842485060323,
          0.0016912842485060323,
          0.0019167888149735032,
          0.0019167888149735032,
          0.0024805502311421807,
          0.0024805502311421807,
          0.002875183222460255,
          0.002875183222460255,
          0.0032134400721614614,
          0.0032134400721614614,
          0.0034953207802458,
          0.0034953207802458,
          0.00400270605479761,
          0.00400270605479761,
          0.004453715187732551,
          0.004453715187732551,
          0.004622843612583155,
          0.004622843612583155,
          0.004679219754200023,
          0.004679219754200023,
          0.0049047243206674935,
          0.0049047243206674935,
          0.005017476603901229,
          0.005017476603901229,
          0.005186605028751832,
          0.005186605028751832,
          0.005299357311985568,
          0.005299357311985568,
          0.0055248618784530384,
          0.0055248618784530384,
          0.005637614161686774,
          0.005637614161686774,
          0.005863118728154245,
          0.005863118728154245,
          0.006426880144322923,
          0.006426880144322923,
          0.0072161461269590705,
          0.0072161461269590705,
          0.007441650693426542,
          0.007441650693426542,
          0.00749802683504341,
          0.00749802683504341,
          0.007723531401510881,
          0.007723531401510881,
          0.007836283684744616,
          0.007836283684744616,
          0.008569173525763897,
          0.008569173525763897,
          0.008681925808997633,
          0.008681925808997633,
          0.009640320216484383,
          0.009640320216484383,
          0.009809448641334987,
          0.009809448641334987,
          0.009922200924568723,
          0.009922200924568723,
          0.010542338482354268,
          0.010542338482354268,
          0.010655090765588003,
          0.010655090765588003,
          0.010824219190438607,
          0.010824219190438607,
          0.011444356748224152,
          0.011444356748224152,
          0.01172623745630849,
          0.01172623745630849,
          0.01245912729732777,
          0.01245912729732777,
          0.013417521704814523,
          0.013417521704814523,
          0.013643026271281994,
          0.013643026271281994,
          0.013755778554515728,
          0.013755778554515728,
          0.013812154696132596,
          0.013812154696132596,
          0.014375916112301275,
          0.014375916112301275,
          0.014657796820385613,
          0.014657796820385613,
          0.01471417296200248,
          0.01471417296200248,
          0.01522155823655429,
          0.01522155823655429,
          0.015503438944638629,
          0.015503438944638629,
          0.017307475476378398,
          0.017307475476378398,
          0.017589356184462734,
          0.017589356184462734,
          0.01877325515841696,
          0.01877325515841696,
          0.01944976885781937,
          0.01944976885781937,
          0.019788025707520578,
          0.019788025707520578,
          0.01995715413237118,
          0.01995715413237118,
          0.020126282557221785,
          0.020126282557221785,
          0.02023903484045552,
          0.02023903484045552,
          0.020351787123689256,
          0.020351787123689256,
          0.020577291690156724,
          0.020577291690156724,
          0.02221219979704589,
          0.02221219979704589,
          0.022832337354831436,
          0.022832337354831436,
          0.022945089638065172,
          0.022945089638065172,
          0.023565227195850715,
          0.023565227195850715,
          0.023847107903935055,
          0.023847107903935055,
          0.02395986018716879,
          0.02395986018716879,
          0.02452362160333747,
          0.02452362160333747,
          0.025876649002142295,
          0.025876649002142295,
          0.026271281993460367,
          0.026271281993460367,
          0.02694779569286278,
          0.02694779569286278,
          0.02722967640094712,
          0.02722967640094712,
          0.027342428684180856,
          0.027342428684180856,
          0.02818807080843387,
          0.02818807080843387,
          0.02942834592400496,
          0.02942834592400496,
          0.029935731198556772,
          0.029935731198556772,
          0.030781373322809787,
          0.030781373322809787,
          0.03095050174766039,
          0.03095050174766039,
          0.031908896155147144,
          0.031908896155147144,
          0.03264178599616642,
          0.03264178599616642,
          0.03292366670425076,
          0.03292366670425076,
          0.033318299695568834,
          0.033318299695568834,
          0.034333070244672456,
          0.034333070244672456,
          0.03506596008569174,
          0.03506596008569174,
          0.03540421693539294,
          0.03540421693539294,
          0.03596797835156162,
          0.03596797835156162,
          0.036306235201262825,
          0.036306235201262825,
          0.03641898748449656,
          0.03641898748449656,
          0.03664449205096403,
          0.03664449205096403,
          0.036813620475814636,
          0.036813620475814636,
          0.03698274890066524,
          0.03698274890066524,
          0.03703912504228211,
          0.03703912504228211,
          0.03811027173300259,
          0.03811027173300259,
          0.03895591385725561,
          0.03895591385725561,
          0.039012289998872476,
          0.039012289998872476,
          0.03912504228210621,
          0.03912504228210621,
          0.04121095952193032,
          0.04121095952193032,
          0.04166196865486526,
          0.04166196865486526,
          0.041774720938098996,
          0.041774720938098996,
          0.04216935392941707,
          0.04216935392941707,
          0.042338482354267674,
          0.042338482354267674,
          0.04273311534558575,
          0.04273311534558575,
          0.04357875746983876,
          0.04357875746983876,
          0.04464990416055925,
          0.04464990416055925,
          0.04549554628481227,
          0.04549554628481227,
          0.04662306911714962,
          0.04662306911714962,
          0.04690494982523396,
          0.04690494982523396,
          0.04713045439170143,
          0.04713045439170143,
          0.05124591272973278,
          0.05124591272973278,
          0.05316270154470628,
          0.05316270154470628,
          0.056319765475250874,
          0.056319765475250874,
          0.05637614161686774,
          0.05637614161686774,
          0.05722178374112076,
          0.05722178374112076,
          0.058800315706393054,
          0.058800315706393054,
          0.05947682940579547,
          0.05947682940579547,
          0.0644943060096967,
          0.0644943060096967,
          0.06545270041718344,
          0.06545270041718344,
          0.06562182884203405,
          0.06562182884203405,
          0.06579095726688465,
          0.06579095726688465,
          0.06629834254143646,
          0.06629834254143646,
          0.06748224151539069,
          0.06748224151539069,
          0.06804600293155937,
          0.06804600293155937,
          0.06894802119742925,
          0.06894802119742925,
          0.07018829631300035,
          0.07018829631300035,
          0.07035742473785093,
          0.07035742473785093,
          0.0730634795354606,
          0.0730634795354606,
          0.07373999323486301,
          0.07373999323486301,
          0.07396549780133048,
          0.07396549780133048,
          0.07965948810463412,
          0.07965948810463412,
          0.07982861652948472,
          0.07982861652948472,
          0.08101251550343895,
          0.08101251550343895,
          0.08236554290224377,
          0.08236554290224377,
          0.0842259555756004,
          0.0842259555756004,
          0.087383019506145,
          0.087383019506145,
          0.08749577178937873,
          0.08749577178937873,
          0.08817228548878114,
          0.08817228548878114,
          0.08980719359567031,
          0.08980719359567031,
          0.0905400834366896,
          0.0905400834366896,
          0.09093471642800767,
          0.09093471642800767,
          0.09161123012741008,
          0.09161123012741008,
          0.09358439508400045,
          0.09358439508400045,
          0.09392265193370165,
          0.09392265193370165,
          0.09409178035855226,
          0.09409178035855226,
          0.09600856917352577,
          0.09600856917352577,
          0.09674145901454505,
          0.09674145901454505,
          0.09724884428909686,
          0.09724884428909686,
          0.09848911940466794,
          0.09848911940466794,
          0.09877100011275228,
          0.09877100011275228,
          0.10080054121095952,
          0.10080054121095952,
          0.10102604577742699,
          0.10102604577742699,
          0.10581801781486075,
          0.10581801781486075,
          0.10615627466456196,
          0.10615627466456196,
          0.11089187056037884,
          0.11089187056037884,
          0.11309054008343669,
          0.11309054008343669,
          0.11387980606607284,
          0.11387980606607284,
          0.1158529710226632,
          0.1158529710226632,
          0.11596572330589694,
          0.11596572330589694,
          0.11720599842146803,
          0.11720599842146803,
          0.12075769534333071,
          0.12075769534333071,
          0.12853760288645846,
          0.12853760288645846,
          0.12943962115232832,
          0.12943962115232832,
          0.13186379524185365,
          0.13186379524185365,
          0.1326530612244898,
          0.1326530612244898,
          0.132991318074191,
          0.132991318074191,
          0.13383696019844402,
          0.13383696019844402,
          0.13394971248167775,
          0.13394971248167775,
          0.1350772353140151,
          0.1350772353140151,
          0.13569737287180064,
          0.13569737287180064,
          0.1364866388544368,
          0.1364866388544368,
          0.13733228097868982,
          0.13733228097868982,
          0.13772691397000789,
          0.13772691397000789,
          0.14139136317510428,
          0.14139136317510428,
          0.14150411545833802,
          0.14150411545833802,
          0.1415604915999549,
          0.1415604915999549,
          0.1427443905739091,
          0.1427443905739091,
          0.14466117938888262,
          0.14466117938888262,
          0.15576727928740558,
          0.15576727928740558,
          0.1571203066862104,
          0.1571203066862104,
          0.15914984778441763,
          0.15914984778441763,
          0.15960085691735257,
          0.15960085691735257,
          0.16439282895478632,
          0.16439282895478632,
          0.1754425527116924,
          0.1754425527116924,
          0.1784304882173864,
          0.1784304882173864,
          0.18012177246589242,
          0.18012177246589242,
          0.18694328560153342,
          0.18694328560153342,
          0.1894802119742925,
          0.1894802119742925,
          0.19810576164167323,
          0.19810576164167323,
          0.19917690833239374,
          0.19917690833239374,
          0.20007892659826362,
          0.20007892659826362,
          0.20752057729169016,
          0.20752057729169016,
          0.2103957605141504,
          0.2103957605141504,
          0.2107340173638516,
          0.2107340173638516,
          0.21428571428571427,
          0.21428571428571427,
          0.21665351223362272,
          0.21665351223362272,
          0.22167098883752395,
          0.22167098883752395,
          0.24579997744954335,
          0.24579997744954335,
          0.24822415153906865,
          0.24822415153906865,
          0.24895704138008795,
          0.24895704138008795,
          0.25087383019506143,
          0.25087383019506143,
          0.25245236216033373,
          0.25245236216033373,
          0.25329800428458676,
          0.25329800428458676,
          0.2575825910474687,
          0.2575825910474687,
          0.2612470402525651,
          0.2612470402525651,
          0.2702108467696471,
          0.2702108467696471,
          0.27195850715977,
          0.27195850715977,
          0.2768068553388206,
          0.2768068553388206,
          0.2999210734017364,
          0.2999210734017364,
          0.30555868756342314,
          0.30555868756342314,
          0.3113090540083437,
          0.3113090540083437,
          0.3182433194272184,
          0.3182433194272184,
          0.3191453376930883,
          0.3191453376930883,
          0.3276581350772353,
          0.3276581350772353,
          0.32934941932574135,
          0.32934941932574135,
          0.3394971248167775,
          0.3394971248167775,
          0.3552824444695005,
          0.3552824444695005,
          0.3600744165069343,
          0.3600744165069343,
          0.3624422144548427,
          0.3624422144548427,
          0.3689254707407825,
          0.3689254707407825,
          0.3703348742812042,
          0.3703348742812042,
          0.37546510316833914,
          0.37546510316833914,
          0.3773818919833127,
          0.3773818919833127,
          0.3965497801330477,
          0.3965497801330477,
          0.4035404216935393,
          0.4035404216935393,
          0.4188183560717105,
          0.4188183560717105,
          0.42490697936633215,
          0.42490697936633215,
          0.4340962904498816,
          0.4340962904498816,
          0.4382681249295298,
          0.4382681249295298,
          0.44655541774720936,
          0.44655541774720936,
          0.5158416957943398,
          0.5158416957943398,
          0.5501747660390123,
          0.5501747660390123,
          0.5522606832788364,
          0.5522606832788364,
          0.6462397113541549,
          0.6462397113541549,
          0.7256736948923216,
          0.7256736948923216,
          0.8556770774608186,
          0.8556770774608186,
          1
         ],
         "xaxis": "x2",
         "y": [
          0,
          0.0038910505836575876,
          0.0038910505836575876,
          0.007782101167315175,
          0.007782101167315175,
          0.011673151750972763,
          0.011673151750972763,
          0.01556420233463035,
          0.01556420233463035,
          0.019455252918287938,
          0.019455252918287938,
          0.023346303501945526,
          0.023346303501945526,
          0.027237354085603113,
          0.027237354085603113,
          0.0311284046692607,
          0.0311284046692607,
          0.03501945525291829,
          0.03501945525291829,
          0.04669260700389105,
          0.04669260700389105,
          0.05058365758754864,
          0.05058365758754864,
          0.054474708171206226,
          0.054474708171206226,
          0.058365758754863814,
          0.058365758754863814,
          0.0622568093385214,
          0.0622568093385214,
          0.06614785992217899,
          0.06614785992217899,
          0.07003891050583658,
          0.07003891050583658,
          0.07392996108949416,
          0.07392996108949416,
          0.08171206225680934,
          0.08171206225680934,
          0.08949416342412451,
          0.08949416342412451,
          0.09727626459143969,
          0.09727626459143969,
          0.10116731517509728,
          0.10116731517509728,
          0.10894941634241245,
          0.10894941634241245,
          0.11284046692607004,
          0.11284046692607004,
          0.11673151750972763,
          0.11673151750972763,
          0.12062256809338522,
          0.12062256809338522,
          0.1245136186770428,
          0.1245136186770428,
          0.12840466926070038,
          0.12840466926070038,
          0.13229571984435798,
          0.13229571984435798,
          0.13618677042801555,
          0.13618677042801555,
          0.14007782101167315,
          0.14007782101167315,
          0.14396887159533073,
          0.14396887159533073,
          0.1517509727626459,
          0.1517509727626459,
          0.1556420233463035,
          0.1556420233463035,
          0.15953307392996108,
          0.15953307392996108,
          0.16342412451361868,
          0.16342412451361868,
          0.16731517509727625,
          0.16731517509727625,
          0.17120622568093385,
          0.17120622568093385,
          0.1828793774319066,
          0.1828793774319066,
          0.19066147859922178,
          0.19066147859922178,
          0.19455252918287938,
          0.19455252918287938,
          0.19844357976653695,
          0.19844357976653695,
          0.20233463035019456,
          0.20233463035019456,
          0.20622568093385213,
          0.20622568093385213,
          0.21011673151750973,
          0.21011673151750973,
          0.2140077821011673,
          0.2140077821011673,
          0.2178988326848249,
          0.2178988326848249,
          0.22178988326848248,
          0.22178988326848248,
          0.22568093385214008,
          0.22568093385214008,
          0.22957198443579765,
          0.22957198443579765,
          0.23735408560311283,
          0.23735408560311283,
          0.245136186770428,
          0.245136186770428,
          0.2490272373540856,
          0.2490272373540856,
          0.25680933852140075,
          0.25680933852140075,
          0.2607003891050584,
          0.2607003891050584,
          0.26459143968871596,
          0.26459143968871596,
          0.2723735408560311,
          0.2723735408560311,
          0.27626459143968873,
          0.27626459143968873,
          0.2801556420233463,
          0.2801556420233463,
          0.2840466926070039,
          0.2840466926070039,
          0.28793774319066145,
          0.28793774319066145,
          0.2918287937743191,
          0.2918287937743191,
          0.29571984435797666,
          0.29571984435797666,
          0.29961089494163423,
          0.29961089494163423,
          0.3035019455252918,
          0.3035019455252918,
          0.30739299610894943,
          0.30739299610894943,
          0.311284046692607,
          0.311284046692607,
          0.3151750972762646,
          0.3151750972762646,
          0.31906614785992216,
          0.31906614785992216,
          0.3229571984435798,
          0.3229571984435798,
          0.32684824902723736,
          0.32684824902723736,
          0.33073929961089493,
          0.33073929961089493,
          0.3346303501945525,
          0.3346303501945525,
          0.33852140077821014,
          0.33852140077821014,
          0.3424124513618677,
          0.3424124513618677,
          0.3463035019455253,
          0.3463035019455253,
          0.35019455252918286,
          0.35019455252918286,
          0.3540856031128405,
          0.3540856031128405,
          0.35797665369649806,
          0.35797665369649806,
          0.36186770428015563,
          0.36186770428015563,
          0.3657587548638132,
          0.3657587548638132,
          0.36964980544747084,
          0.36964980544747084,
          0.3735408560311284,
          0.3735408560311284,
          0.377431906614786,
          0.377431906614786,
          0.38132295719844356,
          0.38132295719844356,
          0.38910505836575876,
          0.38910505836575876,
          0.39299610894941633,
          0.39299610894941633,
          0.3968871595330739,
          0.3968871595330739,
          0.40077821011673154,
          0.40077821011673154,
          0.4046692607003891,
          0.4046692607003891,
          0.4085603112840467,
          0.4085603112840467,
          0.41245136186770426,
          0.41245136186770426,
          0.4163424124513619,
          0.4163424124513619,
          0.42023346303501946,
          0.42023346303501946,
          0.42412451361867703,
          0.42412451361867703,
          0.4280155642023346,
          0.4280155642023346,
          0.43190661478599224,
          0.43190661478599224,
          0.4357976653696498,
          0.4357976653696498,
          0.4396887159533074,
          0.4396887159533074,
          0.4474708171206226,
          0.4474708171206226,
          0.45136186770428016,
          0.45136186770428016,
          0.45525291828793774,
          0.45525291828793774,
          0.4591439688715953,
          0.4591439688715953,
          0.4708171206225681,
          0.4708171206225681,
          0.47470817120622566,
          0.47470817120622566,
          0.4785992217898833,
          0.4785992217898833,
          0.48249027237354086,
          0.48249027237354086,
          0.48638132295719844,
          0.48638132295719844,
          0.490272373540856,
          0.490272373540856,
          0.49416342412451364,
          0.49416342412451364,
          0.4980544747081712,
          0.4980544747081712,
          0.5019455252918288,
          0.5019455252918288,
          0.5058365758754864,
          0.5058365758754864,
          0.5097276264591439,
          0.5097276264591439,
          0.5136186770428015,
          0.5136186770428015,
          0.5175097276264592,
          0.5175097276264592,
          0.5214007782101168,
          0.5214007782101168,
          0.5252918287937743,
          0.5252918287937743,
          0.5291828793774319,
          0.5291828793774319,
          0.5330739299610895,
          0.5330739299610895,
          0.5408560311284046,
          0.5408560311284046,
          0.5447470817120622,
          0.5447470817120622,
          0.5486381322957199,
          0.5486381322957199,
          0.5525291828793775,
          0.5525291828793775,
          0.556420233463035,
          0.556420233463035,
          0.5603112840466926,
          0.5603112840466926,
          0.5642023346303502,
          0.5642023346303502,
          0.5680933852140078,
          0.5680933852140078,
          0.5719844357976653,
          0.5719844357976653,
          0.5758754863813229,
          0.5758754863813229,
          0.5797665369649806,
          0.5797665369649806,
          0.5836575875486382,
          0.5836575875486382,
          0.5875486381322957,
          0.5875486381322957,
          0.5914396887159533,
          0.5914396887159533,
          0.5953307392996109,
          0.5953307392996109,
          0.5992217898832685,
          0.5992217898832685,
          0.603112840466926,
          0.603112840466926,
          0.6070038910505836,
          0.6070038910505836,
          0.6108949416342413,
          0.6108949416342413,
          0.6147859922178989,
          0.6147859922178989,
          0.6186770428015564,
          0.6186770428015564,
          0.622568093385214,
          0.622568093385214,
          0.6264591439688716,
          0.6264591439688716,
          0.6303501945525292,
          0.6303501945525292,
          0.6342412451361867,
          0.6342412451361867,
          0.6381322957198443,
          0.6381322957198443,
          0.642023346303502,
          0.642023346303502,
          0.6459143968871596,
          0.6459143968871596,
          0.6498054474708171,
          0.6498054474708171,
          0.6536964980544747,
          0.6536964980544747,
          0.6575875486381323,
          0.6575875486381323,
          0.6614785992217899,
          0.6614785992217899,
          0.6653696498054474,
          0.6653696498054474,
          0.669260700389105,
          0.669260700389105,
          0.6731517509727627,
          0.6731517509727627,
          0.6770428015564203,
          0.6770428015564203,
          0.6809338521400778,
          0.6809338521400778,
          0.6848249027237354,
          0.6848249027237354,
          0.688715953307393,
          0.688715953307393,
          0.6926070038910506,
          0.6926070038910506,
          0.6964980544747081,
          0.6964980544747081,
          0.7003891050583657,
          0.7003891050583657,
          0.7042801556420234,
          0.7042801556420234,
          0.708171206225681,
          0.708171206225681,
          0.7120622568093385,
          0.7120622568093385,
          0.7159533073929961,
          0.7159533073929961,
          0.7198443579766537,
          0.7198443579766537,
          0.7237354085603113,
          0.7237354085603113,
          0.7276264591439688,
          0.7276264591439688,
          0.7315175097276264,
          0.7315175097276264,
          0.7354085603112841,
          0.7354085603112841,
          0.7392996108949417,
          0.7392996108949417,
          0.7431906614785992,
          0.7431906614785992,
          0.7470817120622568,
          0.7470817120622568,
          0.7509727626459144,
          0.7509727626459144,
          0.754863813229572,
          0.754863813229572,
          0.7587548638132295,
          0.7587548638132295,
          0.7626459143968871,
          0.7626459143968871,
          0.7665369649805448,
          0.7665369649805448,
          0.7704280155642024,
          0.7704280155642024,
          0.77431906614786,
          0.77431906614786,
          0.7782101167315175,
          0.7782101167315175,
          0.7821011673151751,
          0.7821011673151751,
          0.7859922178988327,
          0.7859922178988327,
          0.7898832684824902,
          0.7898832684824902,
          0.7937743190661478,
          0.7937743190661478,
          0.7976653696498055,
          0.7976653696498055,
          0.8015564202334631,
          0.8015564202334631,
          0.8054474708171206,
          0.8054474708171206,
          0.8093385214007782,
          0.8093385214007782,
          0.8132295719844358,
          0.8132295719844358,
          0.8171206225680934,
          0.8171206225680934,
          0.8210116731517509,
          0.8210116731517509,
          0.8249027237354085,
          0.8249027237354085,
          0.8287937743190662,
          0.8287937743190662,
          0.8326848249027238,
          0.8326848249027238,
          0.8365758754863813,
          0.8365758754863813,
          0.8404669260700389,
          0.8404669260700389,
          0.8443579766536965,
          0.8443579766536965,
          0.8482490272373541,
          0.8482490272373541,
          0.8521400778210116,
          0.8521400778210116,
          0.8560311284046692,
          0.8560311284046692,
          0.8599221789883269,
          0.8599221789883269,
          0.8638132295719845,
          0.8638132295719845,
          0.867704280155642,
          0.867704280155642,
          0.8715953307392996,
          0.8715953307392996,
          0.8754863813229572,
          0.8754863813229572,
          0.8793774319066148,
          0.8793774319066148,
          0.8832684824902723,
          0.8832684824902723,
          0.8871595330739299,
          0.8871595330739299,
          0.8910505836575876,
          0.8910505836575876,
          0.8949416342412452,
          0.8949416342412452,
          0.8988326848249028,
          0.8988326848249028,
          0.9027237354085603,
          0.9027237354085603,
          0.9066147859922179,
          0.9066147859922179,
          0.9105058365758755,
          0.9105058365758755,
          0.914396887159533,
          0.914396887159533,
          0.9182879377431906,
          0.9182879377431906,
          0.9221789883268483,
          0.9221789883268483,
          0.9260700389105059,
          0.9260700389105059,
          0.9299610894941635,
          0.9299610894941635,
          0.933852140077821,
          0.933852140077821,
          0.9377431906614786,
          0.9377431906614786,
          0.9416342412451362,
          0.9416342412451362,
          0.9455252918287937,
          0.9455252918287937,
          0.9494163424124513,
          0.9494163424124513,
          0.953307392996109,
          0.953307392996109,
          0.9571984435797666,
          0.9571984435797666,
          0.9610894941634242,
          0.9610894941634242,
          0.9649805447470817,
          0.9649805447470817,
          0.9688715953307393,
          0.9688715953307393,
          0.9727626459143969,
          0.9727626459143969,
          0.9766536964980544,
          0.9766536964980544,
          0.980544747081712,
          0.980544747081712,
          0.9844357976653697,
          0.9844357976653697,
          0.9883268482490273,
          0.9883268482490273,
          0.9922178988326849,
          0.9922178988326849,
          0.9961089494163424,
          0.9961089494163424,
          1,
          1
         ],
         "yaxis": "y2"
        },
        {
         "fill": "tozeroy",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          5.637614161686774e-05,
          0.0002818807080843387,
          0.0002818807080843387,
          0.0007328898410192806,
          0.0007328898410192806,
          0.0008456421242530162,
          0.0008456421242530162,
          0.0009020182658698838,
          0.0009020182658698838,
          0.0011838989739542227,
          0.0011838989739542227,
          0.001522155823655429,
          0.001522155823655429,
          0.0016912842485060323,
          0.0016912842485060323,
          0.002424174089525313,
          0.002424174089525313,
          0.0027624309392265192,
          0.0027624309392265192,
          0.0032134400721614614,
          0.0032134400721614614,
          0.0033261923553951967,
          0.0033261923553951967,
          0.0033825684970120646,
          0.0033825684970120646,
          0.003438944638628932,
          0.003438944638628932,
          0.003551696921862668,
          0.003551696921862668,
          0.004340962904498816,
          0.004340962904498816,
          0.004397339046115683,
          0.004397339046115683,
          0.005073852745518097,
          0.005073852745518097,
          0.005299357311985568,
          0.005299357311985568,
          0.0054121095952193035,
          0.0054121095952193035,
          0.00575036644492051,
          0.00575036644492051,
          0.005975871011387981,
          0.005975871011387981,
          0.006596008569173526,
          0.006596008569173526,
          0.006934265418874732,
          0.006934265418874732,
          0.0071597699853422035,
          0.0071597699853422035,
          0.00800541210959522,
          0.00800541210959522,
          0.008174540534445822,
          0.008174540534445822,
          0.008400045100913293,
          0.008400045100913293,
          0.009132934941932574,
          0.009132934941932574,
          0.009414815650016914,
          0.009414815650016914,
          0.009696696358101251,
          0.009696696358101251,
          0.009753072499718119,
          0.009753072499718119,
          0.010147705491036194,
          0.010147705491036194,
          0.010316833915886796,
          0.010316833915886796,
          0.010598714623971136,
          0.010598714623971136,
          0.010655090765588003,
          0.010655090765588003,
          0.010824219190438607,
          0.010824219190438607,
          0.010993347615289209,
          0.010993347615289209,
          0.011387980606607284,
          0.011387980606607284,
          0.011557109031457888,
          0.011557109031457888,
          0.011613485173074754,
          0.011613485173074754,
          0.012289998872477167,
          0.012289998872477167,
          0.012515503438944638,
          0.012515503438944638,
          0.013022888713496448,
          0.013022888713496448,
          0.013248393279963919,
          0.013248393279963919,
          0.013755778554515728,
          0.013755778554515728,
          0.01522155823655429,
          0.01522155823655429,
          0.015334310519788025,
          0.015334310519788025,
          0.015447062803021761,
          0.015447062803021761,
          0.015672567369489233,
          0.015672567369489233,
          0.016292704927274776,
          0.016292704927274776,
          0.01646183335212538,
          0.01646183335212538,
          0.016630961776975983,
          0.016630961776975983,
          0.01674371406020972,
          0.01674371406020972,
          0.01725109933476153,
          0.01725109933476153,
          0.01849137445033262,
          0.01849137445033262,
          0.01871687901680009,
          0.01871687901680009,
          0.018886007441650695,
          0.018886007441650695,
          0.019055135866501295,
          0.019055135866501295,
          0.019280640432968767,
          0.019280640432968767,
          0.02040816326530612,
          0.02040816326530612,
          0.02221219979704589,
          0.02221219979704589,
          0.02249408050513023,
          0.02249408050513023,
          0.023339722629383244,
          0.023339722629383244,
          0.02345247491261698,
          0.02345247491261698,
          0.023508851054233847,
          0.023508851054233847,
          0.023790731762318187,
          0.023790731762318187,
          0.023847107903935055,
          0.023847107903935055,
          0.024354493178486866,
          0.024354493178486866,
          0.02491825459465554,
          0.02491825459465554,
          0.025031006877889277,
          0.025031006877889277,
          0.025312887585973616,
          0.025312887585973616,
          0.025538392152441088,
          0.025538392152441088,
          0.025651144435674823,
          0.025651144435674823,
          0.02576389671890856,
          0.02576389671890856,
          0.026327658135077234,
          0.026327658135077234,
          0.026609538843161574,
          0.026609538843161574,
          0.030555868756342315,
          0.030555868756342315,
          0.030781373322809787,
          0.030781373322809787,
          0.03123238245574473,
          0.03123238245574473,
          0.032754538279400155,
          0.032754538279400155,
          0.03286729056263389,
          0.03286729056263389,
          0.033318299695568834,
          0.033318299695568834,
          0.035235088510542335,
          0.035235088510542335,
          0.03534784079377607,
          0.03534784079377607,
          0.036080730634795354,
          0.036080730634795354,
          0.03754651031683392,
          0.03754651031683392,
          0.037602886458450786,
          0.037602886458450786,
          0.03782839102491826,
          0.03782839102491826,
          0.038223024016236326,
          0.038223024016236326,
          0.03833577629947006,
          0.03833577629947006,
          0.03839215244108693,
          0.03839215244108693,
          0.04059082196414478,
          0.04059082196414478,
          0.04081632653061224,
          0.04081632653061224,
          0.04295861991205322,
          0.04295861991205322,
          0.043353252903371296,
          0.043353252903371296,
          0.04391701431953997,
          0.04391701431953997,
          0.043973390461156836,
          0.043973390461156836,
          0.045326417859961665,
          0.045326417859961665,
          0.04628481226744842,
          0.04628481226744842,
          0.04634118840906529,
          0.04634118840906529,
          0.04639756455068215,
          0.04639756455068215,
          0.04679219754200022,
          0.04679219754200022,
          0.048370729507272524,
          0.048370729507272524,
          0.0487653624985906,
          0.0487653624985906,
          0.048878114781824335,
          0.048878114781824335,
          0.04899086706505807,
          0.04899086706505807,
          0.04955462848122674,
          0.04955462848122674,
          0.05073852745518097,
          0.05073852745518097,
          0.05152779343781712,
          0.05152779343781712,
          0.05175329800428459,
          0.05175329800428459,
          0.052768068553388205,
          0.052768068553388205,
          0.05541774720938099,
          0.05541774720938099,
          0.05615063705040027,
          0.05615063705040027,
          0.06161912278723644,
          0.06161912278723644,
          0.0627466456195738,
          0.0627466456195738,
          0.06285939790280753,
          0.06285939790280753,
          0.06596008569173525,
          0.06596008569173525,
          0.06618559025820273,
          0.06618559025820273,
          0.06629834254143646,
          0.06629834254143646,
          0.06894802119742925,
          0.06894802119742925,
          0.0693990303303642,
          0.0693990303303642,
          0.07114669072048709,
          0.07114669072048709,
          0.07396549780133048,
          0.07396549780133048,
          0.07802457999774495,
          0.07802457999774495,
          0.07926485511331605,
          0.07926485511331605,
          0.08056150637050401,
          0.08056150637050401,
          0.08354944187619799,
          0.08354944187619799,
          0.08394407486751607,
          0.08394407486751607,
          0.08501522155823656,
          0.08501522155823656,
          0.08766490021422933,
          0.08766490021422933,
          0.08975081745405344,
          0.08975081745405344,
          0.09155485398579322,
          0.09155485398579322,
          0.09178035855226069,
          0.09178035855226069,
          0.09200586311872816,
          0.09200586311872816,
          0.09358439508400045,
          0.09358439508400045,
          0.0945427894914872,
          0.0945427894914872,
          0.09510655090765588,
          0.09510655090765588,
          0.09533205547412335,
          0.09533205547412335,
          0.09617769759837637,
          0.09617769759837637,
          0.09860187168790167,
          0.09860187168790167,
          0.10034953207802458,
          0.10034953207802458,
          0.10158980719359567,
          0.10158980719359567,
          0.10288645845078363,
          0.10288645845078363,
          0.10547976096515954,
          0.10547976096515954,
          0.10570526553162701,
          0.10570526553162701,
          0.10615627466456196,
          0.10615627466456196,
          0.10739654978013305,
          0.10739654978013305,
          0.10796031119630173,
          0.10796031119630173,
          0.10897508174540535,
          0.10897508174540535,
          0.11083549441876198,
          0.11083549441876198,
          0.11089187056037884,
          0.11089187056037884,
          0.11399255834930658,
          0.11399255834930658,
          0.11416168677415718,
          0.11416168677415718,
          0.11478182433194273,
          0.11478182433194273,
          0.11951742022775962,
          0.11951742022775962,
          0.11963017251099335,
          0.11963017251099335,
          0.1209268237681813,
          0.1209268237681813,
          0.12205434660051866,
          0.12205434660051866,
          0.1225053557334536,
          0.1225053557334536,
          0.12752283233735484,
          0.12752283233735484,
          0.1279174653286729,
          0.1279174653286729,
          0.12910136430262714,
          0.12910136430262714,
          0.12994700642688015,
          0.12994700642688015,
          0.13152553839215245,
          0.13152553839215245,
          0.1335550794903597,
          0.1335550794903597,
          0.13699402412898862,
          0.13699402412898862,
          0.13744503326192356,
          0.13744503326192356,
          0.13975645506821513,
          0.13975645506821513,
          0.1408839779005525,
          0.1408839779005525,
          0.15041154583380315,
          0.15041154583380315,
          0.15311760063141278,
          0.15311760063141278,
          0.15334310519788025,
          0.15334310519788025,
          0.15368136204758145,
          0.15368136204758145,
          0.15841695794339836,
          0.15841695794339836,
          0.16039012289998872,
          0.16039012289998872,
          0.16281429698951405,
          0.16281429698951405,
          0.16625324162814298,
          0.16625324162814298,
          0.1684519111512008,
          0.1684519111512008,
          0.17403314917127072,
          0.17403314917127072,
          0.1758371857030105,
          0.1758371857030105,
          0.17600631412786108,
          0.17600631412786108,
          0.1835043409629045,
          0.1835043409629045,
          0.18711241402638404,
          0.18711241402638404,
          0.18818356071710451,
          0.18818356071710451,
          0.18863456985003946,
          0.18863456985003946,
          0.19246814747998647,
          0.19246814747998647,
          0.20334874281204193,
          0.20334874281204193,
          0.2044762656443793,
          0.2044762656443793,
          0.21417296200248054,
          0.21417296200248054,
          0.22888713496448304,
          0.22888713496448304,
          0.23187507047017702,
          0.23187507047017702,
          0.23548314353365657,
          0.23548314353365657,
          0.23683617093246137,
          0.23683617093246137,
          0.23779456533994814,
          0.23779456533994814,
          0.2460818581576277,
          0.2460818581576277,
          0.25369263727590485,
          0.25369263727590485,
          0.25431277483369036,
          0.25431277483369036,
          0.2560604352238133,
          0.2560604352238133,
          0.25887924230465664,
          0.25887924230465664,
          0.26203630623520124,
          0.26203630623520124,
          0.2646296087495772,
          0.2646296087495772,
          0.2693652046453941,
          0.2693652046453941,
          0.27094373661066634,
          0.27094373661066634,
          0.2717893787349194,
          0.2717893787349194,
          0.2743826812492953,
          0.2743826812492953,
          0.2764122223475025,
          0.2764122223475025,
          0.2891532303529146,
          0.2891532303529146,
          0.29738414702897736,
          0.29738414702897736,
          0.30009020182658697,
          0.30009020182658697,
          0.332055474123351,
          0.332055474123351,
          0.33509978577066185,
          0.33509978577066185,
          0.3507723531401511,
          0.3507723531401511,
          0.375972488442891,
          0.375972488442891,
          0.38741684519111513,
          0.38741684519111513,
          0.38769872589919946,
          0.38769872589919946,
          0.38961551471417294,
          0.38961551471417294,
          0.4034840455519224,
          0.4034840455519224,
          0.4349983087157515,
          0.4349983087157515,
          0.4681474799864697,
          0.4681474799864697,
          0.4703461495095276,
          0.4703461495095276,
          0.5039463299131808,
          0.5039463299131808,
          0.5113879806066073,
          0.5113879806066073,
          0.5233397226293832,
          0.5233397226293832,
          0.5271169241177134,
          0.5271169241177134,
          0.5489344909234412,
          0.5489344909234412,
          0.563084902469275,
          0.563084902469275,
          0.5727252226857594,
          0.5727252226857594,
          0.5849588454166197,
          0.5849588454166197,
          0.5863118728154245,
          0.5863118728154245,
          0.5912729732777089,
          0.5912729732777089,
          0.6079039350546849,
          0.6079039350546849,
          0.6319201713834706,
          0.6319201713834706,
          0.7785545157289435,
          0.7785545157289435,
          0.838425978126057,
          0.838425978126057,
          0.9158867967076333,
          0.9158867967076333,
          1
         ],
         "xaxis": "x3",
         "y": [
          0,
          0,
          0,
          0.007751937984496124,
          0.007751937984496124,
          0.01937984496124031,
          0.01937984496124031,
          0.027131782945736434,
          0.027131782945736434,
          0.03488372093023256,
          0.03488372093023256,
          0.03875968992248062,
          0.03875968992248062,
          0.04263565891472868,
          0.04263565891472868,
          0.05813953488372093,
          0.05813953488372093,
          0.06201550387596899,
          0.06201550387596899,
          0.06976744186046512,
          0.06976744186046512,
          0.07364341085271318,
          0.07364341085271318,
          0.07751937984496124,
          0.07751937984496124,
          0.08139534883720931,
          0.08139534883720931,
          0.08527131782945736,
          0.08527131782945736,
          0.08914728682170543,
          0.08914728682170543,
          0.09302325581395349,
          0.09302325581395349,
          0.09689922480620156,
          0.09689922480620156,
          0.10465116279069768,
          0.10465116279069768,
          0.10852713178294573,
          0.10852713178294573,
          0.1124031007751938,
          0.1124031007751938,
          0.11627906976744186,
          0.11627906976744186,
          0.12015503875968993,
          0.12015503875968993,
          0.13178294573643412,
          0.13178294573643412,
          0.13565891472868216,
          0.13565891472868216,
          0.13953488372093023,
          0.13953488372093023,
          0.1434108527131783,
          0.1434108527131783,
          0.14728682170542637,
          0.14728682170542637,
          0.1511627906976744,
          0.1511627906976744,
          0.15503875968992248,
          0.15503875968992248,
          0.15891472868217055,
          0.15891472868217055,
          0.16279069767441862,
          0.16279069767441862,
          0.16666666666666666,
          0.16666666666666666,
          0.17054263565891473,
          0.17054263565891473,
          0.1744186046511628,
          0.1744186046511628,
          0.17829457364341086,
          0.17829457364341086,
          0.1821705426356589,
          0.1821705426356589,
          0.18604651162790697,
          0.18604651162790697,
          0.18992248062015504,
          0.18992248062015504,
          0.1937984496124031,
          0.1937984496124031,
          0.19767441860465115,
          0.19767441860465115,
          0.20155038759689922,
          0.20155038759689922,
          0.2054263565891473,
          0.2054263565891473,
          0.20930232558139536,
          0.20930232558139536,
          0.2131782945736434,
          0.2131782945736434,
          0.21705426356589147,
          0.21705426356589147,
          0.22093023255813954,
          0.22093023255813954,
          0.2248062015503876,
          0.2248062015503876,
          0.22868217054263565,
          0.22868217054263565,
          0.2364341085271318,
          0.2364341085271318,
          0.24031007751937986,
          0.24031007751937986,
          0.24806201550387597,
          0.24806201550387597,
          0.25193798449612403,
          0.25193798449612403,
          0.2558139534883721,
          0.2558139534883721,
          0.2596899224806202,
          0.2596899224806202,
          0.26744186046511625,
          0.26744186046511625,
          0.2713178294573643,
          0.2713178294573643,
          0.2751937984496124,
          0.2751937984496124,
          0.28294573643410853,
          0.28294573643410853,
          0.2868217054263566,
          0.2868217054263566,
          0.29069767441860467,
          0.29069767441860467,
          0.29457364341085274,
          0.29457364341085274,
          0.29844961240310075,
          0.29844961240310075,
          0.3023255813953488,
          0.3023255813953488,
          0.3062015503875969,
          0.3062015503875969,
          0.31007751937984496,
          0.31007751937984496,
          0.313953488372093,
          0.313953488372093,
          0.3178294573643411,
          0.3178294573643411,
          0.32170542635658916,
          0.32170542635658916,
          0.32558139534883723,
          0.32558139534883723,
          0.32945736434108525,
          0.32945736434108525,
          0.3333333333333333,
          0.3333333333333333,
          0.3372093023255814,
          0.3372093023255814,
          0.34108527131782945,
          0.34108527131782945,
          0.3449612403100775,
          0.3449612403100775,
          0.35271317829457366,
          0.35271317829457366,
          0.35658914728682173,
          0.35658914728682173,
          0.3643410852713178,
          0.3643410852713178,
          0.3682170542635659,
          0.3682170542635659,
          0.37209302325581395,
          0.37209302325581395,
          0.375968992248062,
          0.375968992248062,
          0.3798449612403101,
          0.3798449612403101,
          0.38372093023255816,
          0.38372093023255816,
          0.3875968992248062,
          0.3875968992248062,
          0.3953488372093023,
          0.3953488372093023,
          0.3992248062015504,
          0.3992248062015504,
          0.40310077519379844,
          0.40310077519379844,
          0.4069767441860465,
          0.4069767441860465,
          0.4108527131782946,
          0.4108527131782946,
          0.41472868217054265,
          0.41472868217054265,
          0.4186046511627907,
          0.4186046511627907,
          0.42248062015503873,
          0.42248062015503873,
          0.4263565891472868,
          0.4263565891472868,
          0.43023255813953487,
          0.43023255813953487,
          0.43410852713178294,
          0.43410852713178294,
          0.437984496124031,
          0.437984496124031,
          0.4418604651162791,
          0.4418604651162791,
          0.44573643410852715,
          0.44573643410852715,
          0.4496124031007752,
          0.4496124031007752,
          0.45348837209302323,
          0.45348837209302323,
          0.4573643410852713,
          0.4573643410852713,
          0.46124031007751937,
          0.46124031007751937,
          0.46511627906976744,
          0.46511627906976744,
          0.4689922480620155,
          0.4689922480620155,
          0.4728682170542636,
          0.4728682170542636,
          0.47674418604651164,
          0.47674418604651164,
          0.4806201550387597,
          0.4806201550387597,
          0.4844961240310077,
          0.4844961240310077,
          0.49612403100775193,
          0.49612403100775193,
          0.5,
          0.5,
          0.5038759689922481,
          0.5038759689922481,
          0.5077519379844961,
          0.5077519379844961,
          0.5116279069767442,
          0.5116279069767442,
          0.5155038759689923,
          0.5155038759689923,
          0.5232558139534884,
          0.5232558139534884,
          0.5271317829457365,
          0.5271317829457365,
          0.5310077519379846,
          0.5310077519379846,
          0.5348837209302325,
          0.5348837209302325,
          0.5387596899224806,
          0.5387596899224806,
          0.5426356589147286,
          0.5426356589147286,
          0.5465116279069767,
          0.5465116279069767,
          0.5503875968992248,
          0.5503875968992248,
          0.5542635658914729,
          0.5542635658914729,
          0.5581395348837209,
          0.5581395348837209,
          0.562015503875969,
          0.562015503875969,
          0.5658914728682171,
          0.5658914728682171,
          0.5697674418604651,
          0.5697674418604651,
          0.5736434108527132,
          0.5736434108527132,
          0.5775193798449613,
          0.5775193798449613,
          0.5813953488372093,
          0.5813953488372093,
          0.5852713178294574,
          0.5852713178294574,
          0.5891472868217055,
          0.5891472868217055,
          0.5930232558139535,
          0.5930232558139535,
          0.5968992248062015,
          0.5968992248062015,
          0.6007751937984496,
          0.6007751937984496,
          0.6046511627906976,
          0.6046511627906976,
          0.6085271317829457,
          0.6085271317829457,
          0.6124031007751938,
          0.6124031007751938,
          0.6162790697674418,
          0.6162790697674418,
          0.6201550387596899,
          0.6201550387596899,
          0.624031007751938,
          0.624031007751938,
          0.627906976744186,
          0.627906976744186,
          0.6317829457364341,
          0.6317829457364341,
          0.6356589147286822,
          0.6356589147286822,
          0.6395348837209303,
          0.6395348837209303,
          0.6434108527131783,
          0.6434108527131783,
          0.6472868217054264,
          0.6472868217054264,
          0.6511627906976745,
          0.6511627906976745,
          0.6550387596899225,
          0.6550387596899225,
          0.6589147286821705,
          0.6589147286821705,
          0.6627906976744186,
          0.6627906976744186,
          0.6666666666666666,
          0.6666666666666666,
          0.6705426356589147,
          0.6705426356589147,
          0.6744186046511628,
          0.6744186046511628,
          0.6782945736434108,
          0.6782945736434108,
          0.6821705426356589,
          0.6821705426356589,
          0.686046511627907,
          0.686046511627907,
          0.689922480620155,
          0.689922480620155,
          0.6937984496124031,
          0.6937984496124031,
          0.6976744186046512,
          0.6976744186046512,
          0.7015503875968992,
          0.7015503875968992,
          0.7054263565891473,
          0.7054263565891473,
          0.7093023255813954,
          0.7093023255813954,
          0.7131782945736435,
          0.7131782945736435,
          0.7170542635658915,
          0.7170542635658915,
          0.7209302325581395,
          0.7209302325581395,
          0.7248062015503876,
          0.7248062015503876,
          0.7286821705426356,
          0.7286821705426356,
          0.7325581395348837,
          0.7325581395348837,
          0.7364341085271318,
          0.7364341085271318,
          0.7403100775193798,
          0.7403100775193798,
          0.7441860465116279,
          0.7441860465116279,
          0.748062015503876,
          0.748062015503876,
          0.751937984496124,
          0.751937984496124,
          0.7558139534883721,
          0.7558139534883721,
          0.7596899224806202,
          0.7596899224806202,
          0.7674418604651163,
          0.7674418604651163,
          0.7713178294573644,
          0.7713178294573644,
          0.7751937984496124,
          0.7751937984496124,
          0.7790697674418605,
          0.7790697674418605,
          0.7829457364341085,
          0.7829457364341085,
          0.7868217054263565,
          0.7868217054263565,
          0.7906976744186046,
          0.7906976744186046,
          0.7945736434108527,
          0.7945736434108527,
          0.7984496124031008,
          0.7984496124031008,
          0.8023255813953488,
          0.8023255813953488,
          0.8062015503875969,
          0.8062015503875969,
          0.810077519379845,
          0.810077519379845,
          0.813953488372093,
          0.813953488372093,
          0.8178294573643411,
          0.8178294573643411,
          0.8217054263565892,
          0.8217054263565892,
          0.8255813953488372,
          0.8255813953488372,
          0.8294573643410853,
          0.8294573643410853,
          0.8333333333333334,
          0.8333333333333334,
          0.8372093023255814,
          0.8372093023255814,
          0.8410852713178295,
          0.8410852713178295,
          0.8449612403100775,
          0.8449612403100775,
          0.8488372093023255,
          0.8488372093023255,
          0.8527131782945736,
          0.8527131782945736,
          0.8565891472868217,
          0.8565891472868217,
          0.8604651162790697,
          0.8604651162790697,
          0.8643410852713178,
          0.8643410852713178,
          0.8682170542635659,
          0.8682170542635659,
          0.872093023255814,
          0.872093023255814,
          0.875968992248062,
          0.875968992248062,
          0.8798449612403101,
          0.8798449612403101,
          0.8837209302325582,
          0.8837209302325582,
          0.8875968992248062,
          0.8875968992248062,
          0.8914728682170543,
          0.8914728682170543,
          0.8953488372093024,
          0.8953488372093024,
          0.8992248062015504,
          0.8992248062015504,
          0.9031007751937985,
          0.9031007751937985,
          0.9069767441860465,
          0.9069767441860465,
          0.9108527131782945,
          0.9108527131782945,
          0.9147286821705426,
          0.9147286821705426,
          0.9186046511627907,
          0.9186046511627907,
          0.9224806201550387,
          0.9224806201550387,
          0.9263565891472868,
          0.9263565891472868,
          0.9302325581395349,
          0.9302325581395349,
          0.9341085271317829,
          0.9341085271317829,
          0.937984496124031,
          0.937984496124031,
          0.9418604651162791,
          0.9418604651162791,
          0.9457364341085271,
          0.9457364341085271,
          0.9496124031007752,
          0.9496124031007752,
          0.9534883720930233,
          0.9534883720930233,
          0.9573643410852714,
          0.9573643410852714,
          0.9612403100775194,
          0.9612403100775194,
          0.9651162790697675,
          0.9651162790697675,
          0.9689922480620154,
          0.9689922480620154,
          0.9728682170542635,
          0.9728682170542635,
          0.9767441860465116,
          0.9767441860465116,
          0.9806201550387597,
          0.9806201550387597,
          0.9844961240310077,
          0.9844961240310077,
          0.9883720930232558,
          0.9883720930232558,
          0.9922480620155039,
          0.9922480620155039,
          0.9961240310077519,
          0.9961240310077519,
          1,
          1
         ],
         "yaxis": "y3"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Train",
          "x": 0.14444444444444446,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Validation",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Test",
          "x": 0.8555555555555556,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "showarrow": false,
          "text": "AUC=0.9060",
          "x": 0.7,
          "xref": "x",
          "y": 0.5,
          "yref": "y"
         },
         {
          "showarrow": false,
          "text": "AUC=0.8930",
          "x": 0.7,
          "xref": "x2",
          "y": 0.5,
          "yref": "y2"
         },
         {
          "showarrow": false,
          "text": "AUC=0.8797",
          "x": 0.7,
          "xref": "x3",
          "y": 0.5,
          "yref": "y3"
         }
        ],
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "XGBoost Model"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.2888888888888889
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.35555555555555557,
          0.6444444444444445
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.7111111111111111,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"5a98682f-2e6c-4ace-8c85-58f7713748ca\" class=\"plotly-graph-div\" style=\"height:400px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5a98682f-2e6c-4ace-8c85-58f7713748ca\")) {                    Plotly.newPlot(                        \"5a98682f-2e6c-4ace-8c85-58f7713748ca\",                        [{\"fill\":\"tozeroy\",\"showlegend\":false,\"x\":[0.0,0.0,0.0,1.8792753514244906e-05,1.8792753514244906e-05,9.396376757122454e-05,9.396376757122454e-05,0.0002443057956851838,0.0002443057956851838,0.0002630985491994287,0.0002630985491994287,0.0003194768097421634,0.0003194768097421634,0.0003570623167706532,0.0003570623167706532,0.00037585507028489815,0.00037585507028489815,0.0004698188378561227,0.0004698188378561227,0.0004886115913703676,0.0004886115913703676,0.000601368112455837,0.000601368112455837,0.0008832594151695106,0.0008832594151695106,0.0009396376757122454,0.0009396376757122454,0.0009772231827407352,0.0009772231827407352,0.00099601593625498,0.00099601593625498,0.0011087724573404494,0.0011087724573404494,0.0011839434713974291,0.0011839434713974291,0.0012779072389686537,0.0012779072389686537,0.0012966999924828985,0.0012966999924828985,0.0013154927459971435,0.0013154927459971435,0.0013530782530256334,0.0013530782530256334,0.001409456513568368,0.001409456513568368,0.0015222130346538374,0.0015222130346538374,0.0015410057881680825,0.0015410057881680825,0.001804104337367511,0.001804104337367511,0.001973239118995715,0.001973239118995715,0.00199203187250996,0.00199203187250996,0.0020296173795384498,0.0020296173795384498,0.00206720288656694,0.00206720288656694,0.0021047883935954295,0.0021047883935954295,0.0021235811471096745,0.0021235811471096745,0.002161166654138164,0.002161166654138164,0.002217544914680899,0.002217544914680899,0.002255130421709389,0.002255130421709389,0.0023490941892806132,0.0023490941892806132,0.0023678869427948583,0.0023678869427948583,0.002424265203337593,0.002424265203337593,0.002443057956851838,0.002443057956851838,0.0024806434638803276,0.0024806434638803276,0.002593399984965797,0.002593399984965797,0.002612192738480042,0.002612192738480042,0.002630985491994287,0.002630985491994287,0.002649778245508532,0.002649778245508532,0.002969255055250695,0.002969255055250695,0.0031383898368788996,0.0031383898368788996,0.0031571825903931442,0.0031571825903931442,0.003232353604450124,0.003232353604450124,0.003251146357964369,0.003251146357964369,0.0032887318649928586,0.0032887318649928586,0.0033451101255355933,0.0033451101255355933,0.0034766594001353077,0.0034766594001353077,0.003702172442306247,0.003702172442306247,0.003777343456363226,0.003777343456363226,0.003814928963391716,0.003814928963391716,0.0040592347590768995,0.0040592347590768995,0.004115613019619635,0.004115613019619635,0.0044162970758475535,0.0044162970758475535,0.004472675336390288,0.004472675336390288,0.004491468089904533,0.004491468089904533,0.004529053596933023,0.004529053596933023,0.004623017364504247,0.004623017364504247,0.0046981883785612265,0.0046981883785612265,0.0047169811320754715,0.0047169811320754715,0.004848530406675186,0.004848530406675186,0.004867323160189431,0.004867323160189431,0.00494249417424641,0.00494249417424641,0.00505525069533188,0.00505525069533188,0.005074043448846125,0.005074043448846125,0.005205592723445839,0.005205592723445839,0.005224385476960084,0.005224385476960084,0.005261970983988574,0.005261970983988574,0.005280763737502819,0.005280763737502819,0.0053747275050740435,0.0053747275050740435,0.0053935202585882885,0.0053935202585882885,0.005431105765616778,0.005431105765616778,0.005468691272645268,0.005468691272645268,0.005506276779673758,0.005506276779673758,0.005525069533188003,0.005525069533188003,0.005637826054273472,0.005637826054273472,0.005656618807787717,0.005656618807787717,0.005675411561301962,0.005675411561301962,0.00608885213861535,0.00608885213861535,0.006201608659700819,0.006201608659700819,0.006389536194843269,0.006389536194843269,0.006408328948357514,0.006408328948357514,0.006445914455386003,0.006445914455386003,0.006896940539727881,0.006896940539727881,0.0069909043072991055,0.0069909043072991055,0.007047282567841841,0.007047282567841841,0.007235210102984289,0.007235210102984289,0.007254002856498534,0.007254002856498534,0.007291588363527024,0.007291588363527024,0.007592272419754942,0.007592272419754942,0.0076486506802976775,0.0076486506802976775,0.0076674434338119225,0.0076674434338119225,0.007686236187326167,0.007686236187326167,0.007723821694354657,0.007723821694354657,0.007780199954897392,0.007780199954897392,0.007949334736525596,0.007949334736525596,0.00800571299706833,0.00800571299706833,0.008062091257611065,0.008062091257611065,0.008212433285725025,0.008212433285725025,0.008250018792753513,0.008250018792753513,0.008400360820867474,0.008400360820867474,0.008701044877095393,0.008701044877095393,0.008795008644666617,0.008795008644666617,0.008813801398180861,0.008813801398180861,0.008907765165752085,0.008907765165752085,0.008926557919266331,0.008926557919266331,0.008945350672780576,0.008945350672780576,0.008964143426294821,0.008964143426294821,0.00907689994738029,0.00907689994738029,0.009095692700894536,0.009095692700894536,0.00911448545440878,0.00911448545440878,0.009133278207923024,0.009133278207923024,0.00918965646846576,0.00918965646846576,0.009264827482522739,0.009264827482522739,0.009283620236036984,0.009283620236036984,0.009302412989551229,0.009302412989551229,0.009527926031722167,0.009527926031722167,0.009659475306321882,0.009659475306321882,0.009697060813350372,0.009697060813350372,0.00988498834849282,0.00988498834849282,0.009903781102007066,0.009903781102007066,0.010054123130121025,0.010054123130121025,0.010091708637149515,0.010091708637149515,0.01011050139066376,0.01011050139066376,0.010204465158234985,0.010204465158234985,0.01022325791174923,0.01022325791174923,0.0103360144328347,0.0103360144328347,0.010354807186348944,0.010354807186348944,0.010411185446891678,0.010411185446891678,0.010467563707434414,0.010467563707434414,0.010561527475005638,0.010561527475005638,0.010711869503119597,0.010711869503119597,0.010768247763662331,0.010768247763662331,0.010843418777719311,0.010843418777719311,0.010918589791776291,0.010918589791776291,0.010937382545290536,0.010937382545290536,0.01103134631286176,0.01103134631286176,0.011050139066376006,0.011050139066376006,0.011087724573404496,0.011087724573404496,0.011369615876118169,0.011369615876118169,0.011745470946403066,0.011745470946403066,0.011801849206945802,0.011801849206945802,0.011895812974517027,0.011895812974517027,0.012046155002630985,0.012046155002630985,0.012064947756145231,0.012064947756145231,0.012102533263173721,0.012102533263173721,0.012121326016687965,0.012121326016687965,0.01214011877020221,0.01214011877020221,0.012234082537773434,0.012234082537773434,0.012271668044801924,0.012271668044801924,0.01229046079831617,0.01229046079831617,0.012365631812373148,0.012365631812373148,0.012722694129143803,0.012722694129143803,0.012760279636172291,0.012760279636172291,0.012929414417800496,0.012929414417800496,0.013004585431857476,0.013004585431857476,0.01306096369240021,0.01306096369240021,0.013079756445914456,0.013079756445914456,0.013154927459971434,0.013154927459971434,0.013549575283770578,0.013549575283770578,0.013568368037284824,0.013568368037284824,0.013699917311884538,0.013699917311884538,0.013793881079455762,0.013793881079455762,0.013831466586484252,0.013831466586484252,0.013981808614598211,0.013981808614598211,0.01420732165676915,0.01420732165676915,0.014658347741111027,0.014658347741111027,0.014827482522739232,0.014827482522739232,0.01505299556491017,0.01505299556491017,0.01539126512816658,0.01539126512816658,0.015410057881680823,0.015410057881680823,0.01542885063519507,0.01542885063519507,0.01546643614222356,0.01546643614222356,0.015541607156280538,0.015541607156280538,0.015635570923851764,0.015635570923851764,0.015767120198451477,0.015767120198451477,0.0158610839660227,0.0158610839660227,0.01593625498007968,0.01593625498007968,0.016142975268736374,0.016142975268736374,0.016368488310907314,0.016368488310907314,0.016500037585507027,0.016500037585507027,0.016518830339021275,0.016518830339021275,0.016763136134706456,0.016763136134706456,0.016969856423363152,0.016969856423363152,0.016988649176877396,0.016988649176877396,0.01719536946553409,0.01719536946553409,0.01723295497256258,0.01723295497256258,0.017439675261219274,0.017439675261219274,0.017777944824475683,0.017777944824475683,0.017947079606103887,0.017947079606103887,0.01800345786664662,0.01800345786664662,0.01822897090881756,0.01822897090881756,0.018435691197474253,0.018435691197474253,0.01864241148613095,0.01864241148613095,0.018661204239645193,0.018661204239645193,0.01871758250018793,0.01871758250018793,0.018830339021273398,0.018830339021273398,0.019281365105615275,0.019281365105615275,0.01930015785912952,0.01930015785912952,0.019563256408328948,0.019563256408328948,0.019751183943471397,0.019751183943471397,0.01976997669698564,0.01976997669698564,0.020277381041870254,0.020277381041870254,0.0202961737953845,0.0202961737953845,0.020765992633240624,0.020765992633240624,0.020878749154326092,0.020878749154326092,0.02132977523866797,0.02132977523866797,0.021630459294895887,0.021630459294895887,0.02232579117492295,0.02232579117492295,0.022494925956551153,0.022494925956551153,0.022664060738179358,0.022664060738179358,0.022720438998722094,0.022720438998722094,0.023152672329549727,0.023152672329549727,0.023227843343606704,0.023227843343606704,0.023453356385777644,0.023453356385777644,0.023566112906863113,0.023566112906863113,0.023716454934977073,0.023716454934977073,0.02382921145606254,0.02382921145606254,0.023848004209576786,0.023848004209576786,0.02390438247011952,0.02390438247011952,0.023998346237690746,0.023998346237690746,0.024148688265804706,0.024148688265804706,0.024336615800947155,0.024336615800947155,0.02439299406148989,0.02439299406148989,0.02443057956851838,0.02443057956851838,0.024468165075546867,0.024468165075546867,0.0249003984063745,0.0249003984063745,0.025257460723145154,0.025257460723145154,0.0252762534766594,0.0252762534766594,0.02572727956100128,0.02572727956100128,0.025764865068029767,0.025764865068029767,0.02585882883560099,0.02585882883560099,0.026103134631286176,0.026103134631286176,0.0261970983988574,0.0261970983988574,0.02627226941291438,0.02627226941291438,0.02634744042697136,0.02634744042697136,0.02726828534916936,0.02726828534916936,0.02745621288431181,0.02745621288431181,0.02753138389836879,0.02753138389836879,0.027550176651883033,0.027550176651883033,0.02773810418702548,0.02773810418702548,0.02807637375028189,0.02807637375028189,0.028095166503796135,0.028095166503796135,0.028771705630308952,0.028771705630308952,0.028846876644365933,0.028846876644365933,0.028940840411937157,0.028940840411937157,0.028978425918965645,0.028978425918965645,0.029166353454108097,0.029166353454108097,0.02918514620762234,0.02918514620762234,0.02926031722167932,0.02926031722167932,0.02952341577087875,0.02952341577087875,0.029579794031421483,0.029579794031421483,0.029730136059535443,0.029730136059535443,0.029918063594677892,0.029918063594677892,0.0302563331579343,0.0302563331579343,0.030632188228219198,0.030632188228219198,0.03070735924227618,0.03070735924227618,0.030970457791475607,0.030970457791475607,0.031045628805532587,0.031045628805532587,0.031158385326618056,0.031158385326618056,0.031214763587160792,0.031214763587160792,0.031327520108246264,0.031327520108246264,0.031459069382845976,0.031459069382845976,0.031646996917988425,0.031646996917988425,0.03221077952341577,0.03221077952341577,0.03241749981207247,0.03241749981207247,0.032605427347214916,0.032605427347214916,0.03292490415695708,0.03292490415695708,0.03384574907915508,0.03384574907915508,0.03424039690295422,0.03424039690295422,0.034447117191610914,0.034447117191610914,0.034898143275952795,0.034898143275952795,0.03499210704352402,0.03499210704352402,0.03502969255055251,0.03502969255055251,0.035236412839209204,0.035236412839209204,0.03529279109975193,0.03529279109975193,0.03544313312786589,0.03544313312786589,0.03549951138840863,0.03549951138840863,0.036044501240321734,0.036044501240321734,0.036063293993835975,0.036063293993835975,0.0361572577614072,0.0361572577614072,0.03673983312034879,0.03673983312034879,0.03709689543711945,0.03709689543711945,0.03741637224686161,0.03741637224686161,0.03758550702848981,0.03758550702848981,0.03764188528903255,0.03764188528903255,0.03856273021123055,0.03856273021123055,0.03869427948583026,0.03869427948583026,0.03876945049988725,0.03876945049988725,0.038807036006915735,0.038807036006915735,0.03886341426745847,0.03886341426745847,0.03905134180260092,0.03905134180260092,0.03925806209125761,0.03925806209125761,0.039314440351800345,0.039314440351800345,0.03933323310531459,0.03933323310531459,0.039464782379914305,0.039464782379914305,0.039803051943170714,0.039803051943170714,0.03985943020371345,0.03985943020371345,0.04017890701345561,0.04017890701345561,0.04044200556265504,0.04044200556265504,0.04087423889348267,0.04087423889348267,0.04102458092159663,0.04102458092159663,0.041099751935653614,0.041099751935653614,0.04115613019619635,0.04115613019619635,0.041250093963767574,0.041250093963767574,0.041513192512967,0.041513192512967,0.041719912801623696,0.041719912801623696,0.041907840336766145,0.041907840336766145,0.042377659174622266,0.042377659174622266,0.04254679395625047,0.04254679395625047,0.0426031722167932,0.0426031722167932,0.04267834323085019,0.04267834323085019,0.04269713598436443,0.04269713598436443,0.04314816206870631,0.04314816206870631,0.04361798090656243,0.04361798090656243,0.04391866496279035,0.04391866496279035,0.04397504322333308,0.04397504322333308,0.04414417800496129,0.04414417800496129,0.044219349019018264,0.044219349019018264,0.04423814177253251,0.04423814177253251,0.044332105540103736,0.044332105540103736,0.04440727655416071,0.04440727655416071,0.04446365481470345,0.04446365481470345,0.044482447568217696,0.044482447568217696,0.04457641133578892,0.04457641133578892,0.044820717131474105,0.044820717131474105,0.045158986694730514,0.045158986694730514,0.04606103886341427,0.04606103886341427,0.046324137412613695,0.046324137412613695,0.04651206494775614,0.04651206494775614,0.04669999248289859,0.04669999248289859,0.04701946929264076,0.04701946929264076,0.04722618958129745,0.04722618958129745,0.04752687363752537,0.04752687363752537,0.04782755769375329,0.04782755769375329,0.04801548522889574,0.04801548522889574,0.04814703450349545,0.04814703450349545,0.048240998271066676,0.048240998271066676,0.0485416823272946,0.0485416823272946,0.048616853341351574,0.048616853341351574,0.048936330151093735,0.048936330151093735,0.04897391565812223,0.04897391565812223,0.049030293918664966,0.049030293918664966,0.04912425768623619,0.04912425768623619,0.04951890551003533,0.04951890551003533,0.04961286927760655,0.04961286927760655,0.049800796812749,0.049800796812749,0.0498383823197775,0.0498383823197775,0.05075922724197549,0.05075922724197549,0.05186799969931594,0.05186799969931594,0.05246936781177178,0.05246936781177178,0.0526009170863715,0.0526009170863715,0.052826430128542434,0.052826430128542434,0.05284522288205668,0.05284522288205668,0.05288280838908517,0.05288280838908517,0.053465383748026764,0.053465383748026764,0.05401037359993986,0.05401037359993986,0.05442381417725325,0.05442381417725325,0.05466811997293843,0.05466811997293843,0.05472449823348117,0.05472449823348117,0.055419830113508234,0.055419830113508234,0.055889648951364355,0.055889648951364355,0.0568292866270766,0.0568292866270766,0.05712997068330452,0.05712997068330452,0.057223934450875744,0.057223934450875744,0.05728031271141848,0.05728031271141848,0.05746824024656093,0.05746824024656093,0.05782530256333158,0.05782530256333158,0.057994437344959786,0.057994437344959786,0.059497857626099375,0.059497857626099375,0.059836127189355784,0.059836127189355784,0.06066300834398256,0.06066300834398256,0.06092610689318199,0.06092610689318199,0.06122679094940991,0.06122679094940991,0.061940915582951214,0.061940915582951214,0.06231677065323611,0.06231677065323611,0.0625234909418928,0.0625234909418928,0.06286176050514922,0.06286176050514922,0.0628993460121777,0.0628993460121777,0.06383898368788996,0.06383898368788996,0.06417725325114636,0.06417725325114636,0.0641960460046606,0.0641960460046606,0.06509809817334436,0.06509809817334436,0.06633841990528452,0.06633841990528452,0.06637600541231302,0.06637600541231302,0.06648876193339848,0.06648876193339848,0.06654514019394121,0.06654514019394121,0.06725926482748253,0.06725926482748253,0.06752236337668195,0.06752236337668195,0.06759753439073893,0.06759753439073893,0.0677102909118244,0.0677102909118244,0.06812373148913779,0.06812373148913779,0.06831165902428024,0.06831165902428024,0.06883785612267909,0.06883785612267909,0.0699278358265053,0.0699278358265053,0.07066075321356086,0.07066075321356086,0.07135608509358791,0.07135608509358791,0.07145004886115913,0.07145004886115913,0.07195745320604376,0.07195745320604376,0.07203262422010073,0.07203262422010073,0.07242727204389987,0.07242727204389987,0.07259640682552808,0.07259640682552808,0.07353604450124032,0.07353604450124032,0.0739870705855822,0.0739870705855822,0.07404344884612493,0.07404344884612493,0.07409982710666767,0.07409982710666767,0.0743629256558671,0.0743629256558671,0.07441930391640983,0.07441930391640983,0.0745320604374953,0.0745320604374953,0.07473878072615199,0.07473878072615199,0.07515222130346538,0.07515222130346538,0.07547169811320754,0.07547169811320754,0.0757911749229497,0.0757911749229497,0.07594151695106367,0.07594151695106367,0.07633616477486281,0.07633616477486281,0.07650529955649102,0.07650529955649102,0.07693753288731865,0.07693753288731865,0.07720063143651808,0.07720063143651808,0.07742614447868902,0.07742614447868902,0.07744493723220326,0.07744493723220326,0.07748252273923176,0.07748252273923176,0.0782154401262873,0.0782154401262873,0.07891077200631437,0.07891077200631437,0.07932421258362775,0.07932421258362775,0.07962489663985567,0.07962489663985567,0.07986920243554085,0.07986920243554085,0.0800947154777118,0.0800947154777118,0.08015109373825453,0.08015109373825453,0.08041419228745396,0.08041419228745396,0.08048936330151094,0.08048936330151094,0.08095918213936706,0.08095918213936706,0.08120348793505225,0.08120348793505225,0.08169209952642262,0.08169209952642262,0.08184244155453657,0.08184244155453657,0.08191761256859355,0.08191761256859355,0.08343982560324739,0.08343982560324739,0.08362775313838984,0.08362775313838984,0.08419153574381719,0.08419153574381719,0.08460497632113058,0.08460497632113058,0.08464256182815906,0.08464256182815906,0.08516875892655792,0.08516875892655792,0.08526272269412914,0.08526272269412914,0.08531910095467188,0.08531910095467188,0.08545065022927159,0.08545065022927159,0.08776215891152371,0.08776215891152371,0.08796887920018041,0.08796887920018041,0.08832594151695107,0.08832594151695107,0.08836352702397955,0.08836352702397955,0.08873938209426445,0.08873938209426445,0.08885213861534992,0.08885213861534992,0.0895098849883485,0.0895098849883485,0.09001728933323311,0.09001728933323311,0.09050590092460348,0.09050590092460348,0.09052469367811772,0.09052469367811772,0.0905998646921747,0.0905998646921747,0.09178380816357212,0.09178380816357212,0.09217845598737127,0.09217845598737127,0.09234759076899947,0.09234759076899947,0.09255431105765617,0.09255431105765617,0.09371946177553936,0.09371946177553936,0.09460272119070887,0.09460272119070887,0.09464030669773735,0.09464030669773735,0.09471547771179434,0.09471547771179434,0.09505374727505074,0.09505374727505074,0.09589942118319177,0.09589942118319177,0.09631286176050514,0.09631286176050514,0.09668871683079004,0.09672630233781854,0.0970833646545892,0.0970833646545892,0.09730887769676012,0.09730887769676012,0.09753439073893107,0.09753439073893107,0.09779748928813049,0.09779748928813049,0.09824851537247238,0.09824851537247238,0.10168758926557919,0.10168758926557919,0.10217620085694956,0.10217620085694956,0.1021949936104638,0.1021949936104638,0.10230775013154927,0.10230775013154927,0.1040742689618883,0.1040742689618883,0.10463805156731565,0.10463805156731565,0.10465684432082989,0.10465684432082989,0.10491994287002931,0.10491994287002931,0.10531459069382847,0.10531459069382847,0.10540855446139968,0.10540855446139968,0.10572803127114185,0.10572803127114185,0.1059159588062843,0.1059159588062843,0.1077952341577088,0.1077952341577088,0.10788919792528001,0.10788919792528001,0.10903555588964896,0.10903555588964896,0.1092234834247914,0.1092234834247914,0.10956175298804781,0.10956175298804781,0.11018191385401789,0.11018191385401789,0.11025708486807487,0.11025708486807487,0.11375253702172443,0.11375253702172443,0.11388408629632414,0.11388408629632414,0.11472976020446515,0.11472976020446515,0.11501165150717883,0.11501165150717883,0.1155002630985492,0.1155002630985492,0.11649627903480418,0.11649627903480418,0.1175110877245734,0.1175110877245734,0.11822521235811471,0.11822521235811471,0.11952191235059761,0.11952191235059761,0.119935352927911,0.119935352927911,0.12002931669548222,0.12002931669548222,0.1205179282868526,0.1205179282868526,0.12055551379388108,0.12055551379388108,0.12125084567390815,0.12125084567390815,0.12213410508907765,0.12213410508907765,0.12286702247613321,0.12286702247613321,0.1234120123280463,0.1234120123280463,0.12534766594001354,0.12534766594001354,0.126456438397354,0.126456438397354,0.12653160941141095,0.12653160941141095,0.1270953920168383,0.1270953920168383,0.1276779673757799,0.1276779673757799,0.1278471021574081,0.1278471021574081,0.12833571374877847,0.12833571374877847,0.12882432534014884,0.12882432534014884,0.13008343982560325,0.13008343982560325,0.13277080357814028,0.13277080357814028,0.1335413064722243,0.1335413064722243,0.13442456588739382,0.13442456588739382,0.13481921371119296,0.13481921371119296,0.13718710065398781,0.13718710065398781,0.1375629557242727,0.1375629557242727,0.14028790498383822,0.14028790498383822,0.14137788468766443,0.14137788468766443,0.1424490716379764,0.1424490716379764,0.143952491919116,0.143952491919116,0.14444110351048636,0.14444110351048636,0.1452867774186274,0.1452867774186274,0.14590693828459747,0.14590693828459747,0.14607607306622566,0.14607607306622566,0.1470532962489664,0.1470532962489664,0.1472224310305946,0.1472224310305946,0.14724122378410884,0.14724122378410884,0.14750432233330826,0.14750432233330826,0.14757949334736525,0.14757949334736525,0.14902653536796212,0.14902653536796212,0.14919567014959031,0.14919567014959031,0.15017289333233105,0.15017289333233105,0.15075546869127265,0.15075546869127265,0.15124408028264302,0.1512816657896715,0.15193941216267007,0.15193941216267007,0.15201458317672706,0.15201458317672706,0.15214613245132677,0.15214613245132677,0.1524844020145832,0.1524844020145832,0.153160941141096,0.153160941141096,0.15428850635195068,0.15428850635195068,0.1544200556265504,0.1544200556265504,0.15579192663309027,0.15579192663309027,0.15656242952717433,0.15656242952717433,0.1568067353228595,0.1568067353228595,0.1579343005337142,0.1579343005337142,0.15799067879425693,0.15799067879425693,0.16003908892730964,0.16003908892730964,0.16017063820190935,0.16017063820190935,0.16037735849056603,0.16037735849056603,0.16141095993384952,0.16141095993384952,0.16468089904532812,0.16468089904532812,0.16590242802375404,0.16590242802375404,0.16851462076223409,0.16851462076223409,0.16900323235360445,0.16900323235360445,0.17020596857851614,0.17020596857851614,0.17076975118394347,0.17076975118394347,0.1714462903104563,0.1714462903104563,0.17184093813425544,0.17184093813425544,0.17223558595805458,0.17223558595805458,0.17289333233105314,0.17289333233105314,0.1760129294144178,0.1760129294144178,0.1763887844847027,0.1763887844847027,0.17685860332255882,0.17685860332255882,0.1769713598436443,0.1769713598436443,0.17712170187175824,0.17712170187175824,0.17740359317447194,0.17740359317447194,0.17809892505449898,0.17809892505449898,0.18010974968052318,0.18010974968052318,0.1806547395324363,0.1806547395324363,0.18396226415094338,0.18396226415094338,0.1843757047282568,0.1843757047282568,0.18582274674885363,0.18582274674885363,0.1872697887694505,0.1872697887694505,0.1874013380440502,0.1874013380440502,0.18792753514244906,0.18792753514244906,0.19151695106366984,0.19151695106366984,0.1917048785988123,0.1917048785988123,0.19226866120423963,0.19226866120423963,0.19369691047132226,0.19369691047132226,0.19510636698489062,0.19510636698489062,0.19659099451251597,0.19659099451251597,0.19700443508982937,0.19700443508982937,0.19867699015259715,0.19867699015259715,0.2016274524543336,0.2016274524543336,0.2050101480868977,0.2050101480868977,0.20692700894535068,0.20692700894535068,0.20775389009997744,0.20775389009997744,0.20910696835300308,0.20910696835300308,0.20952040893031648,0.20952040893031648,0.21074193790874238,0.21074193790874238,0.21156881906336916,0.21156881906336916,0.21235811471096744,0.21235811471096744,0.21260242050665262,0.21260242050665262,0.21545891904081785,0.21545891904081785,0.2188604074268962,0.2188604074268962,0.21897316394798166,0.21897316394798166,0.2199128016236939,0.2199128016236939,0.22000676539126512,0.22000676539126512,0.22026986394046455,0.22026986394046455,0.22104036683454859,0.22104036683454859,0.22316394798165828,0.22316394798165828,0.22489288130496882,0.22489288130496882,0.22579493347365256,0.22579493347365256,0.2271480117266782,0.2271480117266782,0.22867022476133203,0.22867022476133203,0.23009847402841466,0.23009847402841466,0.23043674359167104,0.23043674359167104,0.23289859430203713,0.23289859430203713,0.23466511313237615,0.23466511313237615,0.23848004209576787,0.23848004209576787,0.23936330151093738,0.23936330151093738,0.2403781102007066,0.2403781102007066,0.240716379763963,0.240716379763963,0.24299030293918664,0.24299030293918664,0.2430466811997294,0.2430466811997294,0.24383597684732766,0.24383597684732766,0.245320604374953,0.245320604374953,0.24575283770578066,0.24575283770578066,0.24725625798692025,0.24725625798692025,0.2489476058032023,0.2489476058032023,0.2493234608734872,0.2493234608734872,0.24971810869728633,0.24971810869728633,0.2523490941892806,0.2523490941892806,0.2527249492595655,0.2527249492595655,0.25420957678719086,0.25420957678719086,0.2565398782229572,0.2565398782229572,0.2571976245959558,0.2571976245959558,0.25905810719386607,0.25905810719386607,0.2590768999473803,0.2590768999473803,0.26048635646094864,0.26048635646094864,0.26230925355183043,0.26230925355183043,0.2633052694880854,0.2633052694880854,0.26749605352176203,0.26749605352176203,0.26854844771855974,0.26854844771855974,0.26911223032398707,0.26911223032398707,0.2762910621664286,0.2762910621664286,0.27670450274374203,0.27670450274374203,0.27672329549725627,0.27672329549725627,0.2799932346087349,0.2799932346087349,0.2821731940163873,0.2821731940163873,0.28781102007066073,0.28781102007066073,0.2913440577313388,0.2913440577313388,0.2922649026535368,0.2922649026535368,0.29245283018867924,0.29245283018867924,0.2925467939562505,0.2925467939562505,0.29322333308276327,0.29322333308276327,0.2945200330752462,0.2945200330752462,0.29487709539201684,0.29487709539201684,0.2972073968277832,0.2972073968277832,0.2982409982710667,0.2982409982710667,0.2993873562354356,0.2993873562354356,0.2995376982635496,0.2995376982635496,0.30495001127565213,0.30495001127565213,0.30620912576110654,0.30620912576110654,0.3071863489438473,0.3071863489438473,0.30752461850710366,0.30752461850710366,0.3114335112380666,0.3114335112380666,0.3114710967450951,0.3114710967450951,0.3123543561602646,0.3123543561602646,0.31637600541231303,0.31637600541231303,0.31645117642637,0.31645117642637,0.3207359242276178,0.3207359242276178,0.3223521010298429,0.3223521010298429,0.33793129369315195,0.33793129369315195,0.34170863714951516,0.34170863714951516,0.3442832443809667,0.3442832443809667,0.3447530632188228,0.3447530632188228,0.3475531834924453,0.3475531834924453,0.3728858152296474,0.3728858152296474,0.3738442456588739,0.3738442456588739,0.3748402615951289,0.3748402615951289,0.37688867172818163,0.37688867172818163,0.37728331955198074,0.37728331955198074,0.37904983838231976,0.37904983838231976,0.38205667894459894,0.38205667894459894,0.38478162820416445,0.38478162820416445,0.3899120499135533,0.3899120499135533,0.3940840411937157,0.3940840411937157,0.3959445237916259,0.3959445237916259,0.3979365556641359,0.3979365556641359,0.39979703826204616,0.39979703826204616,0.4065060512666316,0.4065060512666316,0.4073705179282869,0.4073705179282869,0.41297075847553183,0.41297075847553183,0.4212019845147711,0.4212019845147711,0.42285574682402466,0.42285574682402466,0.42742238592798615,0.42742238592798615,0.43065473953243627,0.43065473953243627,0.43181989025031947,0.43181989025031947,0.43719461775539353,0.43719461775539353,0.4456325640832895,0.4456325640832895,0.46985642336315114,0.46985642336315114,0.4875591971735699,0.4875591971735699,0.4891753739757949,0.4891753739757949,0.510204465158235,0.510204465158235,0.5134180260091709,0.5134180260091709,0.5322483650304443,0.5322483650304443,0.5534277982409983,0.5534277982409983,0.5617905735548373,0.5617905735548373,0.5837217169059611,0.5837217169059611,0.6113658573254154,0.6113658573254154,0.686236187326167,0.686236187326167,0.8081447793730737,0.8081447793730737,1.0],\"y\":[0.0,0.00129366106080207,0.00258732212160414,0.00258732212160414,0.0038809831824062097,0.0038809831824062097,0.00646830530401035,0.00646830530401035,0.007761966364812419,0.007761966364812419,0.009055627425614488,0.009055627425614488,0.01034928848641656,0.01034928848641656,0.0129366106080207,0.0129366106080207,0.015523932729624839,0.015523932729624839,0.016817593790426907,0.016817593790426907,0.019404915912031046,0.019404915912031046,0.02199223803363519,0.02199223803363519,0.02328589909443726,0.02328589909443726,0.02457956015523933,0.02457956015523933,0.027166882276843468,0.027166882276843468,0.028460543337645538,0.028460543337645538,0.029754204398447608,0.029754204398447608,0.031047865459249677,0.031047865459249677,0.03363518758085381,0.03363518758085381,0.03492884864165589,0.03492884864165589,0.03622250970245795,0.03622250970245795,0.037516170763260026,0.037516170763260026,0.03880983182406209,0.03880983182406209,0.040103492884864166,0.040103492884864166,0.042690815006468305,0.042690815006468305,0.04398447606727038,0.04398447606727038,0.045278137128072445,0.045278137128072445,0.04657179818887452,0.04657179818887452,0.047865459249676584,0.047865459249676584,0.04915912031047866,0.04915912031047866,0.050452781371280724,0.050452781371280724,0.05304010349288486,0.05304010349288486,0.054333764553686936,0.054333764553686936,0.055627425614489,0.055627425614489,0.056921086675291076,0.056921086675291076,0.05821474773609314,0.05821474773609314,0.059508408796895215,0.059508408796895215,0.062095730918499355,0.062095730918499355,0.06338939197930142,0.06338939197930142,0.0646830530401035,0.0646830530401035,0.06597671410090557,0.06597671410090557,0.0685640362225097,0.0685640362225097,0.06985769728331177,0.06985769728331177,0.07115135834411385,0.07115135834411385,0.0724450194049159,0.0724450194049159,0.07632600258732213,0.07632600258732213,0.07761966364812418,0.07761966364812418,0.07891332470892626,0.07891332470892626,0.08020698576972833,0.08020698576972833,0.0815006468305304,0.0815006468305304,0.08279430789133248,0.08279430789133248,0.08408796895213454,0.08408796895213454,0.08667529107373868,0.08667529107373868,0.08796895213454076,0.08796895213454076,0.08926261319534282,0.08926261319534282,0.09055627425614489,0.09055627425614489,0.09184993531694696,0.09184993531694696,0.09314359637774904,0.09314359637774904,0.0944372574385511,0.0944372574385511,0.09702457956015524,0.09702457956015524,0.09961190168175937,0.09961190168175937,0.10090556274256145,0.10090556274256145,0.10219922380336352,0.10219922380336352,0.1034928848641656,0.1034928848641656,0.10478654592496765,0.10478654592496765,0.10608020698576973,0.10608020698576973,0.1073738680465718,0.1073738680465718,0.10866752910737387,0.10866752910737387,0.10996119016817593,0.10996119016817593,0.111254851228978,0.111254851228978,0.11254851228978008,0.11254851228978008,0.11384217335058215,0.11384217335058215,0.11513583441138421,0.11513583441138421,0.11642949547218628,0.11642949547218628,0.11901681759379043,0.11901681759379043,0.1203104786545925,0.1203104786545925,0.12160413971539456,0.12160413971539456,0.12289780077619664,0.12289780077619664,0.12419146183699871,0.12419146183699871,0.12548512289780078,0.12548512289780078,0.12677878395860284,0.12677878395860284,0.12807244501940493,0.12807244501940493,0.129366106080207,0.129366106080207,0.13065976714100905,0.13065976714100905,0.13195342820181113,0.13195342820181113,0.1332470892626132,0.1332470892626132,0.13454075032341525,0.13454075032341525,0.13583441138421734,0.13583441138421734,0.13971539456662355,0.13971539456662355,0.1410090556274256,0.1410090556274256,0.1423027166882277,0.1423027166882277,0.14359637774902975,0.14359637774902975,0.1448900388098318,0.1448900388098318,0.1461836998706339,0.1461836998706339,0.14747736093143596,0.14747736093143596,0.14877102199223805,0.14877102199223805,0.15135834411384216,0.15135834411384216,0.15653298835705046,0.15653298835705046,0.15782664941785252,0.15782664941785252,0.1591203104786546,0.1591203104786546,0.16041397153945666,0.16041397153945666,0.1630012936610608,0.1630012936610608,0.16429495472186287,0.16429495472186287,0.16688227684346701,0.16688227684346701,0.16817593790426907,0.16817593790426907,0.16946959896507116,0.16946959896507116,0.17076326002587322,0.17076326002587322,0.17205692108667528,0.17205692108667528,0.17464424320827943,0.17464424320827943,0.1759379042690815,0.1759379042690815,0.17723156532988357,0.17723156532988357,0.17852522639068563,0.17852522639068563,0.17981888745148772,0.17981888745148772,0.18111254851228978,0.18111254851228978,0.18240620957309184,0.18240620957309184,0.18369987063389392,0.18369987063389392,0.18499353169469598,0.18499353169469598,0.18628719275549807,0.18628719275549807,0.18758085381630013,0.18758085381630013,0.19016817593790428,0.19016817593790428,0.19146183699870634,0.19146183699870634,0.1927554980595084,0.1927554980595084,0.19534282018111254,0.19534282018111254,0.19663648124191463,0.19663648124191463,0.1979301423027167,0.1979301423027167,0.19922380336351875,0.19922380336351875,0.20051746442432083,0.20051746442432083,0.2018111254851229,0.2018111254851229,0.20439844760672704,0.20439844760672704,0.2056921086675291,0.2056921086675291,0.2069857697283312,0.2069857697283312,0.20827943078913325,0.20827943078913325,0.2095730918499353,0.2095730918499353,0.2108667529107374,0.2108667529107374,0.21216041397153945,0.21216041397153945,0.21345407503234154,0.21345407503234154,0.2147477360931436,0.2147477360931436,0.21604139715394566,0.21604139715394566,0.21733505821474774,0.21733505821474774,0.222509702457956,0.222509702457956,0.2238033635187581,0.2238033635187581,0.22509702457956016,0.22509702457956016,0.22639068564036222,0.22639068564036222,0.22897800776196636,0.22897800776196636,0.23027166882276842,0.23027166882276842,0.2315653298835705,0.2315653298835705,0.23415265200517466,0.23415265200517466,0.23544631306597671,0.23544631306597671,0.23673997412677877,0.23673997412677877,0.23803363518758086,0.23803363518758086,0.240620957309185,0.240620957309185,0.2445019404915912,0.2445019404915912,0.24579560155239327,0.24579560155239327,0.24708926261319533,0.24708926261319533,0.24838292367399742,0.24838292367399742,0.25097024579560157,0.25097024579560157,0.2522639068564036,0.2522639068564036,0.2535575679172057,0.2535575679172057,0.25485122897800777,0.25485122897800777,0.25614489003880986,0.25614489003880986,0.2574385510996119,0.2574385510996119,0.258732212160414,0.258732212160414,0.26002587322121606,0.26002587322121606,0.2613195342820181,0.2613195342820181,0.2626131953428202,0.2626131953428202,0.26390685640362227,0.26390685640362227,0.2652005174644243,0.2652005174644243,0.2664941785252264,0.2664941785252264,0.2677878395860285,0.2677878395860285,0.2690815006468305,0.2690815006468305,0.2703751617076326,0.2703751617076326,0.2716688227684347,0.2716688227684347,0.2729624838292367,0.2729624838292367,0.2742561448900388,0.2742561448900388,0.2755498059508409,0.2755498059508409,0.278137128072445,0.278137128072445,0.2794307891332471,0.2794307891332471,0.2807244501940492,0.2807244501940492,0.2820181112548512,0.2820181112548512,0.2833117723156533,0.2833117723156533,0.2846054333764554,0.2846054333764554,0.2858990944372574,0.2858990944372574,0.2871927554980595,0.2871927554980595,0.2884864165588616,0.2884864165588616,0.2923673997412678,0.2923673997412678,0.2936610608020699,0.2936610608020699,0.2949547218628719,0.2949547218628719,0.296248382923674,0.296248382923674,0.2975420439844761,0.2975420439844761,0.2988357050452781,0.2988357050452781,0.3001293661060802,0.3001293661060802,0.3014230271668823,0.3014230271668823,0.3027166882276843,0.3027166882276843,0.3040103492884864,0.3040103492884864,0.3053040103492885,0.3053040103492885,0.30659767141009053,0.30659767141009053,0.3078913324708926,0.3078913324708926,0.3091849935316947,0.3091849935316947,0.31047865459249674,0.31047865459249674,0.3117723156532988,0.3117723156532988,0.3130659767141009,0.3130659767141009,0.314359637774903,0.314359637774903,0.31565329883570503,0.31565329883570503,0.3169469598965071,0.3169469598965071,0.3182406209573092,0.3182406209573092,0.31953428201811124,0.31953428201811124,0.3208279430789133,0.3208279430789133,0.3221216041397154,0.3221216041397154,0.32341526520051744,0.32341526520051744,0.32470892626131953,0.32470892626131953,0.3260025873221216,0.3260025873221216,0.32729624838292365,0.32729624838292365,0.32858990944372574,0.32858990944372574,0.3298835705045278,0.3298835705045278,0.3311772315653299,0.3311772315653299,0.33247089262613194,0.33247089262613194,0.33376455368693403,0.33376455368693403,0.33635187580853815,0.33635187580853815,0.33764553686934023,0.33764553686934023,0.3389391979301423,0.3389391979301423,0.34023285899094435,0.34023285899094435,0.34152652005174644,0.34152652005174644,0.3428201811125485,0.3428201811125485,0.34411384217335056,0.34411384217335056,0.34540750323415265,0.34540750323415265,0.34670116429495473,0.34670116429495473,0.34799482535575677,0.34799482535575677,0.34928848641655885,0.34928848641655885,0.351875808538163,0.351875808538163,0.35316946959896506,0.35316946959896506,0.35446313065976714,0.35446313065976714,0.35575679172056923,0.35575679172056923,0.35705045278137126,0.35705045278137126,0.35834411384217335,0.35834411384217335,0.35963777490297544,0.35963777490297544,0.36093143596377747,0.36093143596377747,0.3648124191461837,0.3648124191461837,0.36610608020698576,0.36610608020698576,0.36739974126778785,0.36739974126778785,0.36869340232858994,0.36869340232858994,0.37128072445019406,0.37128072445019406,0.3738680465717982,0.3738680465717982,0.37516170763260026,0.37516170763260026,0.37645536869340235,0.37645536869340235,0.3777490297542044,0.3777490297542044,0.37904269081500647,0.37904269081500647,0.3816300129366106,0.3816300129366106,0.3829236739974127,0.3829236739974127,0.38421733505821476,0.38421733505821476,0.3855109961190168,0.3855109961190168,0.3868046571798189,0.3868046571798189,0.38809831824062097,0.38809831824062097,0.38939197930142305,0.38939197930142305,0.3906856403622251,0.3906856403622251,0.39197930142302717,0.39197930142302717,0.39327296248382926,0.39327296248382926,0.3945666235446313,0.3945666235446313,0.3958602846054334,0.3958602846054334,0.3984476067270375,0.3984476067270375,0.3997412677878396,0.3997412677878396,0.40103492884864167,0.40103492884864167,0.4023285899094437,0.4023285899094437,0.4036222509702458,0.4036222509702458,0.4049159120310479,0.4049159120310479,0.40620957309184996,0.40620957309184996,0.407503234152652,0.407503234152652,0.4087968952134541,0.4087968952134541,0.4113842173350582,0.4113842173350582,0.4126778783958603,0.4126778783958603,0.4139715394566624,0.4139715394566624,0.4152652005174644,0.4152652005174644,0.4165588615782665,0.4165588615782665,0.4191461836998706,0.4191461836998706,0.4217335058214748,0.4217335058214748,0.4230271668822768,0.4230271668822768,0.4243208279430789,0.4243208279430789,0.425614489003881,0.425614489003881,0.4269081500646831,0.4269081500646831,0.4282018111254851,0.4282018111254851,0.4294954721862872,0.4294954721862872,0.4307891332470893,0.4307891332470893,0.4320827943078913,0.4320827943078913,0.4333764553686934,0.4333764553686934,0.4346701164294955,0.4346701164294955,0.4359637774902975,0.4359637774902975,0.4372574385510996,0.4372574385510996,0.4385510996119017,0.4385510996119017,0.4398447606727037,0.4398447606727037,0.4424320827943079,0.4424320827943079,0.44372574385511,0.44372574385511,0.445019404915912,0.445019404915912,0.4489003880983182,0.4489003880983182,0.4501940491591203,0.4501940491591203,0.4514877102199224,0.4514877102199224,0.45278137128072443,0.45278137128072443,0.4540750323415265,0.4540750323415265,0.4553686934023286,0.4553686934023286,0.45666235446313064,0.45666235446313064,0.4579560155239327,0.4579560155239327,0.4592496765847348,0.4592496765847348,0.46054333764553684,0.46054333764553684,0.46183699870633893,0.46183699870633893,0.463130659767141,0.463130659767141,0.4644243208279431,0.4644243208279431,0.46571798188874514,0.46571798188874514,0.4670116429495472,0.4670116429495472,0.4683053040103493,0.4683053040103493,0.46959896507115134,0.46959896507115134,0.47089262613195343,0.47089262613195343,0.4721862871927555,0.4721862871927555,0.47347994825355755,0.47347994825355755,0.47477360931435963,0.47477360931435963,0.4760672703751617,0.4760672703751617,0.47736093143596375,0.47736093143596375,0.47865459249676584,0.47865459249676584,0.4799482535575679,0.4799482535575679,0.48124191461837,0.48124191461837,0.48253557567917205,0.48253557567917205,0.48382923673997413,0.48382923673997413,0.4851228978007762,0.4851228978007762,0.48641655886157825,0.48641655886157825,0.48771021992238034,0.48771021992238034,0.4890038809831824,0.4890038809831824,0.49029754204398446,0.49029754204398446,0.49159120310478654,0.49159120310478654,0.49288486416558863,0.49288486416558863,0.49547218628719275,0.49547218628719275,0.49676584734799484,0.49676584734799484,0.49805950840879687,0.49805950840879687,0.49935316946959896,0.49935316946959896,0.500646830530401,0.500646830530401,0.5019404915912031,0.5019404915912031,0.5032341526520052,0.5032341526520052,0.5045278137128072,0.5045278137128072,0.5058214747736093,0.5058214747736093,0.5071151358344114,0.5071151358344114,0.5084087968952135,0.5084087968952135,0.5097024579560155,0.5097024579560155,0.5109961190168176,0.5109961190168176,0.5122897800776197,0.5122897800776197,0.5135834411384217,0.5135834411384217,0.5148771021992238,0.5148771021992238,0.5161707632600259,0.5161707632600259,0.517464424320828,0.517464424320828,0.51875808538163,0.51875808538163,0.5200517464424321,0.5200517464424321,0.5213454075032341,0.5213454075032341,0.5226390685640362,0.5226390685640362,0.5239327296248383,0.5239327296248383,0.5252263906856404,0.5252263906856404,0.5265200517464425,0.5265200517464425,0.5278137128072445,0.5278137128072445,0.5291073738680466,0.5291073738680466,0.5304010349288486,0.5304010349288486,0.5316946959896507,0.5316946959896507,0.5329883570504528,0.5329883570504528,0.5342820181112549,0.5342820181112549,0.535575679172057,0.535575679172057,0.536869340232859,0.536869340232859,0.538163001293661,0.538163001293661,0.5394566623544631,0.5394566623544631,0.5407503234152652,0.5407503234152652,0.5420439844760673,0.5420439844760673,0.5433376455368694,0.5433376455368694,0.5446313065976714,0.5446313065976714,0.5459249676584734,0.5459249676584734,0.5472186287192755,0.5472186287192755,0.5485122897800776,0.5485122897800776,0.5498059508408797,0.5498059508408797,0.5510996119016818,0.5510996119016818,0.5523932729624839,0.5523932729624839,0.553686934023286,0.553686934023286,0.5549805950840879,0.5549805950840879,0.55627425614489,0.55627425614489,0.5575679172056921,0.5575679172056921,0.5588615782664942,0.5588615782664942,0.5601552393272963,0.5601552393272963,0.5614489003880984,0.5614489003880984,0.5627425614489003,0.5627425614489003,0.5640362225097024,0.5640362225097024,0.5653298835705045,0.5653298835705045,0.5666235446313066,0.5666235446313066,0.5679172056921087,0.5679172056921087,0.5692108667529108,0.5692108667529108,0.5705045278137129,0.5705045278137129,0.5717981888745148,0.5717981888745148,0.5730918499353169,0.5730918499353169,0.574385510996119,0.574385510996119,0.5756791720569211,0.5756791720569211,0.5769728331177232,0.5769728331177232,0.5782664941785253,0.5782664941785253,0.5795601552393272,0.5795601552393272,0.5808538163001293,0.5808538163001293,0.5821474773609314,0.5821474773609314,0.5834411384217335,0.5834411384217335,0.5847347994825356,0.5847347994825356,0.5860284605433377,0.5860284605433377,0.5873221216041398,0.5873221216041398,0.5886157826649417,0.5886157826649417,0.5912031047865459,0.5912031047865459,0.5937904269081501,0.5937904269081501,0.5950840879689522,0.5950840879689522,0.5963777490297542,0.5963777490297542,0.5976714100905562,0.5976714100905562,0.5989650711513583,0.5989650711513583,0.6002587322121604,0.6002587322121604,0.6015523932729625,0.6015523932729625,0.6028460543337646,0.6028460543337646,0.6041397153945667,0.6041397153945667,0.6054333764553687,0.6054333764553687,0.6067270375161707,0.6067270375161707,0.6080206985769728,0.6080206985769728,0.6093143596377749,0.6093143596377749,0.610608020698577,0.610608020698577,0.6119016817593791,0.6119016817593791,0.6131953428201811,0.6131953428201811,0.6144890038809832,0.6144890038809832,0.6157826649417852,0.6157826649417852,0.6170763260025873,0.6170763260025873,0.6183699870633894,0.6183699870633894,0.6196636481241915,0.6196636481241915,0.6209573091849935,0.6209573091849935,0.6222509702457956,0.6222509702457956,0.6235446313065977,0.6235446313065977,0.6248382923673997,0.6248382923673997,0.6261319534282018,0.6261319534282018,0.6274256144890039,0.6274256144890039,0.628719275549806,0.628719275549806,0.630012936610608,0.630012936610608,0.6313065976714101,0.6313065976714101,0.6326002587322122,0.6326002587322122,0.6338939197930142,0.6338939197930142,0.6351875808538163,0.6351875808538163,0.6364812419146184,0.6364812419146184,0.6377749029754204,0.6377749029754204,0.6403622250970246,0.6403622250970246,0.6416558861578266,0.6416558861578266,0.6429495472186287,0.6429495472186287,0.6442432082794308,0.6442432082794308,0.6455368693402329,0.6455368693402329,0.6468305304010349,0.6468305304010349,0.648124191461837,0.648124191461837,0.6494178525226391,0.6494178525226391,0.6507115135834411,0.6507115135834411,0.6520051746442432,0.6520051746442432,0.6532988357050453,0.6532988357050453,0.6545924967658473,0.6545924967658473,0.6558861578266494,0.6558861578266494,0.6571798188874515,0.6571798188874515,0.6584734799482536,0.6584734799482536,0.6597671410090556,0.6597671410090556,0.6610608020698577,0.6610608020698577,0.6623544631306598,0.6623544631306598,0.6636481241914618,0.6636481241914618,0.6649417852522639,0.6649417852522639,0.666235446313066,0.666235446313066,0.6675291073738681,0.6675291073738681,0.6688227684346701,0.6688227684346701,0.6701164294954722,0.6701164294954722,0.6714100905562742,0.6714100905562742,0.6727037516170763,0.6727037516170763,0.6739974126778784,0.6739974126778784,0.6752910737386805,0.6752910737386805,0.6765847347994826,0.6765847347994826,0.6778783958602846,0.6778783958602846,0.6791720569210866,0.6791720569210866,0.6804657179818887,0.6804657179818887,0.6817593790426908,0.6817593790426908,0.6830530401034929,0.6830530401034929,0.684346701164295,0.684346701164295,0.684346701164295,0.684346701164295,0.685640362225097,0.685640362225097,0.6869340232858991,0.6869340232858991,0.6882276843467011,0.6882276843467011,0.6895213454075032,0.6895213454075032,0.6908150064683053,0.6908150064683053,0.6921086675291074,0.6921086675291074,0.6934023285899095,0.6934023285899095,0.6946959896507116,0.6946959896507116,0.6959896507115135,0.6959896507115135,0.6972833117723156,0.6972833117723156,0.6985769728331177,0.6985769728331177,0.6998706338939198,0.6998706338939198,0.7011642949547219,0.7011642949547219,0.702457956015524,0.702457956015524,0.703751617076326,0.703751617076326,0.705045278137128,0.705045278137128,0.7063389391979301,0.7063389391979301,0.7076326002587322,0.7076326002587322,0.7089262613195343,0.7089262613195343,0.7102199223803364,0.7102199223803364,0.7115135834411385,0.7115135834411385,0.7128072445019404,0.7128072445019404,0.7141009055627425,0.7141009055627425,0.7153945666235446,0.7153945666235446,0.7166882276843467,0.7166882276843467,0.7179818887451488,0.7179818887451488,0.7192755498059509,0.7192755498059509,0.720569210866753,0.720569210866753,0.7218628719275549,0.7218628719275549,0.723156532988357,0.723156532988357,0.7244501940491591,0.7244501940491591,0.7257438551099612,0.7257438551099612,0.7270375161707633,0.7270375161707633,0.7283311772315654,0.7283311772315654,0.7296248382923674,0.7296248382923674,0.7309184993531694,0.7309184993531694,0.7322121604139715,0.7322121604139715,0.7335058214747736,0.7335058214747736,0.7347994825355757,0.7347994825355757,0.7360931435963778,0.7360931435963778,0.7373868046571799,0.7373868046571799,0.7386804657179818,0.7386804657179818,0.7399741267787839,0.7399741267787839,0.741267787839586,0.741267787839586,0.7425614489003881,0.7425614489003881,0.7438551099611902,0.7438551099611902,0.7451487710219923,0.7451487710219923,0.7464424320827943,0.7464424320827943,0.7477360931435963,0.7477360931435963,0.7490297542043984,0.7490297542043984,0.7503234152652005,0.7503234152652005,0.7516170763260026,0.7516170763260026,0.7529107373868047,0.7529107373868047,0.7542043984476067,0.7542043984476067,0.7554980595084088,0.7554980595084088,0.7567917205692108,0.7567917205692108,0.7580853816300129,0.7580853816300129,0.759379042690815,0.759379042690815,0.7606727037516171,0.7606727037516171,0.7619663648124192,0.7619663648124192,0.7632600258732212,0.7632600258732212,0.7645536869340233,0.7645536869340233,0.7658473479948253,0.7658473479948253,0.7671410090556274,0.7671410090556274,0.7684346701164295,0.7684346701164295,0.7697283311772316,0.7697283311772316,0.7710219922380336,0.7710219922380336,0.7723156532988357,0.7723156532988357,0.7736093143596378,0.7736093143596378,0.7749029754204398,0.7749029754204398,0.7761966364812419,0.7761966364812419,0.777490297542044,0.777490297542044,0.7787839586028461,0.7787839586028461,0.7787839586028461,0.7787839586028461,0.7800776196636481,0.7800776196636481,0.7813712807244502,0.7813712807244502,0.7826649417852523,0.7826649417852523,0.7839586028460543,0.7839586028460543,0.7852522639068564,0.7852522639068564,0.7865459249676585,0.7865459249676585,0.7878395860284605,0.7878395860284605,0.7891332470892626,0.7891332470892626,0.7904269081500647,0.7904269081500647,0.7917205692108668,0.7917205692108668,0.7930142302716688,0.7930142302716688,0.7943078913324709,0.7943078913324709,0.795601552393273,0.795601552393273,0.796895213454075,0.796895213454075,0.7981888745148771,0.7981888745148771,0.7994825355756792,0.7994825355756792,0.8007761966364813,0.8007761966364813,0.8020698576972833,0.8020698576972833,0.8033635187580854,0.8033635187580854,0.8046571798188874,0.8046571798188874,0.8059508408796895,0.8059508408796895,0.8072445019404916,0.8072445019404916,0.8098318240620958,0.8098318240620958,0.8111254851228978,0.8111254851228978,0.8124191461836999,0.8124191461836999,0.8137128072445019,0.8137128072445019,0.815006468305304,0.815006468305304,0.8163001293661061,0.8163001293661061,0.8175937904269082,0.8175937904269082,0.8188874514877102,0.8188874514877102,0.8201811125485123,0.8201811125485123,0.8214747736093143,0.8214747736093143,0.8227684346701164,0.8227684346701164,0.8240620957309185,0.8240620957309185,0.8253557567917206,0.8253557567917206,0.8266494178525227,0.8266494178525227,0.8279430789133247,0.8279430789133247,0.8292367399741267,0.8292367399741267,0.8318240620957309,0.8318240620957309,0.833117723156533,0.833117723156533,0.8344113842173351,0.8344113842173351,0.8357050452781372,0.8357050452781372,0.8369987063389392,0.8369987063389392,0.8382923673997412,0.8382923673997412,0.8395860284605433,0.8395860284605433,0.8408796895213454,0.8408796895213454,0.8421733505821475,0.8421733505821475,0.8434670116429496,0.8434670116429496,0.8447606727037517,0.8447606727037517,0.8460543337645536,0.8460543337645536,0.8473479948253557,0.8473479948253557,0.8486416558861578,0.8486416558861578,0.8499353169469599,0.8499353169469599,0.851228978007762,0.851228978007762,0.8525226390685641,0.8525226390685641,0.8538163001293662,0.8538163001293662,0.8551099611901681,0.8551099611901681,0.8564036222509702,0.8564036222509702,0.8576972833117723,0.8576972833117723,0.8589909443725744,0.8589909443725744,0.8602846054333765,0.8602846054333765,0.8615782664941786,0.8615782664941786,0.8628719275549805,0.8628719275549805,0.8641655886157826,0.8641655886157826,0.8654592496765847,0.8654592496765847,0.8667529107373868,0.8667529107373868,0.8680465717981889,0.8680465717981889,0.869340232858991,0.869340232858991,0.8706338939197931,0.8706338939197931,0.871927554980595,0.871927554980595,0.8732212160413971,0.8732212160413971,0.8745148771021992,0.8745148771021992,0.8758085381630013,0.8758085381630013,0.8771021992238034,0.8771021992238034,0.8783958602846055,0.8783958602846055,0.8796895213454075,0.8796895213454075,0.8809831824062095,0.8809831824062095,0.8822768434670116,0.8822768434670116,0.8835705045278137,0.8835705045278137,0.8848641655886158,0.8848641655886158,0.8861578266494179,0.8861578266494179,0.88745148771022,0.88745148771022,0.888745148771022,0.888745148771022,0.890038809831824,0.890038809831824,0.8913324708926261,0.8913324708926261,0.8926261319534282,0.8926261319534282,0.8939197930142303,0.8939197930142303,0.8952134540750324,0.8952134540750324,0.8965071151358344,0.8965071151358344,0.8978007761966365,0.8978007761966365,0.8990944372574385,0.8990944372574385,0.9003880983182406,0.9003880983182406,0.9016817593790427,0.9016817593790427,0.9029754204398448,0.9029754204398448,0.9042690815006468,0.9042690815006468,0.9055627425614489,0.9055627425614489,0.906856403622251,0.906856403622251,0.908150064683053,0.908150064683053,0.9094437257438551,0.9094437257438551,0.9107373868046572,0.9107373868046572,0.9120310478654593,0.9120310478654593,0.9133247089262613,0.9133247089262613,0.9146183699870634,0.9146183699870634,0.9159120310478654,0.9159120310478654,0.9172056921086675,0.9172056921086675,0.9184993531694696,0.9184993531694696,0.9197930142302717,0.9197930142302717,0.9210866752910737,0.9210866752910737,0.9223803363518758,0.9223803363518758,0.9236739974126779,0.9236739974126779,0.92496765847348,0.92496765847348,0.926261319534282,0.926261319534282,0.9275549805950841,0.9275549805950841,0.9288486416558862,0.9288486416558862,0.9301423027166882,0.9301423027166882,0.9314359637774903,0.9314359637774903,0.9327296248382924,0.9327296248382924,0.9340232858990944,0.9340232858990944,0.9353169469598965,0.9353169469598965,0.9366106080206986,0.9366106080206986,0.9379042690815006,0.9379042690815006,0.9391979301423027,0.9391979301423027,0.9404915912031048,0.9404915912031048,0.9417852522639069,0.9417852522639069,0.943078913324709,0.943078913324709,0.944372574385511,0.944372574385511,0.9456662354463131,0.9456662354463131,0.9469598965071151,0.9469598965071151,0.9482535575679172,0.9482535575679172,0.9495472186287193,0.9495472186287193,0.9508408796895214,0.9508408796895214,0.9521345407503234,0.9521345407503234,0.9534282018111255,0.9534282018111255,0.9547218628719275,0.9547218628719275,0.9560155239327296,0.9560155239327296,0.9573091849935317,0.9573091849935317,0.9586028460543338,0.9586028460543338,0.9598965071151359,0.9598965071151359,0.9611901681759379,0.9611901681759379,0.96248382923674,0.96248382923674,0.963777490297542,0.963777490297542,0.9650711513583441,0.9650711513583441,0.9663648124191462,0.9663648124191462,0.9676584734799483,0.9676584734799483,0.9689521345407504,0.9689521345407504,0.9702457956015524,0.9702457956015524,0.9715394566623544,0.9715394566623544,0.9728331177231565,0.9728331177231565,0.9741267787839586,0.9741267787839586,0.9754204398447607,0.9754204398447607,0.9767141009055628,0.9767141009055628,0.9780077619663649,0.9780077619663649,0.9793014230271668,0.9793014230271668,0.9805950840879689,0.9805950840879689,0.981888745148771,0.981888745148771,0.9831824062095731,0.9831824062095731,0.9844760672703752,0.9844760672703752,0.9857697283311773,0.9857697283311773,0.9870633893919794,0.9870633893919794,0.9883570504527813,0.9883570504527813,0.9896507115135834,0.9896507115135834,0.9909443725743855,0.9909443725743855,0.9922380336351876,0.9922380336351876,0.9935316946959897,0.9935316946959897,0.9948253557567918,0.9948253557567918,0.9961190168175937,0.9961190168175937,0.9974126778783958,0.9974126778783958,0.9987063389391979,0.9987063389391979,1.0,1.0],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"fill\":\"tozeroy\",\"showlegend\":false,\"x\":[0.0,0.0,5.637614161686774e-05,5.637614161686774e-05,0.00016912842485060324,0.00016912842485060324,0.00022550456646747095,0.00022550456646747095,0.00033825684970120647,0.00033825684970120647,0.0005637614161686774,0.0005637614161686774,0.0007328898410192806,0.0007328898410192806,0.0010147705491036193,0.0010147705491036193,0.0011838989739542227,0.0011838989739542227,0.001296651257187958,0.001296651257187958,0.0013530273988048259,0.0013530273988048259,0.0014094035404216936,0.0014094035404216936,0.0016912842485060323,0.0016912842485060323,0.0019167888149735032,0.0019167888149735032,0.0024805502311421807,0.0024805502311421807,0.002875183222460255,0.002875183222460255,0.0032134400721614614,0.0032134400721614614,0.0034953207802458,0.0034953207802458,0.00400270605479761,0.00400270605479761,0.004453715187732551,0.004453715187732551,0.004622843612583155,0.004622843612583155,0.004679219754200023,0.004679219754200023,0.0049047243206674935,0.0049047243206674935,0.005017476603901229,0.005017476603901229,0.005186605028751832,0.005186605028751832,0.005299357311985568,0.005299357311985568,0.0055248618784530384,0.0055248618784530384,0.005637614161686774,0.005637614161686774,0.005863118728154245,0.005863118728154245,0.006426880144322923,0.006426880144322923,0.0072161461269590705,0.0072161461269590705,0.007441650693426542,0.007441650693426542,0.00749802683504341,0.00749802683504341,0.007723531401510881,0.007723531401510881,0.007836283684744616,0.007836283684744616,0.008569173525763897,0.008569173525763897,0.008681925808997633,0.008681925808997633,0.009640320216484383,0.009640320216484383,0.009809448641334987,0.009809448641334987,0.009922200924568723,0.009922200924568723,0.010542338482354268,0.010542338482354268,0.010655090765588003,0.010655090765588003,0.010824219190438607,0.010824219190438607,0.011444356748224152,0.011444356748224152,0.01172623745630849,0.01172623745630849,0.01245912729732777,0.01245912729732777,0.013417521704814523,0.013417521704814523,0.013643026271281994,0.013643026271281994,0.013755778554515728,0.013755778554515728,0.013812154696132596,0.013812154696132596,0.014375916112301275,0.014375916112301275,0.014657796820385613,0.014657796820385613,0.01471417296200248,0.01471417296200248,0.01522155823655429,0.01522155823655429,0.015503438944638629,0.015503438944638629,0.017307475476378398,0.017307475476378398,0.017589356184462734,0.017589356184462734,0.01877325515841696,0.01877325515841696,0.01944976885781937,0.01944976885781937,0.019788025707520578,0.019788025707520578,0.01995715413237118,0.01995715413237118,0.020126282557221785,0.020126282557221785,0.02023903484045552,0.02023903484045552,0.020351787123689256,0.020351787123689256,0.020577291690156724,0.020577291690156724,0.02221219979704589,0.02221219979704589,0.022832337354831436,0.022832337354831436,0.022945089638065172,0.022945089638065172,0.023565227195850715,0.023565227195850715,0.023847107903935055,0.023847107903935055,0.02395986018716879,0.02395986018716879,0.02452362160333747,0.02452362160333747,0.025876649002142295,0.025876649002142295,0.026271281993460367,0.026271281993460367,0.02694779569286278,0.02694779569286278,0.02722967640094712,0.02722967640094712,0.027342428684180856,0.027342428684180856,0.02818807080843387,0.02818807080843387,0.02942834592400496,0.02942834592400496,0.029935731198556772,0.029935731198556772,0.030781373322809787,0.030781373322809787,0.03095050174766039,0.03095050174766039,0.031908896155147144,0.031908896155147144,0.03264178599616642,0.03264178599616642,0.03292366670425076,0.03292366670425076,0.033318299695568834,0.033318299695568834,0.034333070244672456,0.034333070244672456,0.03506596008569174,0.03506596008569174,0.03540421693539294,0.03540421693539294,0.03596797835156162,0.03596797835156162,0.036306235201262825,0.036306235201262825,0.03641898748449656,0.03641898748449656,0.03664449205096403,0.03664449205096403,0.036813620475814636,0.036813620475814636,0.03698274890066524,0.03698274890066524,0.03703912504228211,0.03703912504228211,0.03811027173300259,0.03811027173300259,0.03895591385725561,0.03895591385725561,0.039012289998872476,0.039012289998872476,0.03912504228210621,0.03912504228210621,0.04121095952193032,0.04121095952193032,0.04166196865486526,0.04166196865486526,0.041774720938098996,0.041774720938098996,0.04216935392941707,0.04216935392941707,0.042338482354267674,0.042338482354267674,0.04273311534558575,0.04273311534558575,0.04357875746983876,0.04357875746983876,0.04464990416055925,0.04464990416055925,0.04549554628481227,0.04549554628481227,0.04662306911714962,0.04662306911714962,0.04690494982523396,0.04690494982523396,0.04713045439170143,0.04713045439170143,0.05124591272973278,0.05124591272973278,0.05316270154470628,0.05316270154470628,0.056319765475250874,0.056319765475250874,0.05637614161686774,0.05637614161686774,0.05722178374112076,0.05722178374112076,0.058800315706393054,0.058800315706393054,0.05947682940579547,0.05947682940579547,0.0644943060096967,0.0644943060096967,0.06545270041718344,0.06545270041718344,0.06562182884203405,0.06562182884203405,0.06579095726688465,0.06579095726688465,0.06629834254143646,0.06629834254143646,0.06748224151539069,0.06748224151539069,0.06804600293155937,0.06804600293155937,0.06894802119742925,0.06894802119742925,0.07018829631300035,0.07018829631300035,0.07035742473785093,0.07035742473785093,0.0730634795354606,0.0730634795354606,0.07373999323486301,0.07373999323486301,0.07396549780133048,0.07396549780133048,0.07965948810463412,0.07965948810463412,0.07982861652948472,0.07982861652948472,0.08101251550343895,0.08101251550343895,0.08236554290224377,0.08236554290224377,0.0842259555756004,0.0842259555756004,0.087383019506145,0.087383019506145,0.08749577178937873,0.08749577178937873,0.08817228548878114,0.08817228548878114,0.08980719359567031,0.08980719359567031,0.0905400834366896,0.0905400834366896,0.09093471642800767,0.09093471642800767,0.09161123012741008,0.09161123012741008,0.09358439508400045,0.09358439508400045,0.09392265193370165,0.09392265193370165,0.09409178035855226,0.09409178035855226,0.09600856917352577,0.09600856917352577,0.09674145901454505,0.09674145901454505,0.09724884428909686,0.09724884428909686,0.09848911940466794,0.09848911940466794,0.09877100011275228,0.09877100011275228,0.10080054121095952,0.10080054121095952,0.10102604577742699,0.10102604577742699,0.10581801781486075,0.10581801781486075,0.10615627466456196,0.10615627466456196,0.11089187056037884,0.11089187056037884,0.11309054008343669,0.11309054008343669,0.11387980606607284,0.11387980606607284,0.1158529710226632,0.1158529710226632,0.11596572330589694,0.11596572330589694,0.11720599842146803,0.11720599842146803,0.12075769534333071,0.12075769534333071,0.12853760288645846,0.12853760288645846,0.12943962115232832,0.12943962115232832,0.13186379524185365,0.13186379524185365,0.1326530612244898,0.1326530612244898,0.132991318074191,0.132991318074191,0.13383696019844402,0.13383696019844402,0.13394971248167775,0.13394971248167775,0.1350772353140151,0.1350772353140151,0.13569737287180064,0.13569737287180064,0.1364866388544368,0.1364866388544368,0.13733228097868982,0.13733228097868982,0.13772691397000789,0.13772691397000789,0.14139136317510428,0.14139136317510428,0.14150411545833802,0.14150411545833802,0.1415604915999549,0.1415604915999549,0.1427443905739091,0.1427443905739091,0.14466117938888262,0.14466117938888262,0.15576727928740558,0.15576727928740558,0.1571203066862104,0.1571203066862104,0.15914984778441763,0.15914984778441763,0.15960085691735257,0.15960085691735257,0.16439282895478632,0.16439282895478632,0.1754425527116924,0.1754425527116924,0.1784304882173864,0.1784304882173864,0.18012177246589242,0.18012177246589242,0.18694328560153342,0.18694328560153342,0.1894802119742925,0.1894802119742925,0.19810576164167323,0.19810576164167323,0.19917690833239374,0.19917690833239374,0.20007892659826362,0.20007892659826362,0.20752057729169016,0.20752057729169016,0.2103957605141504,0.2103957605141504,0.2107340173638516,0.2107340173638516,0.21428571428571427,0.21428571428571427,0.21665351223362272,0.21665351223362272,0.22167098883752395,0.22167098883752395,0.24579997744954335,0.24579997744954335,0.24822415153906865,0.24822415153906865,0.24895704138008795,0.24895704138008795,0.25087383019506143,0.25087383019506143,0.25245236216033373,0.25245236216033373,0.25329800428458676,0.25329800428458676,0.2575825910474687,0.2575825910474687,0.2612470402525651,0.2612470402525651,0.2702108467696471,0.2702108467696471,0.27195850715977,0.27195850715977,0.2768068553388206,0.2768068553388206,0.2999210734017364,0.2999210734017364,0.30555868756342314,0.30555868756342314,0.3113090540083437,0.3113090540083437,0.3182433194272184,0.3182433194272184,0.3191453376930883,0.3191453376930883,0.3276581350772353,0.3276581350772353,0.32934941932574135,0.32934941932574135,0.3394971248167775,0.3394971248167775,0.3552824444695005,0.3552824444695005,0.3600744165069343,0.3600744165069343,0.3624422144548427,0.3624422144548427,0.3689254707407825,0.3689254707407825,0.3703348742812042,0.3703348742812042,0.37546510316833914,0.37546510316833914,0.3773818919833127,0.3773818919833127,0.3965497801330477,0.3965497801330477,0.4035404216935393,0.4035404216935393,0.4188183560717105,0.4188183560717105,0.42490697936633215,0.42490697936633215,0.4340962904498816,0.4340962904498816,0.4382681249295298,0.4382681249295298,0.44655541774720936,0.44655541774720936,0.5158416957943398,0.5158416957943398,0.5501747660390123,0.5501747660390123,0.5522606832788364,0.5522606832788364,0.6462397113541549,0.6462397113541549,0.7256736948923216,0.7256736948923216,0.8556770774608186,0.8556770774608186,1.0],\"y\":[0.0,0.0038910505836575876,0.0038910505836575876,0.007782101167315175,0.007782101167315175,0.011673151750972763,0.011673151750972763,0.01556420233463035,0.01556420233463035,0.019455252918287938,0.019455252918287938,0.023346303501945526,0.023346303501945526,0.027237354085603113,0.027237354085603113,0.0311284046692607,0.0311284046692607,0.03501945525291829,0.03501945525291829,0.04669260700389105,0.04669260700389105,0.05058365758754864,0.05058365758754864,0.054474708171206226,0.054474708171206226,0.058365758754863814,0.058365758754863814,0.0622568093385214,0.0622568093385214,0.06614785992217899,0.06614785992217899,0.07003891050583658,0.07003891050583658,0.07392996108949416,0.07392996108949416,0.08171206225680934,0.08171206225680934,0.08949416342412451,0.08949416342412451,0.09727626459143969,0.09727626459143969,0.10116731517509728,0.10116731517509728,0.10894941634241245,0.10894941634241245,0.11284046692607004,0.11284046692607004,0.11673151750972763,0.11673151750972763,0.12062256809338522,0.12062256809338522,0.1245136186770428,0.1245136186770428,0.12840466926070038,0.12840466926070038,0.13229571984435798,0.13229571984435798,0.13618677042801555,0.13618677042801555,0.14007782101167315,0.14007782101167315,0.14396887159533073,0.14396887159533073,0.1517509727626459,0.1517509727626459,0.1556420233463035,0.1556420233463035,0.15953307392996108,0.15953307392996108,0.16342412451361868,0.16342412451361868,0.16731517509727625,0.16731517509727625,0.17120622568093385,0.17120622568093385,0.1828793774319066,0.1828793774319066,0.19066147859922178,0.19066147859922178,0.19455252918287938,0.19455252918287938,0.19844357976653695,0.19844357976653695,0.20233463035019456,0.20233463035019456,0.20622568093385213,0.20622568093385213,0.21011673151750973,0.21011673151750973,0.2140077821011673,0.2140077821011673,0.2178988326848249,0.2178988326848249,0.22178988326848248,0.22178988326848248,0.22568093385214008,0.22568093385214008,0.22957198443579765,0.22957198443579765,0.23735408560311283,0.23735408560311283,0.245136186770428,0.245136186770428,0.2490272373540856,0.2490272373540856,0.25680933852140075,0.25680933852140075,0.2607003891050584,0.2607003891050584,0.26459143968871596,0.26459143968871596,0.2723735408560311,0.2723735408560311,0.27626459143968873,0.27626459143968873,0.2801556420233463,0.2801556420233463,0.2840466926070039,0.2840466926070039,0.28793774319066145,0.28793774319066145,0.2918287937743191,0.2918287937743191,0.29571984435797666,0.29571984435797666,0.29961089494163423,0.29961089494163423,0.3035019455252918,0.3035019455252918,0.30739299610894943,0.30739299610894943,0.311284046692607,0.311284046692607,0.3151750972762646,0.3151750972762646,0.31906614785992216,0.31906614785992216,0.3229571984435798,0.3229571984435798,0.32684824902723736,0.32684824902723736,0.33073929961089493,0.33073929961089493,0.3346303501945525,0.3346303501945525,0.33852140077821014,0.33852140077821014,0.3424124513618677,0.3424124513618677,0.3463035019455253,0.3463035019455253,0.35019455252918286,0.35019455252918286,0.3540856031128405,0.3540856031128405,0.35797665369649806,0.35797665369649806,0.36186770428015563,0.36186770428015563,0.3657587548638132,0.3657587548638132,0.36964980544747084,0.36964980544747084,0.3735408560311284,0.3735408560311284,0.377431906614786,0.377431906614786,0.38132295719844356,0.38132295719844356,0.38910505836575876,0.38910505836575876,0.39299610894941633,0.39299610894941633,0.3968871595330739,0.3968871595330739,0.40077821011673154,0.40077821011673154,0.4046692607003891,0.4046692607003891,0.4085603112840467,0.4085603112840467,0.41245136186770426,0.41245136186770426,0.4163424124513619,0.4163424124513619,0.42023346303501946,0.42023346303501946,0.42412451361867703,0.42412451361867703,0.4280155642023346,0.4280155642023346,0.43190661478599224,0.43190661478599224,0.4357976653696498,0.4357976653696498,0.4396887159533074,0.4396887159533074,0.4474708171206226,0.4474708171206226,0.45136186770428016,0.45136186770428016,0.45525291828793774,0.45525291828793774,0.4591439688715953,0.4591439688715953,0.4708171206225681,0.4708171206225681,0.47470817120622566,0.47470817120622566,0.4785992217898833,0.4785992217898833,0.48249027237354086,0.48249027237354086,0.48638132295719844,0.48638132295719844,0.490272373540856,0.490272373540856,0.49416342412451364,0.49416342412451364,0.4980544747081712,0.4980544747081712,0.5019455252918288,0.5019455252918288,0.5058365758754864,0.5058365758754864,0.5097276264591439,0.5097276264591439,0.5136186770428015,0.5136186770428015,0.5175097276264592,0.5175097276264592,0.5214007782101168,0.5214007782101168,0.5252918287937743,0.5252918287937743,0.5291828793774319,0.5291828793774319,0.5330739299610895,0.5330739299610895,0.5408560311284046,0.5408560311284046,0.5447470817120622,0.5447470817120622,0.5486381322957199,0.5486381322957199,0.5525291828793775,0.5525291828793775,0.556420233463035,0.556420233463035,0.5603112840466926,0.5603112840466926,0.5642023346303502,0.5642023346303502,0.5680933852140078,0.5680933852140078,0.5719844357976653,0.5719844357976653,0.5758754863813229,0.5758754863813229,0.5797665369649806,0.5797665369649806,0.5836575875486382,0.5836575875486382,0.5875486381322957,0.5875486381322957,0.5914396887159533,0.5914396887159533,0.5953307392996109,0.5953307392996109,0.5992217898832685,0.5992217898832685,0.603112840466926,0.603112840466926,0.6070038910505836,0.6070038910505836,0.6108949416342413,0.6108949416342413,0.6147859922178989,0.6147859922178989,0.6186770428015564,0.6186770428015564,0.622568093385214,0.622568093385214,0.6264591439688716,0.6264591439688716,0.6303501945525292,0.6303501945525292,0.6342412451361867,0.6342412451361867,0.6381322957198443,0.6381322957198443,0.642023346303502,0.642023346303502,0.6459143968871596,0.6459143968871596,0.6498054474708171,0.6498054474708171,0.6536964980544747,0.6536964980544747,0.6575875486381323,0.6575875486381323,0.6614785992217899,0.6614785992217899,0.6653696498054474,0.6653696498054474,0.669260700389105,0.669260700389105,0.6731517509727627,0.6731517509727627,0.6770428015564203,0.6770428015564203,0.6809338521400778,0.6809338521400778,0.6848249027237354,0.6848249027237354,0.688715953307393,0.688715953307393,0.6926070038910506,0.6926070038910506,0.6964980544747081,0.6964980544747081,0.7003891050583657,0.7003891050583657,0.7042801556420234,0.7042801556420234,0.708171206225681,0.708171206225681,0.7120622568093385,0.7120622568093385,0.7159533073929961,0.7159533073929961,0.7198443579766537,0.7198443579766537,0.7237354085603113,0.7237354085603113,0.7276264591439688,0.7276264591439688,0.7315175097276264,0.7315175097276264,0.7354085603112841,0.7354085603112841,0.7392996108949417,0.7392996108949417,0.7431906614785992,0.7431906614785992,0.7470817120622568,0.7470817120622568,0.7509727626459144,0.7509727626459144,0.754863813229572,0.754863813229572,0.7587548638132295,0.7587548638132295,0.7626459143968871,0.7626459143968871,0.7665369649805448,0.7665369649805448,0.7704280155642024,0.7704280155642024,0.77431906614786,0.77431906614786,0.7782101167315175,0.7782101167315175,0.7821011673151751,0.7821011673151751,0.7859922178988327,0.7859922178988327,0.7898832684824902,0.7898832684824902,0.7937743190661478,0.7937743190661478,0.7976653696498055,0.7976653696498055,0.8015564202334631,0.8015564202334631,0.8054474708171206,0.8054474708171206,0.8093385214007782,0.8093385214007782,0.8132295719844358,0.8132295719844358,0.8171206225680934,0.8171206225680934,0.8210116731517509,0.8210116731517509,0.8249027237354085,0.8249027237354085,0.8287937743190662,0.8287937743190662,0.8326848249027238,0.8326848249027238,0.8365758754863813,0.8365758754863813,0.8404669260700389,0.8404669260700389,0.8443579766536965,0.8443579766536965,0.8482490272373541,0.8482490272373541,0.8521400778210116,0.8521400778210116,0.8560311284046692,0.8560311284046692,0.8599221789883269,0.8599221789883269,0.8638132295719845,0.8638132295719845,0.867704280155642,0.867704280155642,0.8715953307392996,0.8715953307392996,0.8754863813229572,0.8754863813229572,0.8793774319066148,0.8793774319066148,0.8832684824902723,0.8832684824902723,0.8871595330739299,0.8871595330739299,0.8910505836575876,0.8910505836575876,0.8949416342412452,0.8949416342412452,0.8988326848249028,0.8988326848249028,0.9027237354085603,0.9027237354085603,0.9066147859922179,0.9066147859922179,0.9105058365758755,0.9105058365758755,0.914396887159533,0.914396887159533,0.9182879377431906,0.9182879377431906,0.9221789883268483,0.9221789883268483,0.9260700389105059,0.9260700389105059,0.9299610894941635,0.9299610894941635,0.933852140077821,0.933852140077821,0.9377431906614786,0.9377431906614786,0.9416342412451362,0.9416342412451362,0.9455252918287937,0.9455252918287937,0.9494163424124513,0.9494163424124513,0.953307392996109,0.953307392996109,0.9571984435797666,0.9571984435797666,0.9610894941634242,0.9610894941634242,0.9649805447470817,0.9649805447470817,0.9688715953307393,0.9688715953307393,0.9727626459143969,0.9727626459143969,0.9766536964980544,0.9766536964980544,0.980544747081712,0.980544747081712,0.9844357976653697,0.9844357976653697,0.9883268482490273,0.9883268482490273,0.9922178988326849,0.9922178988326849,0.9961089494163424,0.9961089494163424,1.0,1.0],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"fill\":\"tozeroy\",\"showlegend\":false,\"x\":[0.0,5.637614161686774e-05,0.0002818807080843387,0.0002818807080843387,0.0007328898410192806,0.0007328898410192806,0.0008456421242530162,0.0008456421242530162,0.0009020182658698838,0.0009020182658698838,0.0011838989739542227,0.0011838989739542227,0.001522155823655429,0.001522155823655429,0.0016912842485060323,0.0016912842485060323,0.002424174089525313,0.002424174089525313,0.0027624309392265192,0.0027624309392265192,0.0032134400721614614,0.0032134400721614614,0.0033261923553951967,0.0033261923553951967,0.0033825684970120646,0.0033825684970120646,0.003438944638628932,0.003438944638628932,0.003551696921862668,0.003551696921862668,0.004340962904498816,0.004340962904498816,0.004397339046115683,0.004397339046115683,0.005073852745518097,0.005073852745518097,0.005299357311985568,0.005299357311985568,0.0054121095952193035,0.0054121095952193035,0.00575036644492051,0.00575036644492051,0.005975871011387981,0.005975871011387981,0.006596008569173526,0.006596008569173526,0.006934265418874732,0.006934265418874732,0.0071597699853422035,0.0071597699853422035,0.00800541210959522,0.00800541210959522,0.008174540534445822,0.008174540534445822,0.008400045100913293,0.008400045100913293,0.009132934941932574,0.009132934941932574,0.009414815650016914,0.009414815650016914,0.009696696358101251,0.009696696358101251,0.009753072499718119,0.009753072499718119,0.010147705491036194,0.010147705491036194,0.010316833915886796,0.010316833915886796,0.010598714623971136,0.010598714623971136,0.010655090765588003,0.010655090765588003,0.010824219190438607,0.010824219190438607,0.010993347615289209,0.010993347615289209,0.011387980606607284,0.011387980606607284,0.011557109031457888,0.011557109031457888,0.011613485173074754,0.011613485173074754,0.012289998872477167,0.012289998872477167,0.012515503438944638,0.012515503438944638,0.013022888713496448,0.013022888713496448,0.013248393279963919,0.013248393279963919,0.013755778554515728,0.013755778554515728,0.01522155823655429,0.01522155823655429,0.015334310519788025,0.015334310519788025,0.015447062803021761,0.015447062803021761,0.015672567369489233,0.015672567369489233,0.016292704927274776,0.016292704927274776,0.01646183335212538,0.01646183335212538,0.016630961776975983,0.016630961776975983,0.01674371406020972,0.01674371406020972,0.01725109933476153,0.01725109933476153,0.01849137445033262,0.01849137445033262,0.01871687901680009,0.01871687901680009,0.018886007441650695,0.018886007441650695,0.019055135866501295,0.019055135866501295,0.019280640432968767,0.019280640432968767,0.02040816326530612,0.02040816326530612,0.02221219979704589,0.02221219979704589,0.02249408050513023,0.02249408050513023,0.023339722629383244,0.023339722629383244,0.02345247491261698,0.02345247491261698,0.023508851054233847,0.023508851054233847,0.023790731762318187,0.023790731762318187,0.023847107903935055,0.023847107903935055,0.024354493178486866,0.024354493178486866,0.02491825459465554,0.02491825459465554,0.025031006877889277,0.025031006877889277,0.025312887585973616,0.025312887585973616,0.025538392152441088,0.025538392152441088,0.025651144435674823,0.025651144435674823,0.02576389671890856,0.02576389671890856,0.026327658135077234,0.026327658135077234,0.026609538843161574,0.026609538843161574,0.030555868756342315,0.030555868756342315,0.030781373322809787,0.030781373322809787,0.03123238245574473,0.03123238245574473,0.032754538279400155,0.032754538279400155,0.03286729056263389,0.03286729056263389,0.033318299695568834,0.033318299695568834,0.035235088510542335,0.035235088510542335,0.03534784079377607,0.03534784079377607,0.036080730634795354,0.036080730634795354,0.03754651031683392,0.03754651031683392,0.037602886458450786,0.037602886458450786,0.03782839102491826,0.03782839102491826,0.038223024016236326,0.038223024016236326,0.03833577629947006,0.03833577629947006,0.03839215244108693,0.03839215244108693,0.04059082196414478,0.04059082196414478,0.04081632653061224,0.04081632653061224,0.04295861991205322,0.04295861991205322,0.043353252903371296,0.043353252903371296,0.04391701431953997,0.04391701431953997,0.043973390461156836,0.043973390461156836,0.045326417859961665,0.045326417859961665,0.04628481226744842,0.04628481226744842,0.04634118840906529,0.04634118840906529,0.04639756455068215,0.04639756455068215,0.04679219754200022,0.04679219754200022,0.048370729507272524,0.048370729507272524,0.0487653624985906,0.0487653624985906,0.048878114781824335,0.048878114781824335,0.04899086706505807,0.04899086706505807,0.04955462848122674,0.04955462848122674,0.05073852745518097,0.05073852745518097,0.05152779343781712,0.05152779343781712,0.05175329800428459,0.05175329800428459,0.052768068553388205,0.052768068553388205,0.05541774720938099,0.05541774720938099,0.05615063705040027,0.05615063705040027,0.06161912278723644,0.06161912278723644,0.0627466456195738,0.0627466456195738,0.06285939790280753,0.06285939790280753,0.06596008569173525,0.06596008569173525,0.06618559025820273,0.06618559025820273,0.06629834254143646,0.06629834254143646,0.06894802119742925,0.06894802119742925,0.0693990303303642,0.0693990303303642,0.07114669072048709,0.07114669072048709,0.07396549780133048,0.07396549780133048,0.07802457999774495,0.07802457999774495,0.07926485511331605,0.07926485511331605,0.08056150637050401,0.08056150637050401,0.08354944187619799,0.08354944187619799,0.08394407486751607,0.08394407486751607,0.08501522155823656,0.08501522155823656,0.08766490021422933,0.08766490021422933,0.08975081745405344,0.08975081745405344,0.09155485398579322,0.09155485398579322,0.09178035855226069,0.09178035855226069,0.09200586311872816,0.09200586311872816,0.09358439508400045,0.09358439508400045,0.0945427894914872,0.0945427894914872,0.09510655090765588,0.09510655090765588,0.09533205547412335,0.09533205547412335,0.09617769759837637,0.09617769759837637,0.09860187168790167,0.09860187168790167,0.10034953207802458,0.10034953207802458,0.10158980719359567,0.10158980719359567,0.10288645845078363,0.10288645845078363,0.10547976096515954,0.10547976096515954,0.10570526553162701,0.10570526553162701,0.10615627466456196,0.10615627466456196,0.10739654978013305,0.10739654978013305,0.10796031119630173,0.10796031119630173,0.10897508174540535,0.10897508174540535,0.11083549441876198,0.11083549441876198,0.11089187056037884,0.11089187056037884,0.11399255834930658,0.11399255834930658,0.11416168677415718,0.11416168677415718,0.11478182433194273,0.11478182433194273,0.11951742022775962,0.11951742022775962,0.11963017251099335,0.11963017251099335,0.1209268237681813,0.1209268237681813,0.12205434660051866,0.12205434660051866,0.1225053557334536,0.1225053557334536,0.12752283233735484,0.12752283233735484,0.1279174653286729,0.1279174653286729,0.12910136430262714,0.12910136430262714,0.12994700642688015,0.12994700642688015,0.13152553839215245,0.13152553839215245,0.1335550794903597,0.1335550794903597,0.13699402412898862,0.13699402412898862,0.13744503326192356,0.13744503326192356,0.13975645506821513,0.13975645506821513,0.1408839779005525,0.1408839779005525,0.15041154583380315,0.15041154583380315,0.15311760063141278,0.15311760063141278,0.15334310519788025,0.15334310519788025,0.15368136204758145,0.15368136204758145,0.15841695794339836,0.15841695794339836,0.16039012289998872,0.16039012289998872,0.16281429698951405,0.16281429698951405,0.16625324162814298,0.16625324162814298,0.1684519111512008,0.1684519111512008,0.17403314917127072,0.17403314917127072,0.1758371857030105,0.1758371857030105,0.17600631412786108,0.17600631412786108,0.1835043409629045,0.1835043409629045,0.18711241402638404,0.18711241402638404,0.18818356071710451,0.18818356071710451,0.18863456985003946,0.18863456985003946,0.19246814747998647,0.19246814747998647,0.20334874281204193,0.20334874281204193,0.2044762656443793,0.2044762656443793,0.21417296200248054,0.21417296200248054,0.22888713496448304,0.22888713496448304,0.23187507047017702,0.23187507047017702,0.23548314353365657,0.23548314353365657,0.23683617093246137,0.23683617093246137,0.23779456533994814,0.23779456533994814,0.2460818581576277,0.2460818581576277,0.25369263727590485,0.25369263727590485,0.25431277483369036,0.25431277483369036,0.2560604352238133,0.2560604352238133,0.25887924230465664,0.25887924230465664,0.26203630623520124,0.26203630623520124,0.2646296087495772,0.2646296087495772,0.2693652046453941,0.2693652046453941,0.27094373661066634,0.27094373661066634,0.2717893787349194,0.2717893787349194,0.2743826812492953,0.2743826812492953,0.2764122223475025,0.2764122223475025,0.2891532303529146,0.2891532303529146,0.29738414702897736,0.29738414702897736,0.30009020182658697,0.30009020182658697,0.332055474123351,0.332055474123351,0.33509978577066185,0.33509978577066185,0.3507723531401511,0.3507723531401511,0.375972488442891,0.375972488442891,0.38741684519111513,0.38741684519111513,0.38769872589919946,0.38769872589919946,0.38961551471417294,0.38961551471417294,0.4034840455519224,0.4034840455519224,0.4349983087157515,0.4349983087157515,0.4681474799864697,0.4681474799864697,0.4703461495095276,0.4703461495095276,0.5039463299131808,0.5039463299131808,0.5113879806066073,0.5113879806066073,0.5233397226293832,0.5233397226293832,0.5271169241177134,0.5271169241177134,0.5489344909234412,0.5489344909234412,0.563084902469275,0.563084902469275,0.5727252226857594,0.5727252226857594,0.5849588454166197,0.5849588454166197,0.5863118728154245,0.5863118728154245,0.5912729732777089,0.5912729732777089,0.6079039350546849,0.6079039350546849,0.6319201713834706,0.6319201713834706,0.7785545157289435,0.7785545157289435,0.838425978126057,0.838425978126057,0.9158867967076333,0.9158867967076333,1.0],\"y\":[0.0,0.0,0.0,0.007751937984496124,0.007751937984496124,0.01937984496124031,0.01937984496124031,0.027131782945736434,0.027131782945736434,0.03488372093023256,0.03488372093023256,0.03875968992248062,0.03875968992248062,0.04263565891472868,0.04263565891472868,0.05813953488372093,0.05813953488372093,0.06201550387596899,0.06201550387596899,0.06976744186046512,0.06976744186046512,0.07364341085271318,0.07364341085271318,0.07751937984496124,0.07751937984496124,0.08139534883720931,0.08139534883720931,0.08527131782945736,0.08527131782945736,0.08914728682170543,0.08914728682170543,0.09302325581395349,0.09302325581395349,0.09689922480620156,0.09689922480620156,0.10465116279069768,0.10465116279069768,0.10852713178294573,0.10852713178294573,0.1124031007751938,0.1124031007751938,0.11627906976744186,0.11627906976744186,0.12015503875968993,0.12015503875968993,0.13178294573643412,0.13178294573643412,0.13565891472868216,0.13565891472868216,0.13953488372093023,0.13953488372093023,0.1434108527131783,0.1434108527131783,0.14728682170542637,0.14728682170542637,0.1511627906976744,0.1511627906976744,0.15503875968992248,0.15503875968992248,0.15891472868217055,0.15891472868217055,0.16279069767441862,0.16279069767441862,0.16666666666666666,0.16666666666666666,0.17054263565891473,0.17054263565891473,0.1744186046511628,0.1744186046511628,0.17829457364341086,0.17829457364341086,0.1821705426356589,0.1821705426356589,0.18604651162790697,0.18604651162790697,0.18992248062015504,0.18992248062015504,0.1937984496124031,0.1937984496124031,0.19767441860465115,0.19767441860465115,0.20155038759689922,0.20155038759689922,0.2054263565891473,0.2054263565891473,0.20930232558139536,0.20930232558139536,0.2131782945736434,0.2131782945736434,0.21705426356589147,0.21705426356589147,0.22093023255813954,0.22093023255813954,0.2248062015503876,0.2248062015503876,0.22868217054263565,0.22868217054263565,0.2364341085271318,0.2364341085271318,0.24031007751937986,0.24031007751937986,0.24806201550387597,0.24806201550387597,0.25193798449612403,0.25193798449612403,0.2558139534883721,0.2558139534883721,0.2596899224806202,0.2596899224806202,0.26744186046511625,0.26744186046511625,0.2713178294573643,0.2713178294573643,0.2751937984496124,0.2751937984496124,0.28294573643410853,0.28294573643410853,0.2868217054263566,0.2868217054263566,0.29069767441860467,0.29069767441860467,0.29457364341085274,0.29457364341085274,0.29844961240310075,0.29844961240310075,0.3023255813953488,0.3023255813953488,0.3062015503875969,0.3062015503875969,0.31007751937984496,0.31007751937984496,0.313953488372093,0.313953488372093,0.3178294573643411,0.3178294573643411,0.32170542635658916,0.32170542635658916,0.32558139534883723,0.32558139534883723,0.32945736434108525,0.32945736434108525,0.3333333333333333,0.3333333333333333,0.3372093023255814,0.3372093023255814,0.34108527131782945,0.34108527131782945,0.3449612403100775,0.3449612403100775,0.35271317829457366,0.35271317829457366,0.35658914728682173,0.35658914728682173,0.3643410852713178,0.3643410852713178,0.3682170542635659,0.3682170542635659,0.37209302325581395,0.37209302325581395,0.375968992248062,0.375968992248062,0.3798449612403101,0.3798449612403101,0.38372093023255816,0.38372093023255816,0.3875968992248062,0.3875968992248062,0.3953488372093023,0.3953488372093023,0.3992248062015504,0.3992248062015504,0.40310077519379844,0.40310077519379844,0.4069767441860465,0.4069767441860465,0.4108527131782946,0.4108527131782946,0.41472868217054265,0.41472868217054265,0.4186046511627907,0.4186046511627907,0.42248062015503873,0.42248062015503873,0.4263565891472868,0.4263565891472868,0.43023255813953487,0.43023255813953487,0.43410852713178294,0.43410852713178294,0.437984496124031,0.437984496124031,0.4418604651162791,0.4418604651162791,0.44573643410852715,0.44573643410852715,0.4496124031007752,0.4496124031007752,0.45348837209302323,0.45348837209302323,0.4573643410852713,0.4573643410852713,0.46124031007751937,0.46124031007751937,0.46511627906976744,0.46511627906976744,0.4689922480620155,0.4689922480620155,0.4728682170542636,0.4728682170542636,0.47674418604651164,0.47674418604651164,0.4806201550387597,0.4806201550387597,0.4844961240310077,0.4844961240310077,0.49612403100775193,0.49612403100775193,0.5,0.5,0.5038759689922481,0.5038759689922481,0.5077519379844961,0.5077519379844961,0.5116279069767442,0.5116279069767442,0.5155038759689923,0.5155038759689923,0.5232558139534884,0.5232558139534884,0.5271317829457365,0.5271317829457365,0.5310077519379846,0.5310077519379846,0.5348837209302325,0.5348837209302325,0.5387596899224806,0.5387596899224806,0.5426356589147286,0.5426356589147286,0.5465116279069767,0.5465116279069767,0.5503875968992248,0.5503875968992248,0.5542635658914729,0.5542635658914729,0.5581395348837209,0.5581395348837209,0.562015503875969,0.562015503875969,0.5658914728682171,0.5658914728682171,0.5697674418604651,0.5697674418604651,0.5736434108527132,0.5736434108527132,0.5775193798449613,0.5775193798449613,0.5813953488372093,0.5813953488372093,0.5852713178294574,0.5852713178294574,0.5891472868217055,0.5891472868217055,0.5930232558139535,0.5930232558139535,0.5968992248062015,0.5968992248062015,0.6007751937984496,0.6007751937984496,0.6046511627906976,0.6046511627906976,0.6085271317829457,0.6085271317829457,0.6124031007751938,0.6124031007751938,0.6162790697674418,0.6162790697674418,0.6201550387596899,0.6201550387596899,0.624031007751938,0.624031007751938,0.627906976744186,0.627906976744186,0.6317829457364341,0.6317829457364341,0.6356589147286822,0.6356589147286822,0.6395348837209303,0.6395348837209303,0.6434108527131783,0.6434108527131783,0.6472868217054264,0.6472868217054264,0.6511627906976745,0.6511627906976745,0.6550387596899225,0.6550387596899225,0.6589147286821705,0.6589147286821705,0.6627906976744186,0.6627906976744186,0.6666666666666666,0.6666666666666666,0.6705426356589147,0.6705426356589147,0.6744186046511628,0.6744186046511628,0.6782945736434108,0.6782945736434108,0.6821705426356589,0.6821705426356589,0.686046511627907,0.686046511627907,0.689922480620155,0.689922480620155,0.6937984496124031,0.6937984496124031,0.6976744186046512,0.6976744186046512,0.7015503875968992,0.7015503875968992,0.7054263565891473,0.7054263565891473,0.7093023255813954,0.7093023255813954,0.7131782945736435,0.7131782945736435,0.7170542635658915,0.7170542635658915,0.7209302325581395,0.7209302325581395,0.7248062015503876,0.7248062015503876,0.7286821705426356,0.7286821705426356,0.7325581395348837,0.7325581395348837,0.7364341085271318,0.7364341085271318,0.7403100775193798,0.7403100775193798,0.7441860465116279,0.7441860465116279,0.748062015503876,0.748062015503876,0.751937984496124,0.751937984496124,0.7558139534883721,0.7558139534883721,0.7596899224806202,0.7596899224806202,0.7674418604651163,0.7674418604651163,0.7713178294573644,0.7713178294573644,0.7751937984496124,0.7751937984496124,0.7790697674418605,0.7790697674418605,0.7829457364341085,0.7829457364341085,0.7868217054263565,0.7868217054263565,0.7906976744186046,0.7906976744186046,0.7945736434108527,0.7945736434108527,0.7984496124031008,0.7984496124031008,0.8023255813953488,0.8023255813953488,0.8062015503875969,0.8062015503875969,0.810077519379845,0.810077519379845,0.813953488372093,0.813953488372093,0.8178294573643411,0.8178294573643411,0.8217054263565892,0.8217054263565892,0.8255813953488372,0.8255813953488372,0.8294573643410853,0.8294573643410853,0.8333333333333334,0.8333333333333334,0.8372093023255814,0.8372093023255814,0.8410852713178295,0.8410852713178295,0.8449612403100775,0.8449612403100775,0.8488372093023255,0.8488372093023255,0.8527131782945736,0.8527131782945736,0.8565891472868217,0.8565891472868217,0.8604651162790697,0.8604651162790697,0.8643410852713178,0.8643410852713178,0.8682170542635659,0.8682170542635659,0.872093023255814,0.872093023255814,0.875968992248062,0.875968992248062,0.8798449612403101,0.8798449612403101,0.8837209302325582,0.8837209302325582,0.8875968992248062,0.8875968992248062,0.8914728682170543,0.8914728682170543,0.8953488372093024,0.8953488372093024,0.8992248062015504,0.8992248062015504,0.9031007751937985,0.9031007751937985,0.9069767441860465,0.9069767441860465,0.9108527131782945,0.9108527131782945,0.9147286821705426,0.9147286821705426,0.9186046511627907,0.9186046511627907,0.9224806201550387,0.9224806201550387,0.9263565891472868,0.9263565891472868,0.9302325581395349,0.9302325581395349,0.9341085271317829,0.9341085271317829,0.937984496124031,0.937984496124031,0.9418604651162791,0.9418604651162791,0.9457364341085271,0.9457364341085271,0.9496124031007752,0.9496124031007752,0.9534883720930233,0.9534883720930233,0.9573643410852714,0.9573643410852714,0.9612403100775194,0.9612403100775194,0.9651162790697675,0.9651162790697675,0.9689922480620154,0.9689922480620154,0.9728682170542635,0.9728682170542635,0.9767441860465116,0.9767441860465116,0.9806201550387597,0.9806201550387597,0.9844961240310077,0.9844961240310077,0.9883720930232558,0.9883720930232558,0.9922480620155039,0.9922480620155039,0.9961240310077519,0.9961240310077519,1.0,1.0],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.2888888888888889]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.35555555555555557,0.6444444444444445]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.7111111111111111,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,1.0]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Train\",\"x\":0.14444444444444446,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Validation\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Test\",\"x\":0.8555555555555556,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"showarrow\":false,\"text\":\"AUC=0.9060\",\"x\":0.7,\"xref\":\"x\",\"y\":0.5,\"yref\":\"y\"},{\"showarrow\":false,\"text\":\"AUC=0.8930\",\"x\":0.7,\"xref\":\"x2\",\"y\":0.5,\"yref\":\"y2\"},{\"showarrow\":false,\"text\":\"AUC=0.8797\",\"x\":0.7,\"xref\":\"x3\",\"y\":0.5,\"yref\":\"y3\"}],\"title\":{\"text\":\"XGBoost Model\"},\"height\":400,\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5a98682f-2e6c-4ace-8c85-58f7713748ca');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Train\", \"Validation\", \"Test\"), vertical_spacing = 0.05)\n",
    "\n",
    "xgb_metrics = [(y_train, y_train_lg_pipe_pred[:, 1]),\n",
    "                   (y_val, y_val_lg_pipe_pred[:, 1]),\n",
    "                   (y_test, y_test_lg_pipe_pred[:, 1])]\n",
    "count = 0\n",
    "for i in xgb_metrics:\n",
    "    fpr, tpr, thresholds = roc_curve(i[0], i[1])\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    count+=1\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, fill='tozeroy',showlegend=False),row=1,col=count)\n",
    "    fig.add_annotation(text=f'AUC={auc_score:.4f}',\n",
    "                  xref=\"x\"+str(count), yref=\"y\"+str(count),\n",
    "                  x=0.7, y=0.5, showarrow=False)\n",
    "fig.update_layout(height=400, width=800, title_text=\"XGBoost Model\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eada0e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x16a586640>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, np.where(y_test_lg_pipe_pred[:, 1]>0.7,1,0), labels=lg_pipeline.steps[1][1].best_estimator_.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=lg_pipeline.steps[1][1].best_estimator_.classes_)\n",
    "\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "406cd0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic reg F1 score, validation: 0.1674\n",
      "Logistic reg F1 score, train: 0.1740\n",
      "Logistic reg F1 score, test: 0.1558 \n",
      "\n",
      "Logistic reg Average precision score, validation:  0.0638\n",
      "Logistic reg Average precision score, train:  0.0704\n",
      "Logistic reg Average precision score, test:  0.0585\n"
     ]
    }
   ],
   "source": [
    "print(f'Logistic reg F1 score, validation: {f1_score(y_val, np.where(y_val_lg_pipe_pred [:,1]>0.7,1,0)):.4f}')\n",
    "print(f'Logistic reg F1 score, train: {f1_score(y_train, np.where(y_train_lg_pipe_pred [:,1]>0.7,1,0)):.4f}')\n",
    "print(f'Logistic reg F1 score, test: {f1_score(y_test, np.where(y_test_lg_pipe_pred [:,1]>0.7,1,0)):.4f} \\n')\n",
    "\n",
    "print(f'Logistic reg Average precision score, validation:  {average_precision_score(y_val, np.where(y_val_lg_pipe_pred [:,1]>0.7,1,0)):.4f}')\n",
    "print(f'Logistic reg Average precision score, train:  {average_precision_score(y_train, np.where(y_train_lg_pipe_pred [:,1]>0.7,1,0)):.4f}')\n",
    "print(f'Logistic reg Average precision score, test:  {average_precision_score(y_test, np.where(y_test_lg_pipe_pred [:,1]>0.7,1,0)):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e73cf",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b80fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = df0[df0['default'].isna() == True]\n",
    "\n",
    "pred_df_X = pred_df.drop(['uuid','default'],axis=1)\n",
    "\n",
    "uuids = pred_df['uuid']\n",
    "y_predictions = lg_pipeline.predict_proba(pred_df_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6338f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(np.concatenate([uuids.values.reshape(-1,1),y_predictions.reshape(-1,1)],axis=1), columns=['uuid','pd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72e2e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.set_index('uuid').to_csv(r'predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
